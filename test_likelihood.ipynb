{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee35f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fd99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om\t\t\t\\Omega_m\n",
    "# Obh2\t\t\t\\Omega_{b}h^2\n",
    "# h\t\n",
    "datafile = 'chains/LCDM_phy_HD_nested_dynesty_multi_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc69788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataSet(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Prepare the dataset for regression\n",
    "    '''\n",
    "    def __init__(self, X, y, scale_data=False):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c80912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ncols = 3\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(ncols, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e24b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1036, 3) (1036, 1)\n",
      "(400, 3) (100, 3)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Load Boston dataset\n",
    "    X = np.loadtxt(datafile, usecols=(2,3,4))\n",
    "    y = np.loadtxt(datafile, usecols=1).reshape(-1, 1)\n",
    "    randomize = np.random.permutation(len(X))\n",
    "    X = X[randomize]\n",
    "    y = y[randomize]\n",
    "    print(np.shape(X), np.shape(y))\n",
    "    X_test, y_test = X[:100, :], y[:100, :]\n",
    "    X, y = X[100:600, :], y[100:600, :]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    print(np.shape(X_train), np.shape(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f173672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LoadDataSet(X_train, y_train)\n",
    "dataset_val = LoadDataSet(X_val, y_val)\n",
    "# dataset_test = LoadDataSet(X_test, y_test)\n",
    "X_test, y_test = torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62b89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=1)\n",
    "validloader = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3ea89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "mlp.float()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f533ceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       800\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       40,200\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       201\n",
       "=================================================================\n",
       "Total params: 41,201\n",
       "Trainable params: 41,201\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mlp, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "00af4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 43.468\n",
      "Loss after mini-batch    11: 12.249\n",
      "Loss after mini-batch    21: 8.199\n",
      "Loss after mini-batch    31: 7.028\n",
      "Loss after mini-batch    41: 7.697\n",
      "Loss after mini-batch    51: 6.994\n",
      "Loss after mini-batch    61: 7.762\n",
      "Loss after mini-batch    71: 10.335\n",
      "Loss after mini-batch    81: 8.519\n",
      "Loss after mini-batch    91: 6.157\n",
      "Loss after mini-batch   101: 5.873\n",
      "Loss after mini-batch   111: 5.749\n",
      "Loss after mini-batch   121: 6.025\n",
      "Loss after mini-batch   131: 23.630\n",
      "Loss after mini-batch   141: 19.197\n",
      "Loss after mini-batch   151: 47.126\n",
      "Loss after mini-batch   161: 5.268\n",
      "Loss after mini-batch   171: 4.326\n",
      "Loss after mini-batch   181: 6.877\n",
      "Loss after mini-batch   191: 4.990\n",
      "Loss after mini-batch   201: 3.982\n",
      "Loss after mini-batch   211: 3.099\n",
      "Loss after mini-batch   221: 8.234\n",
      "Loss after mini-batch   231: 3.155\n",
      "Loss after mini-batch   241: 1.927\n",
      "Loss after mini-batch   251: 1.678\n",
      "Loss after mini-batch   261: 1.805\n",
      "Loss after mini-batch   271: 0.645\n",
      "Loss after mini-batch   281: 3.142\n",
      "Loss after mini-batch   291: 0.829\n",
      "Loss after mini-batch   301: 0.393\n",
      "Loss after mini-batch   311: 6.198\n",
      "Loss after mini-batch   321: 0.439\n",
      "Loss after mini-batch   331: 0.087\n",
      "Loss after mini-batch   341: 0.443\n",
      "Loss after mini-batch   351: 59.280\n",
      "Loss after mini-batch   361: 22.101\n",
      "Loss after mini-batch   371: 0.657\n",
      "Loss after mini-batch   381: 0.055\n",
      "Loss after mini-batch   391: 0.168\n",
      "Training Loss: 3.600 \t\t Validation Loss:4.237\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 0.820\n",
      "Loss after mini-batch    11: 73.354\n",
      "Loss after mini-batch    21: 0.332\n",
      "Loss after mini-batch    31: 0.661\n",
      "Loss after mini-batch    41: 2.436\n",
      "Loss after mini-batch    51: 5.002\n",
      "Loss after mini-batch    61: 11.109\n",
      "Loss after mini-batch    71: 2.228\n",
      "Loss after mini-batch    81: 1.119\n",
      "Loss after mini-batch    91: 1.026\n",
      "Loss after mini-batch   101: 0.414\n",
      "Loss after mini-batch   111: 1.039\n",
      "Loss after mini-batch   121: 18.460\n",
      "Loss after mini-batch   131: 3.260\n",
      "Loss after mini-batch   141: 5.129\n",
      "Loss after mini-batch   151: 1.321\n",
      "Loss after mini-batch   161: 1.172\n",
      "Loss after mini-batch   171: 3.048\n",
      "Loss after mini-batch   181: 5.456\n",
      "Loss after mini-batch   191: 15.675\n",
      "Loss after mini-batch   201: 0.911\n",
      "Loss after mini-batch   211: 0.340\n",
      "Loss after mini-batch   221: 0.539\n",
      "Loss after mini-batch   231: 0.244\n",
      "Loss after mini-batch   241: 0.605\n",
      "Loss after mini-batch   251: 0.075\n",
      "Loss after mini-batch   261: 3.722\n",
      "Loss after mini-batch   271: 0.418\n",
      "Loss after mini-batch   281: 0.075\n",
      "Loss after mini-batch   291: 8.315\n",
      "Loss after mini-batch   301: 1.198\n",
      "Loss after mini-batch   311: 0.343\n",
      "Loss after mini-batch   321: 0.125\n",
      "Loss after mini-batch   331: 0.560\n",
      "Loss after mini-batch   341: 9.807\n",
      "Loss after mini-batch   351: 0.186\n",
      "Loss after mini-batch   361: 0.752\n",
      "Loss after mini-batch   371: 0.691\n",
      "Loss after mini-batch   381: 18.682\n",
      "Loss after mini-batch   391: 1.166\n",
      "Training Loss: 0.184 \t\t Validation Loss:0.685\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 0.546\n",
      "Loss after mini-batch    11: 0.121\n",
      "Loss after mini-batch    21: 9.059\n",
      "Loss after mini-batch    31: 0.736\n",
      "Loss after mini-batch    41: 0.682\n",
      "Loss after mini-batch    51: 0.093\n",
      "Loss after mini-batch    61: 1.439\n",
      "Loss after mini-batch    71: 0.805\n",
      "Loss after mini-batch    81: 0.854\n",
      "Loss after mini-batch    91: 0.557\n",
      "Loss after mini-batch   101: 26.159\n",
      "Loss after mini-batch   111: 60.208\n",
      "Loss after mini-batch   121: 0.607\n",
      "Loss after mini-batch   131: 0.360\n",
      "Loss after mini-batch   141: 0.131\n",
      "Loss after mini-batch   151: 0.838\n",
      "Loss after mini-batch   161: 0.133\n",
      "Loss after mini-batch   171: 0.777\n",
      "Loss after mini-batch   181: 32.043\n",
      "Loss after mini-batch   191: 0.694\n",
      "Loss after mini-batch   201: 0.665\n",
      "Loss after mini-batch   211: 0.745\n",
      "Loss after mini-batch   221: 0.528\n",
      "Loss after mini-batch   231: 8.970\n",
      "Loss after mini-batch   241: 0.576\n",
      "Loss after mini-batch   251: 19.135\n",
      "Loss after mini-batch   261: 0.031\n",
      "Loss after mini-batch   271: 0.434\n",
      "Loss after mini-batch   281: 59.278\n",
      "Loss after mini-batch   291: 0.495\n",
      "Loss after mini-batch   301: 0.526\n",
      "Loss after mini-batch   311: 0.146\n",
      "Loss after mini-batch   321: 0.314\n",
      "Loss after mini-batch   331: 0.611\n",
      "Loss after mini-batch   341: 0.033\n",
      "Loss after mini-batch   351: 0.850\n",
      "Loss after mini-batch   361: 14.105\n",
      "Loss after mini-batch   371: 1.234\n",
      "Loss after mini-batch   381: 0.738\n",
      "Loss after mini-batch   391: 35.022\n",
      "Training Loss: 0.897 \t\t Validation Loss:1.178\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 29.397\n",
      "Loss after mini-batch    11: 1.045\n",
      "Loss after mini-batch    21: 0.739\n",
      "Loss after mini-batch    31: 0.679\n",
      "Loss after mini-batch    41: 1.022\n",
      "Loss after mini-batch    51: 1.093\n",
      "Loss after mini-batch    61: 56.158\n",
      "Loss after mini-batch    71: 0.808\n",
      "Loss after mini-batch    81: 0.338\n",
      "Loss after mini-batch    91: 8.838\n",
      "Loss after mini-batch   101: 0.423\n",
      "Loss after mini-batch   111: 0.578\n",
      "Loss after mini-batch   121: 13.221\n",
      "Loss after mini-batch   131: 3.376\n",
      "Loss after mini-batch   141: 5.410\n",
      "Loss after mini-batch   151: 34.632\n",
      "Loss after mini-batch   161: 5.001\n",
      "Loss after mini-batch   171: 5.219\n",
      "Loss after mini-batch   181: 2.453\n",
      "Loss after mini-batch   191: 2.962\n",
      "Loss after mini-batch   201: 1.007\n",
      "Loss after mini-batch   211: 58.844\n",
      "Loss after mini-batch   221: 31.853\n",
      "Loss after mini-batch   231: 0.350\n",
      "Loss after mini-batch   241: 1.677\n",
      "Loss after mini-batch   251: 2.110\n",
      "Loss after mini-batch   261: 3.194\n",
      "Loss after mini-batch   271: 55.187\n",
      "Loss after mini-batch   281: 1.686\n",
      "Loss after mini-batch   291: 0.124\n",
      "Loss after mini-batch   301: 0.286\n",
      "Loss after mini-batch   311: 3.661\n",
      "Loss after mini-batch   321: 16.044\n",
      "Loss after mini-batch   331: 14.748\n",
      "Loss after mini-batch   341: 0.697\n",
      "Loss after mini-batch   351: 12.206\n",
      "Loss after mini-batch   361: 0.399\n",
      "Loss after mini-batch   371: 41.985\n",
      "Loss after mini-batch   381: 5.066\n",
      "Loss after mini-batch   391: 33.585\n",
      "Training Loss: 0.199 \t\t Validation Loss:0.291\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.634\n",
      "Loss after mini-batch    11: 0.330\n",
      "Loss after mini-batch    21: 2.889\n",
      "Loss after mini-batch    31: 0.265\n",
      "Loss after mini-batch    41: 5.238\n",
      "Loss after mini-batch    51: 0.485\n",
      "Loss after mini-batch    61: 3.981\n",
      "Loss after mini-batch    71: 0.735\n",
      "Loss after mini-batch    81: 1.458\n",
      "Loss after mini-batch    91: 0.358\n",
      "Loss after mini-batch   101: 3.587\n",
      "Loss after mini-batch   111: 72.909\n",
      "Loss after mini-batch   121: 0.571\n",
      "Loss after mini-batch   131: 0.815\n",
      "Loss after mini-batch   141: 0.458\n",
      "Loss after mini-batch   151: 0.462\n",
      "Loss after mini-batch   161: 1.047\n",
      "Loss after mini-batch   171: 0.922\n",
      "Loss after mini-batch   181: 45.347\n",
      "Loss after mini-batch   191: 14.779\n",
      "Loss after mini-batch   201: 6.315\n",
      "Loss after mini-batch   211: 0.708\n",
      "Loss after mini-batch   221: 5.916\n",
      "Loss after mini-batch   231: 0.086\n",
      "Loss after mini-batch   241: 0.672\n",
      "Loss after mini-batch   251: 1.817\n",
      "Loss after mini-batch   261: 0.203\n",
      "Loss after mini-batch   271: 14.976\n",
      "Loss after mini-batch   281: 6.524\n",
      "Loss after mini-batch   291: 5.558\n",
      "Loss after mini-batch   301: 2.415\n",
      "Loss after mini-batch   311: 0.519\n",
      "Loss after mini-batch   321: 29.207\n",
      "Loss after mini-batch   331: 7.888\n",
      "Loss after mini-batch   341: 0.057\n",
      "Loss after mini-batch   351: 0.348\n",
      "Loss after mini-batch   361: 51.042\n",
      "Loss after mini-batch   371: 0.476\n",
      "Loss after mini-batch   381: 0.561\n",
      "Loss after mini-batch   391: 0.241\n",
      "Training Loss: 0.256 \t\t Validation Loss:0.667\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.814\n",
      "Loss after mini-batch    11: 0.388\n",
      "Loss after mini-batch    21: 0.403\n",
      "Loss after mini-batch    31: 1.358\n",
      "Loss after mini-batch    41: 0.426\n",
      "Loss after mini-batch    51: 0.679\n",
      "Loss after mini-batch    61: 0.591\n",
      "Loss after mini-batch    71: 1.190\n",
      "Loss after mini-batch    81: 0.575\n",
      "Loss after mini-batch    91: 17.980\n",
      "Loss after mini-batch   101: 57.307\n",
      "Loss after mini-batch   111: 7.014\n",
      "Loss after mini-batch   121: 6.392\n",
      "Loss after mini-batch   131: 17.954\n",
      "Loss after mini-batch   141: 1.120\n",
      "Loss after mini-batch   151: 54.871\n",
      "Loss after mini-batch   161: 1.033\n",
      "Loss after mini-batch   171: 7.603\n",
      "Loss after mini-batch   181: 25.753\n",
      "Loss after mini-batch   191: 1.040\n",
      "Loss after mini-batch   201: 1.096\n",
      "Loss after mini-batch   211: 7.244\n",
      "Loss after mini-batch   221: 50.503\n",
      "Loss after mini-batch   231: 13.786\n",
      "Loss after mini-batch   241: 0.118\n",
      "Loss after mini-batch   251: 7.259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   261: 0.565\n",
      "Loss after mini-batch   271: 0.156\n",
      "Loss after mini-batch   281: 1.368\n",
      "Loss after mini-batch   291: 0.603\n",
      "Loss after mini-batch   301: 0.714\n",
      "Loss after mini-batch   311: 13.414\n",
      "Loss after mini-batch   321: 0.739\n",
      "Loss after mini-batch   331: 0.128\n",
      "Loss after mini-batch   341: 0.250\n",
      "Loss after mini-batch   351: 0.587\n",
      "Loss after mini-batch   361: 0.647\n",
      "Loss after mini-batch   371: 0.273\n",
      "Loss after mini-batch   381: 0.598\n",
      "Loss after mini-batch   391: 21.578\n",
      "Training Loss: 22.872 \t\t Validation Loss:23.444\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 0.050\n",
      "Loss after mini-batch    11: 0.815\n",
      "Loss after mini-batch    21: 4.769\n",
      "Loss after mini-batch    31: 0.793\n",
      "Loss after mini-batch    41: 31.853\n",
      "Loss after mini-batch    51: 0.819\n",
      "Loss after mini-batch    61: 0.667\n",
      "Loss after mini-batch    71: 0.781\n",
      "Loss after mini-batch    81: 0.862\n",
      "Loss after mini-batch    91: 0.823\n",
      "Loss after mini-batch   101: 8.945\n",
      "Loss after mini-batch   111: 0.544\n",
      "Loss after mini-batch   121: 0.328\n",
      "Loss after mini-batch   131: 0.417\n",
      "Loss after mini-batch   141: 0.570\n",
      "Loss after mini-batch   151: 0.402\n",
      "Loss after mini-batch   161: 0.406\n",
      "Loss after mini-batch   171: 4.675\n",
      "Loss after mini-batch   181: 5.174\n",
      "Loss after mini-batch   191: 0.357\n",
      "Loss after mini-batch   201: 16.260\n",
      "Loss after mini-batch   211: 0.952\n",
      "Loss after mini-batch   221: 0.522\n",
      "Loss after mini-batch   231: 1.367\n",
      "Loss after mini-batch   241: 0.117\n",
      "Loss after mini-batch   251: 0.870\n",
      "Loss after mini-batch   261: 0.770\n",
      "Loss after mini-batch   271: 1.005\n",
      "Loss after mini-batch   281: 3.035\n",
      "Loss after mini-batch   291: 0.111\n",
      "Loss after mini-batch   301: 0.808\n",
      "Loss after mini-batch   311: 2.183\n",
      "Loss after mini-batch   321: 57.051\n",
      "Loss after mini-batch   331: 2.143\n",
      "Loss after mini-batch   341: 0.515\n",
      "Loss after mini-batch   351: 3.522\n",
      "Loss after mini-batch   361: 0.660\n",
      "Loss after mini-batch   371: 3.733\n",
      "Loss after mini-batch   381: 0.972\n",
      "Loss after mini-batch   391: 12.777\n",
      "Training Loss: 0.794 \t\t Validation Loss:1.026\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.661\n",
      "Loss after mini-batch    11: 56.715\n",
      "Loss after mini-batch    21: 9.052\n",
      "Loss after mini-batch    31: 3.179\n",
      "Loss after mini-batch    41: 0.073\n",
      "Loss after mini-batch    51: 7.014\n",
      "Loss after mini-batch    61: 0.269\n",
      "Loss after mini-batch    71: 15.552\n",
      "Loss after mini-batch    81: 8.941\n",
      "Loss after mini-batch    91: 35.585\n",
      "Loss after mini-batch   101: 0.017\n",
      "Loss after mini-batch   111: 41.885\n",
      "Loss after mini-batch   121: 0.433\n",
      "Loss after mini-batch   131: 0.063\n",
      "Loss after mini-batch   141: 12.956\n",
      "Loss after mini-batch   151: 0.260\n",
      "Loss after mini-batch   161: 7.635\n",
      "Loss after mini-batch   171: 0.354\n",
      "Loss after mini-batch   181: 0.252\n",
      "Loss after mini-batch   191: 0.395\n",
      "Loss after mini-batch   201: 0.148\n",
      "Loss after mini-batch   211: 7.647\n",
      "Loss after mini-batch   221: 25.632\n",
      "Loss after mini-batch   231: 1.131\n",
      "Loss after mini-batch   241: 21.041\n",
      "Loss after mini-batch   251: 1.264\n",
      "Loss after mini-batch   261: 1.093\n",
      "Loss after mini-batch   271: 1.718\n",
      "Loss after mini-batch   281: 72.462\n",
      "Loss after mini-batch   291: 1.192\n",
      "Loss after mini-batch   301: 1.342\n",
      "Loss after mini-batch   311: 1.958\n",
      "Loss after mini-batch   321: 0.378\n",
      "Loss after mini-batch   331: 1.261\n",
      "Loss after mini-batch   341: 0.633\n",
      "Loss after mini-batch   351: 1.891\n",
      "Loss after mini-batch   361: 1.193\n",
      "Loss after mini-batch   371: 0.193\n",
      "Loss after mini-batch   381: 0.664\n",
      "Loss after mini-batch   391: 0.792\n",
      "Training Loss: 0.742 \t\t Validation Loss:1.351\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.885\n",
      "Loss after mini-batch    11: 0.569\n",
      "Loss after mini-batch    21: 0.831\n",
      "Loss after mini-batch    31: 45.272\n",
      "Loss after mini-batch    41: 0.744\n",
      "Loss after mini-batch    51: 0.753\n",
      "Loss after mini-batch    61: 0.269\n",
      "Loss after mini-batch    71: 16.645\n",
      "Loss after mini-batch    81: 0.640\n",
      "Loss after mini-batch    91: 34.892\n",
      "Loss after mini-batch   101: 21.155\n",
      "Loss after mini-batch   111: 0.750\n",
      "Loss after mini-batch   121: 3.290\n",
      "Loss after mini-batch   131: 14.009\n",
      "Loss after mini-batch   141: 12.566\n",
      "Loss after mini-batch   151: 0.870\n",
      "Loss after mini-batch   161: 0.545\n",
      "Loss after mini-batch   171: 0.643\n",
      "Loss after mini-batch   181: 0.231\n",
      "Loss after mini-batch   191: 0.732\n",
      "Loss after mini-batch   201: 0.717\n",
      "Loss after mini-batch   211: 0.475\n",
      "Loss after mini-batch   221: 1.302\n",
      "Loss after mini-batch   231: 9.358\n",
      "Loss after mini-batch   241: 0.666\n",
      "Loss after mini-batch   251: 0.526\n",
      "Loss after mini-batch   261: 12.610\n",
      "Loss after mini-batch   271: 5.420\n",
      "Loss after mini-batch   281: 0.441\n",
      "Loss after mini-batch   291: 0.482\n",
      "Loss after mini-batch   301: 0.495\n",
      "Loss after mini-batch   311: 0.525\n",
      "Loss after mini-batch   321: 0.553\n",
      "Loss after mini-batch   331: 0.123\n",
      "Loss after mini-batch   341: 0.004\n",
      "Loss after mini-batch   351: 57.077\n",
      "Loss after mini-batch   361: 2.262\n",
      "Loss after mini-batch   371: 0.790\n",
      "Loss after mini-batch   381: 3.913\n",
      "Loss after mini-batch   391: 0.914\n",
      "Training Loss: 7.016 \t\t Validation Loss:7.976\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.833\n",
      "Loss after mini-batch    11: 13.942\n",
      "Loss after mini-batch    21: 0.595\n",
      "Loss after mini-batch    31: 3.095\n",
      "Loss after mini-batch    41: 0.481\n",
      "Loss after mini-batch    51: 0.262\n",
      "Loss after mini-batch    61: 0.492\n",
      "Loss after mini-batch    71: 0.268\n",
      "Loss after mini-batch    81: 0.091\n",
      "Loss after mini-batch    91: 17.308\n",
      "Loss after mini-batch   101: 0.911\n",
      "Loss after mini-batch   111: 7.639\n",
      "Loss after mini-batch   121: 0.599\n",
      "Loss after mini-batch   131: 0.469\n",
      "Loss after mini-batch   141: 0.479\n",
      "Loss after mini-batch   151: 0.465\n",
      "Loss after mini-batch   161: 0.580\n",
      "Loss after mini-batch   171: 0.321\n",
      "Loss after mini-batch   181: 0.675\n",
      "Loss after mini-batch   191: 0.393\n",
      "Loss after mini-batch   201: 9.374\n",
      "Loss after mini-batch   211: 0.465\n",
      "Loss after mini-batch   221: 0.307\n",
      "Loss after mini-batch   231: 0.189\n",
      "Loss after mini-batch   241: 0.791\n",
      "Loss after mini-batch   251: 0.643\n",
      "Loss after mini-batch   261: 4.731\n",
      "Loss after mini-batch   271: 0.724\n",
      "Loss after mini-batch   281: 1.654\n",
      "Loss after mini-batch   291: 0.020\n",
      "Loss after mini-batch   301: 0.621\n",
      "Loss after mini-batch   311: 0.151\n",
      "Loss after mini-batch   321: 97.403\n",
      "Loss after mini-batch   331: 0.619\n",
      "Loss after mini-batch   341: 0.091\n",
      "Loss after mini-batch   351: 22.625\n",
      "Loss after mini-batch   361: 1.094\n",
      "Loss after mini-batch   371: 0.641\n",
      "Loss after mini-batch   381: 0.989\n",
      "Loss after mini-batch   391: 0.761\n",
      "Training Loss: 0.223 \t\t Validation Loss:0.818\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 0.659\n",
      "Loss after mini-batch    11: 0.671\n",
      "Loss after mini-batch    21: 1.777\n",
      "Loss after mini-batch    31: 0.619\n",
      "Loss after mini-batch    41: 0.399\n",
      "Loss after mini-batch    51: 0.442\n",
      "Loss after mini-batch    61: 6.749\n",
      "Loss after mini-batch    71: 0.856\n",
      "Loss after mini-batch    81: 0.886\n",
      "Loss after mini-batch    91: 1.152\n",
      "Loss after mini-batch   101: 0.830\n",
      "Loss after mini-batch   111: 0.916\n",
      "Loss after mini-batch   121: 36.598\n",
      "Loss after mini-batch   131: 0.925\n",
      "Loss after mini-batch   141: 1.024\n",
      "Loss after mini-batch   151: 15.826\n",
      "Loss after mini-batch   161: 3.830\n",
      "Loss after mini-batch   171: 7.581\n",
      "Loss after mini-batch   181: 0.740\n",
      "Loss after mini-batch   191: 3.161\n",
      "Loss after mini-batch   201: 14.942\n",
      "Loss after mini-batch   211: 0.391\n",
      "Loss after mini-batch   221: 2.558\n",
      "Loss after mini-batch   231: 0.640\n",
      "Loss after mini-batch   241: 0.691\n",
      "Loss after mini-batch   251: 0.694\n",
      "Loss after mini-batch   261: 0.638\n",
      "Loss after mini-batch   271: 8.354\n",
      "Loss after mini-batch   281: 1.732\n",
      "Loss after mini-batch   291: 6.939\n",
      "Loss after mini-batch   301: 18.261\n",
      "Loss after mini-batch   311: 0.047\n",
      "Loss after mini-batch   321: 0.620\n",
      "Loss after mini-batch   331: 0.349\n",
      "Loss after mini-batch   341: 0.059\n",
      "Loss after mini-batch   351: 1.285\n",
      "Loss after mini-batch   361: 0.220\n",
      "Loss after mini-batch   371: 0.598\n",
      "Loss after mini-batch   381: 0.492\n",
      "Loss after mini-batch   391: 0.480\n",
      "Training Loss: 0.425 \t\t Validation Loss:34.349\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 0.819\n",
      "Loss after mini-batch    11: 0.404\n",
      "Loss after mini-batch    21: 0.307\n",
      "Loss after mini-batch    31: 7.600\n",
      "Loss after mini-batch    41: 1.230\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 1.721\n",
      "Loss after mini-batch    71: 1.250\n",
      "Loss after mini-batch    81: 0.610\n",
      "Loss after mini-batch    91: 0.623\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   101: 0.522\n",
      "Loss after mini-batch   111: 0.114\n",
      "Loss after mini-batch   121: 1.401\n",
      "Loss after mini-batch   131: 0.016\n",
      "Loss after mini-batch   141: 15.105\n",
      "Loss after mini-batch   151: 17.190\n",
      "Loss after mini-batch   161: 19.636\n",
      "Loss after mini-batch   171: 0.900\n",
      "Loss after mini-batch   181: 0.072\n",
      "Loss after mini-batch   191: 4.570\n",
      "Loss after mini-batch   201: 0.318\n",
      "Loss after mini-batch   211: 18.095\n",
      "Loss after mini-batch   221: 0.150\n",
      "Loss after mini-batch   231: 1.258\n",
      "Loss after mini-batch   241: 0.769\n",
      "Loss after mini-batch   251: 1.106\n",
      "Loss after mini-batch   261: 0.866\n",
      "Loss after mini-batch   271: 0.890\n",
      "Loss after mini-batch   281: 5.440\n",
      "Loss after mini-batch   291: 0.969\n",
      "Loss after mini-batch   301: 4.839\n",
      "Loss after mini-batch   311: 0.634\n",
      "Loss after mini-batch   321: 1.439\n",
      "Loss after mini-batch   331: 0.355\n",
      "Loss after mini-batch   341: 0.836\n",
      "Loss after mini-batch   351: 0.517\n",
      "Loss after mini-batch   361: 0.014\n",
      "Loss after mini-batch   371: 55.792\n",
      "Loss after mini-batch   381: 97.242\n",
      "Loss after mini-batch   391: 0.717\n",
      "Training Loss: 12.651 \t\t Validation Loss:13.770\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 26.779\n",
      "Loss after mini-batch    11: 0.523\n",
      "Loss after mini-batch    21: 0.384\n",
      "Loss after mini-batch    31: 0.341\n",
      "Loss after mini-batch    41: 0.843\n",
      "Loss after mini-batch    51: 4.253\n",
      "Loss after mini-batch    61: 1.842\n",
      "Loss after mini-batch    71: 0.278\n",
      "Loss after mini-batch    81: 54.783\n",
      "Loss after mini-batch    91: 0.417\n",
      "Loss after mini-batch   101: 0.364\n",
      "Loss after mini-batch   111: 55.687\n",
      "Loss after mini-batch   121: 0.159\n",
      "Loss after mini-batch   131: 2.578\n",
      "Loss after mini-batch   141: 8.609\n",
      "Loss after mini-batch   151: 0.684\n",
      "Loss after mini-batch   161: 3.070\n",
      "Loss after mini-batch   171: 7.229\n",
      "Loss after mini-batch   181: 3.852\n",
      "Loss after mini-batch   191: 0.524\n",
      "Loss after mini-batch   201: 0.590\n",
      "Loss after mini-batch   211: 0.584\n",
      "Loss after mini-batch   221: 0.064\n",
      "Loss after mini-batch   231: 4.186\n",
      "Loss after mini-batch   241: 5.478\n",
      "Loss after mini-batch   251: 2.022\n",
      "Loss after mini-batch   261: 0.568\n",
      "Loss after mini-batch   271: 0.610\n",
      "Loss after mini-batch   281: 0.004\n",
      "Loss after mini-batch   291: 13.410\n",
      "Loss after mini-batch   301: 33.015\n",
      "Loss after mini-batch   311: 0.554\n",
      "Loss after mini-batch   321: 4.622\n",
      "Loss after mini-batch   331: 5.241\n",
      "Loss after mini-batch   341: 0.409\n",
      "Loss after mini-batch   351: 0.475\n",
      "Loss after mini-batch   361: 0.697\n",
      "Loss after mini-batch   371: 0.606\n",
      "Loss after mini-batch   381: 4.037\n",
      "Loss after mini-batch   391: 1.495\n",
      "Training Loss: 7.377 \t\t Validation Loss:7.515\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 0.466\n",
      "Loss after mini-batch    11: 3.248\n",
      "Loss after mini-batch    21: 3.603\n",
      "Loss after mini-batch    31: 10.629\n",
      "Loss after mini-batch    41: 18.402\n",
      "Loss after mini-batch    51: 0.171\n",
      "Loss after mini-batch    61: 0.618\n",
      "Loss after mini-batch    71: 1.747\n",
      "Loss after mini-batch    81: 0.110\n",
      "Loss after mini-batch    91: 0.366\n",
      "Loss after mini-batch   101: 0.610\n",
      "Loss after mini-batch   111: 1.787\n",
      "Loss after mini-batch   121: 4.599\n",
      "Loss after mini-batch   131: 0.470\n",
      "Loss after mini-batch   141: 0.884\n",
      "Loss after mini-batch   151: 11.586\n",
      "Loss after mini-batch   161: 0.764\n",
      "Loss after mini-batch   171: 0.335\n",
      "Loss after mini-batch   181: 18.227\n",
      "Loss after mini-batch   191: 0.462\n",
      "Loss after mini-batch   201: 0.595\n",
      "Loss after mini-batch   211: 16.098\n",
      "Loss after mini-batch   221: 0.445\n",
      "Loss after mini-batch   231: 0.636\n",
      "Loss after mini-batch   241: 0.597\n",
      "Loss after mini-batch   251: 0.092\n",
      "Loss after mini-batch   261: 0.747\n",
      "Loss after mini-batch   271: 5.321\n",
      "Loss after mini-batch   281: 3.243\n",
      "Loss after mini-batch   291: 0.699\n",
      "Loss after mini-batch   301: 0.989\n",
      "Loss after mini-batch   311: 7.079\n",
      "Loss after mini-batch   321: 50.114\n",
      "Loss after mini-batch   331: 5.761\n",
      "Loss after mini-batch   341: 0.664\n",
      "Loss after mini-batch   351: 4.718\n",
      "Loss after mini-batch   361: 3.578\n",
      "Loss after mini-batch   371: 0.741\n",
      "Loss after mini-batch   381: 6.757\n",
      "Loss after mini-batch   391: 0.825\n",
      "Training Loss: 0.700 \t\t Validation Loss:16.133\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.421\n",
      "Loss after mini-batch    11: 0.818\n",
      "Loss after mini-batch    21: 0.430\n",
      "Loss after mini-batch    31: 0.659\n",
      "Loss after mini-batch    41: 2.773\n",
      "Loss after mini-batch    51: 0.485\n",
      "Loss after mini-batch    61: 55.517\n",
      "Loss after mini-batch    71: 6.389\n",
      "Loss after mini-batch    81: 7.395\n",
      "Loss after mini-batch    91: 0.252\n",
      "Loss after mini-batch   101: 35.912\n",
      "Loss after mini-batch   111: 17.281\n",
      "Loss after mini-batch   121: 44.803\n",
      "Loss after mini-batch   131: 0.776\n",
      "Loss after mini-batch   141: 13.612\n",
      "Loss after mini-batch   151: 1.559\n",
      "Loss after mini-batch   161: 0.110\n",
      "Loss after mini-batch   171: 13.976\n",
      "Loss after mini-batch   181: 0.383\n",
      "Loss after mini-batch   191: 4.766\n",
      "Loss after mini-batch   201: 0.354\n",
      "Loss after mini-batch   211: 0.338\n",
      "Loss after mini-batch   221: 6.521\n",
      "Loss after mini-batch   231: 19.833\n",
      "Loss after mini-batch   241: 13.124\n",
      "Loss after mini-batch   251: 0.106\n",
      "Loss after mini-batch   261: 0.842\n",
      "Loss after mini-batch   271: 0.810\n",
      "Loss after mini-batch   281: 0.924\n",
      "Loss after mini-batch   291: 0.969\n",
      "Loss after mini-batch   301: 1.023\n",
      "Loss after mini-batch   311: 20.285\n",
      "Loss after mini-batch   321: 28.527\n",
      "Loss after mini-batch   331: 9.789\n",
      "Loss after mini-batch   341: 0.384\n",
      "Loss after mini-batch   351: 8.412\n",
      "Loss after mini-batch   361: 1.266\n",
      "Loss after mini-batch   371: 56.760\n",
      "Loss after mini-batch   381: 0.429\n",
      "Loss after mini-batch   391: 0.914\n",
      "Training Loss: 0.851 \t\t Validation Loss:1.713\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 11.495\n",
      "Loss after mini-batch    11: 1.264\n",
      "Loss after mini-batch    21: 2.808\n",
      "Loss after mini-batch    31: 1.125\n",
      "Loss after mini-batch    41: 3.629\n",
      "Loss after mini-batch    51: 1.042\n",
      "Loss after mini-batch    61: 1.174\n",
      "Loss after mini-batch    71: 1.172\n",
      "Loss after mini-batch    81: 1.203\n",
      "Loss after mini-batch    91: 1.056\n",
      "Loss after mini-batch   101: 0.320\n",
      "Loss after mini-batch   111: 8.333\n",
      "Loss after mini-batch   121: 1.012\n",
      "Loss after mini-batch   131: 0.461\n",
      "Loss after mini-batch   141: 0.037\n",
      "Loss after mini-batch   151: 0.939\n",
      "Loss after mini-batch   161: 0.925\n",
      "Loss after mini-batch   171: 13.492\n",
      "Loss after mini-batch   181: 0.128\n",
      "Loss after mini-batch   191: 0.599\n",
      "Loss after mini-batch   201: 0.554\n",
      "Loss after mini-batch   211: 17.729\n",
      "Loss after mini-batch   221: 0.443\n",
      "Loss after mini-batch   231: 16.328\n",
      "Loss after mini-batch   241: 36.752\n",
      "Loss after mini-batch   251: 15.209\n",
      "Loss after mini-batch   261: 0.674\n",
      "Loss after mini-batch   271: 0.803\n",
      "Loss after mini-batch   281: 6.859\n",
      "Loss after mini-batch   291: 0.648\n",
      "Loss after mini-batch   301: 0.551\n",
      "Loss after mini-batch   311: 0.584\n",
      "Loss after mini-batch   321: 0.377\n",
      "Loss after mini-batch   331: 1.411\n",
      "Loss after mini-batch   341: 0.064\n",
      "Loss after mini-batch   351: 0.477\n",
      "Loss after mini-batch   361: 0.477\n",
      "Loss after mini-batch   371: 0.295\n",
      "Loss after mini-batch   381: 0.537\n",
      "Loss after mini-batch   391: 0.577\n",
      "Training Loss: 1.841 \t\t Validation Loss:23.382\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 0.322\n",
      "Loss after mini-batch    11: 5.876\n",
      "Loss after mini-batch    21: 5.642\n",
      "Loss after mini-batch    31: 7.150\n",
      "Loss after mini-batch    41: 26.236\n",
      "Loss after mini-batch    51: 3.049\n",
      "Loss after mini-batch    61: 13.059\n",
      "Loss after mini-batch    71: 0.715\n",
      "Loss after mini-batch    81: 0.616\n",
      "Loss after mini-batch    91: 0.830\n",
      "Loss after mini-batch   101: 0.394\n",
      "Loss after mini-batch   111: 8.026\n",
      "Loss after mini-batch   121: 0.145\n",
      "Loss after mini-batch   131: 1.669\n",
      "Loss after mini-batch   141: 0.974\n",
      "Loss after mini-batch   151: 0.582\n",
      "Loss after mini-batch   161: 0.445\n",
      "Loss after mini-batch   171: 18.344\n",
      "Loss after mini-batch   181: 0.429\n",
      "Loss after mini-batch   191: 0.159\n",
      "Loss after mini-batch   201: 0.465\n",
      "Loss after mini-batch   211: 0.158\n",
      "Loss after mini-batch   221: 38.841\n",
      "Loss after mini-batch   231: 4.598\n",
      "Loss after mini-batch   241: 0.760\n",
      "Loss after mini-batch   251: 0.586\n",
      "Loss after mini-batch   261: 34.863\n",
      "Loss after mini-batch   271: 0.500\n",
      "Loss after mini-batch   281: 0.567\n",
      "Loss after mini-batch   291: 0.672\n",
      "Loss after mini-batch   301: 0.759\n",
      "Loss after mini-batch   311: 2.762\n",
      "Loss after mini-batch   321: 0.400\n",
      "Loss after mini-batch   331: 6.605\n",
      "Loss after mini-batch   341: 16.530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   351: 2.351\n",
      "Loss after mini-batch   361: 2.070\n",
      "Loss after mini-batch   371: 0.688\n",
      "Loss after mini-batch   381: 0.618\n",
      "Loss after mini-batch   391: 0.093\n",
      "Training Loss: 0.422 \t\t Validation Loss:0.837\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 36.605\n",
      "Loss after mini-batch    11: 0.563\n",
      "Loss after mini-batch    21: 1.634\n",
      "Loss after mini-batch    31: 0.177\n",
      "Loss after mini-batch    41: 0.524\n",
      "Loss after mini-batch    51: 0.644\n",
      "Loss after mini-batch    61: 0.233\n",
      "Loss after mini-batch    71: 15.115\n",
      "Loss after mini-batch    81: 35.091\n",
      "Loss after mini-batch    91: 55.343\n",
      "Loss after mini-batch   101: 0.432\n",
      "Loss after mini-batch   111: 0.090\n",
      "Loss after mini-batch   121: 0.125\n",
      "Loss after mini-batch   131: 0.228\n",
      "Loss after mini-batch   141: 0.522\n",
      "Loss after mini-batch   151: 31.319\n",
      "Loss after mini-batch   161: 0.635\n",
      "Loss after mini-batch   171: 0.586\n",
      "Loss after mini-batch   181: 11.498\n",
      "Loss after mini-batch   191: 0.608\n",
      "Loss after mini-batch   201: 3.564\n",
      "Loss after mini-batch   211: 0.762\n",
      "Loss after mini-batch   221: 0.794\n",
      "Loss after mini-batch   231: 16.958\n",
      "Loss after mini-batch   241: 0.601\n",
      "Loss after mini-batch   251: 0.585\n",
      "Loss after mini-batch   261: 0.528\n",
      "Loss after mini-batch   271: 5.370\n",
      "Loss after mini-batch   281: 0.394\n",
      "Loss after mini-batch   291: 0.863\n",
      "Loss after mini-batch   301: 0.037\n",
      "Loss after mini-batch   311: 0.319\n",
      "Loss after mini-batch   321: 13.324\n",
      "Loss after mini-batch   331: 71.702\n",
      "Loss after mini-batch   341: 0.024\n",
      "Loss after mini-batch   351: 0.657\n",
      "Loss after mini-batch   361: 0.495\n",
      "Loss after mini-batch   371: 7.692\n",
      "Loss after mini-batch   381: 0.517\n",
      "Loss after mini-batch   391: 2.605\n",
      "Training Loss: 0.898 \t\t Validation Loss:1.115\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 2.088\n",
      "Loss after mini-batch    11: 0.156\n",
      "Loss after mini-batch    21: 0.469\n",
      "Loss after mini-batch    31: 0.540\n",
      "Loss after mini-batch    41: 35.057\n",
      "Loss after mini-batch    51: 0.570\n",
      "Loss after mini-batch    61: 0.546\n",
      "Loss after mini-batch    71: 6.994\n",
      "Loss after mini-batch    81: 2.778\n",
      "Loss after mini-batch    91: 0.472\n",
      "Loss after mini-batch   101: 0.210\n",
      "Loss after mini-batch   111: 1.527\n",
      "Loss after mini-batch   121: 0.797\n",
      "Loss after mini-batch   131: 0.329\n",
      "Loss after mini-batch   141: 0.680\n",
      "Loss after mini-batch   151: 0.012\n",
      "Loss after mini-batch   161: 3.952\n",
      "Loss after mini-batch   171: 2.884\n",
      "Loss after mini-batch   181: 0.994\n",
      "Loss after mini-batch   191: 3.704\n",
      "Loss after mini-batch   201: 0.703\n",
      "Loss after mini-batch   211: 0.938\n",
      "Loss after mini-batch   221: 13.297\n",
      "Loss after mini-batch   231: 11.998\n",
      "Loss after mini-batch   241: 0.383\n",
      "Loss after mini-batch   251: 7.263\n",
      "Loss after mini-batch   261: 8.564\n",
      "Loss after mini-batch   271: 0.701\n",
      "Loss after mini-batch   281: 0.712\n",
      "Loss after mini-batch   291: 20.285\n",
      "Loss after mini-batch   301: 0.428\n",
      "Loss after mini-batch   311: 3.482\n",
      "Loss after mini-batch   321: 0.586\n",
      "Loss after mini-batch   331: 15.675\n",
      "Loss after mini-batch   341: 1.895\n",
      "Loss after mini-batch   351: 5.193\n",
      "Loss after mini-batch   361: 0.563\n",
      "Loss after mini-batch   371: 0.579\n",
      "Loss after mini-batch   381: 6.654\n",
      "Loss after mini-batch   391: 45.307\n",
      "Training Loss: 56.308 \t\t Validation Loss:56.451\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.357\n",
      "Loss after mini-batch    11: 0.552\n",
      "Loss after mini-batch    21: 0.400\n",
      "Loss after mini-batch    31: 6.708\n",
      "Loss after mini-batch    41: 0.979\n",
      "Loss after mini-batch    51: 3.052\n",
      "Loss after mini-batch    61: 0.780\n",
      "Loss after mini-batch    71: 4.441\n",
      "Loss after mini-batch    81: 0.391\n",
      "Loss after mini-batch    91: 0.616\n",
      "Loss after mini-batch   101: 1.915\n",
      "Loss after mini-batch   111: 0.205\n",
      "Loss after mini-batch   121: 0.352\n",
      "Loss after mini-batch   131: 0.067\n",
      "Loss after mini-batch   141: 0.475\n",
      "Loss after mini-batch   151: 35.542\n",
      "Loss after mini-batch   161: 0.345\n",
      "Loss after mini-batch   171: 35.164\n",
      "Loss after mini-batch   181: 0.269\n",
      "Loss after mini-batch   191: 0.019\n",
      "Loss after mini-batch   201: 9.297\n",
      "Loss after mini-batch   211: 0.471\n",
      "Loss after mini-batch   221: 0.372\n",
      "Loss after mini-batch   231: 0.293\n",
      "Loss after mini-batch   241: 0.658\n",
      "Loss after mini-batch   251: 0.567\n",
      "Loss after mini-batch   261: 13.009\n",
      "Loss after mini-batch   271: 0.263\n",
      "Loss after mini-batch   281: 0.769\n",
      "Loss after mini-batch   291: 0.565\n",
      "Loss after mini-batch   301: 0.165\n",
      "Loss after mini-batch   311: 3.083\n",
      "Loss after mini-batch   321: 0.182\n",
      "Loss after mini-batch   331: 1.013\n",
      "Loss after mini-batch   341: 0.977\n",
      "Loss after mini-batch   351: 0.807\n",
      "Loss after mini-batch   361: 0.273\n",
      "Loss after mini-batch   371: 0.531\n",
      "Loss after mini-batch   381: 0.279\n",
      "Loss after mini-batch   391: 7.229\n",
      "Training Loss: 0.396 \t\t Validation Loss:0.743\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 49.576\n",
      "Loss after mini-batch    11: 0.044\n",
      "Loss after mini-batch    21: 11.796\n",
      "Loss after mini-batch    31: 0.262\n",
      "Loss after mini-batch    41: 1.006\n",
      "Loss after mini-batch    51: 3.577\n",
      "Loss after mini-batch    61: 0.658\n",
      "Loss after mini-batch    71: 4.477\n",
      "Loss after mini-batch    81: 0.696\n",
      "Loss after mini-batch    91: 54.969\n",
      "Loss after mini-batch   101: 0.542\n",
      "Loss after mini-batch   111: 1.952\n",
      "Loss after mini-batch   121: 2.567\n",
      "Loss after mini-batch   131: 20.593\n",
      "Loss after mini-batch   141: 58.925\n",
      "Loss after mini-batch   151: 44.474\n",
      "Loss after mini-batch   161: 0.421\n",
      "Loss after mini-batch   171: 0.712\n",
      "Loss after mini-batch   181: 3.207\n",
      "Loss after mini-batch   191: 2.132\n",
      "Loss after mini-batch   201: 5.073\n",
      "Loss after mini-batch   211: 0.856\n",
      "Loss after mini-batch   221: 6.362\n",
      "Loss after mini-batch   231: 0.079\n",
      "Loss after mini-batch   241: 0.380\n",
      "Loss after mini-batch   251: 0.767\n",
      "Loss after mini-batch   261: 3.007\n",
      "Loss after mini-batch   271: 16.639\n",
      "Loss after mini-batch   281: 0.050\n",
      "Loss after mini-batch   291: 4.327\n",
      "Loss after mini-batch   301: 0.584\n",
      "Loss after mini-batch   311: 0.552\n",
      "Loss after mini-batch   321: 0.071\n",
      "Loss after mini-batch   331: 9.352\n",
      "Loss after mini-batch   341: 0.038\n",
      "Loss after mini-batch   351: 0.719\n",
      "Loss after mini-batch   361: 0.246\n",
      "Loss after mini-batch   371: 24.356\n",
      "Loss after mini-batch   381: 1.501\n",
      "Loss after mini-batch   391: 38.406\n",
      "Training Loss: 6.634 \t\t Validation Loss:7.134\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.365\n",
      "Loss after mini-batch    11: 0.517\n",
      "Loss after mini-batch    21: 0.332\n",
      "Loss after mini-batch    31: 0.205\n",
      "Loss after mini-batch    41: 8.752\n",
      "Loss after mini-batch    51: 0.004\n",
      "Loss after mini-batch    61: 0.358\n",
      "Loss after mini-batch    71: 0.137\n",
      "Loss after mini-batch    81: 0.656\n",
      "Loss after mini-batch    91: 12.942\n",
      "Loss after mini-batch   101: 0.580\n",
      "Loss after mini-batch   111: 0.656\n",
      "Loss after mini-batch   121: 4.347\n",
      "Loss after mini-batch   131: 2.980\n",
      "Loss after mini-batch   141: 0.604\n",
      "Loss after mini-batch   151: 2.519\n",
      "Loss after mini-batch   161: 0.644\n",
      "Loss after mini-batch   171: 36.264\n",
      "Loss after mini-batch   181: 0.444\n",
      "Loss after mini-batch   191: 7.331\n",
      "Loss after mini-batch   201: 0.526\n",
      "Loss after mini-batch   211: 0.205\n",
      "Loss after mini-batch   221: 7.110\n",
      "Loss after mini-batch   231: 34.385\n",
      "Loss after mini-batch   241: 1.049\n",
      "Loss after mini-batch   251: 58.132\n",
      "Loss after mini-batch   261: 0.896\n",
      "Loss after mini-batch   271: 1.149\n",
      "Loss after mini-batch   281: 16.404\n",
      "Loss after mini-batch   291: 0.943\n",
      "Loss after mini-batch   301: 6.508\n",
      "Loss after mini-batch   311: 0.362\n",
      "Loss after mini-batch   321: 0.929\n",
      "Loss after mini-batch   331: 11.220\n",
      "Loss after mini-batch   341: 0.753\n",
      "Loss after mini-batch   351: 0.789\n",
      "Loss after mini-batch   361: 9.229\n",
      "Loss after mini-batch   371: 0.550\n",
      "Loss after mini-batch   381: 0.368\n",
      "Loss after mini-batch   391: 21.711\n",
      "Training Loss: 0.059 \t\t Validation Loss:0.540\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 0.378\n",
      "Loss after mini-batch    11: 18.734\n",
      "Loss after mini-batch    21: 0.748\n",
      "Loss after mini-batch    31: 0.185\n",
      "Loss after mini-batch    41: 0.149\n",
      "Loss after mini-batch    51: 0.399\n",
      "Loss after mini-batch    61: 11.256\n",
      "Loss after mini-batch    71: 3.224\n",
      "Loss after mini-batch    81: 0.555\n",
      "Loss after mini-batch    91: 0.647\n",
      "Loss after mini-batch   101: 3.188\n",
      "Loss after mini-batch   111: 0.606\n",
      "Loss after mini-batch   121: 4.192\n",
      "Loss after mini-batch   131: 19.793\n",
      "Loss after mini-batch   141: 0.725\n",
      "Loss after mini-batch   151: 0.498\n",
      "Loss after mini-batch   161: 0.470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   171: 1.178\n",
      "Loss after mini-batch   181: 2.996\n",
      "Loss after mini-batch   191: 0.603\n",
      "Loss after mini-batch   201: 0.363\n",
      "Loss after mini-batch   211: 0.632\n",
      "Loss after mini-batch   221: 0.227\n",
      "Loss after mini-batch   231: 0.616\n",
      "Loss after mini-batch   241: 3.133\n",
      "Loss after mini-batch   251: 2.911\n",
      "Loss after mini-batch   261: 0.067\n",
      "Loss after mini-batch   271: 6.826\n",
      "Loss after mini-batch   281: 4.006\n",
      "Loss after mini-batch   291: 5.438\n",
      "Loss after mini-batch   301: 0.682\n",
      "Loss after mini-batch   311: 11.187\n",
      "Loss after mini-batch   321: 7.149\n",
      "Loss after mini-batch   331: 0.533\n",
      "Loss after mini-batch   341: 0.187\n",
      "Loss after mini-batch   351: 0.615\n",
      "Loss after mini-batch   361: 0.581\n",
      "Loss after mini-batch   371: 54.674\n",
      "Loss after mini-batch   381: 0.296\n",
      "Loss after mini-batch   391: 0.688\n",
      "Training Loss: 40.836 \t\t Validation Loss:41.210\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.168\n",
      "Loss after mini-batch    11: 35.135\n",
      "Loss after mini-batch    21: 0.252\n",
      "Loss after mini-batch    31: 20.526\n",
      "Loss after mini-batch    41: 0.235\n",
      "Loss after mini-batch    51: 0.326\n",
      "Loss after mini-batch    61: 0.368\n",
      "Loss after mini-batch    71: 3.424\n",
      "Loss after mini-batch    81: 0.053\n",
      "Loss after mini-batch    91: 5.431\n",
      "Loss after mini-batch   101: 0.427\n",
      "Loss after mini-batch   111: 0.089\n",
      "Loss after mini-batch   121: 0.393\n",
      "Loss after mini-batch   131: 6.684\n",
      "Loss after mini-batch   141: 1.783\n",
      "Loss after mini-batch   151: 7.284\n",
      "Loss after mini-batch   161: 0.240\n",
      "Loss after mini-batch   171: 58.724\n",
      "Loss after mini-batch   181: 3.053\n",
      "Loss after mini-batch   191: 0.449\n",
      "Loss after mini-batch   201: 0.707\n",
      "Loss after mini-batch   211: 0.674\n",
      "Loss after mini-batch   221: 25.723\n",
      "Loss after mini-batch   231: 0.845\n",
      "Loss after mini-batch   241: 0.171\n",
      "Loss after mini-batch   251: 0.063\n",
      "Loss after mini-batch   261: 0.625\n",
      "Loss after mini-batch   271: 7.256\n",
      "Loss after mini-batch   281: 0.529\n",
      "Loss after mini-batch   291: 95.540\n",
      "Loss after mini-batch   301: 0.636\n",
      "Loss after mini-batch   311: 16.364\n",
      "Loss after mini-batch   321: 15.472\n",
      "Loss after mini-batch   331: 0.866\n",
      "Loss after mini-batch   341: 54.165\n",
      "Loss after mini-batch   351: 0.888\n",
      "Loss after mini-batch   361: 0.941\n",
      "Loss after mini-batch   371: 12.716\n",
      "Loss after mini-batch   381: 0.757\n",
      "Loss after mini-batch   391: 0.470\n",
      "Training Loss: 9.055 \t\t Validation Loss:9.496\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 34.440\n",
      "Loss after mini-batch    11: 12.888\n",
      "Loss after mini-batch    21: 0.772\n",
      "Loss after mini-batch    31: 0.628\n",
      "Loss after mini-batch    41: 1.017\n",
      "Loss after mini-batch    51: 1.260\n",
      "Loss after mini-batch    61: 0.758\n",
      "Loss after mini-batch    71: 2.425\n",
      "Loss after mini-batch    81: 0.610\n",
      "Loss after mini-batch    91: 8.768\n",
      "Loss after mini-batch   101: 0.062\n",
      "Loss after mini-batch   111: 3.260\n",
      "Loss after mini-batch   121: 0.482\n",
      "Loss after mini-batch   131: 0.625\n",
      "Loss after mini-batch   141: 0.550\n",
      "Loss after mini-batch   151: 0.616\n",
      "Loss after mini-batch   161: 0.568\n",
      "Loss after mini-batch   171: 8.720\n",
      "Loss after mini-batch   181: 0.900\n",
      "Loss after mini-batch   191: 0.554\n",
      "Loss after mini-batch   201: 0.467\n",
      "Loss after mini-batch   211: 0.194\n",
      "Loss after mini-batch   221: 2.303\n",
      "Loss after mini-batch   231: 0.504\n",
      "Loss after mini-batch   241: 0.000\n",
      "Loss after mini-batch   251: 0.373\n",
      "Loss after mini-batch   261: 6.908\n",
      "Loss after mini-batch   271: 0.028\n",
      "Loss after mini-batch   281: 0.386\n",
      "Loss after mini-batch   291: 4.620\n",
      "Loss after mini-batch   301: 0.871\n",
      "Loss after mini-batch   311: 0.381\n",
      "Loss after mini-batch   321: 0.539\n",
      "Loss after mini-batch   331: 8.664\n",
      "Loss after mini-batch   341: 0.107\n",
      "Loss after mini-batch   351: 0.629\n",
      "Loss after mini-batch   361: 11.816\n",
      "Loss after mini-batch   371: 55.917\n",
      "Loss after mini-batch   381: 8.929\n",
      "Loss after mini-batch   391: 9.609\n",
      "Training Loss: 23.482 \t\t Validation Loss:24.101\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 48.346\n",
      "Loss after mini-batch    11: 2.788\n",
      "Loss after mini-batch    21: 0.796\n",
      "Loss after mini-batch    31: 4.271\n",
      "Loss after mini-batch    41: 0.902\n",
      "Loss after mini-batch    51: 0.137\n",
      "Loss after mini-batch    61: 40.220\n",
      "Loss after mini-batch    71: 23.920\n",
      "Loss after mini-batch    81: 0.246\n",
      "Loss after mini-batch    91: 0.935\n",
      "Loss after mini-batch   101: 0.909\n",
      "Loss after mini-batch   111: 0.878\n",
      "Loss after mini-batch   121: 5.755\n",
      "Loss after mini-batch   131: 0.206\n",
      "Loss after mini-batch   141: 4.103\n",
      "Loss after mini-batch   151: 4.800\n",
      "Loss after mini-batch   161: 8.879\n",
      "Loss after mini-batch   171: 0.730\n",
      "Loss after mini-batch   181: 30.427\n",
      "Loss after mini-batch   191: 0.590\n",
      "Loss after mini-batch   201: 10.102\n",
      "Loss after mini-batch   211: 0.816\n",
      "Loss after mini-batch   221: 0.236\n",
      "Loss after mini-batch   231: 3.262\n",
      "Loss after mini-batch   241: 0.266\n",
      "Loss after mini-batch   251: 0.192\n",
      "Loss after mini-batch   261: 0.309\n",
      "Loss after mini-batch   271: 1.267\n",
      "Loss after mini-batch   281: 3.121\n",
      "Loss after mini-batch   291: 0.505\n",
      "Loss after mini-batch   301: 2.593\n",
      "Loss after mini-batch   311: 55.328\n",
      "Loss after mini-batch   321: 0.259\n",
      "Loss after mini-batch   331: 0.694\n",
      "Loss after mini-batch   341: 0.059\n",
      "Loss after mini-batch   351: 0.587\n",
      "Loss after mini-batch   361: 0.547\n",
      "Loss after mini-batch   371: 0.297\n",
      "Loss after mini-batch   381: 2.969\n",
      "Loss after mini-batch   391: 0.517\n",
      "Training Loss: 0.103 \t\t Validation Loss:0.536\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.212\n",
      "Loss after mini-batch    11: 0.428\n",
      "Loss after mini-batch    21: 0.473\n",
      "Loss after mini-batch    31: 0.411\n",
      "Loss after mini-batch    41: 0.338\n",
      "Loss after mini-batch    51: 0.440\n",
      "Loss after mini-batch    61: 57.113\n",
      "Loss after mini-batch    71: 0.300\n",
      "Loss after mini-batch    81: 0.252\n",
      "Loss after mini-batch    91: 0.565\n",
      "Loss after mini-batch   101: 0.449\n",
      "Loss after mini-batch   111: 0.401\n",
      "Loss after mini-batch   121: 14.672\n",
      "Loss after mini-batch   131: 0.556\n",
      "Loss after mini-batch   141: 4.409\n",
      "Loss after mini-batch   151: 43.729\n",
      "Loss after mini-batch   161: 0.654\n",
      "Loss after mini-batch   171: 3.069\n",
      "Loss after mini-batch   181: 12.805\n",
      "Loss after mini-batch   191: 5.822\n",
      "Loss after mini-batch   201: 8.465\n",
      "Loss after mini-batch   211: 0.862\n",
      "Loss after mini-batch   221: 1.001\n",
      "Loss after mini-batch   231: 53.915\n",
      "Loss after mini-batch   241: 0.099\n",
      "Loss after mini-batch   251: 1.958\n",
      "Loss after mini-batch   261: 0.887\n",
      "Loss after mini-batch   271: 1.194\n",
      "Loss after mini-batch   281: 6.420\n",
      "Loss after mini-batch   291: 0.995\n",
      "Loss after mini-batch   301: 6.812\n",
      "Loss after mini-batch   311: 6.635\n",
      "Loss after mini-batch   321: 0.443\n",
      "Loss after mini-batch   331: 0.301\n",
      "Loss after mini-batch   341: 0.429\n",
      "Loss after mini-batch   351: 0.362\n",
      "Loss after mini-batch   361: 0.283\n",
      "Loss after mini-batch   371: 0.062\n",
      "Loss after mini-batch   381: 0.133\n",
      "Loss after mini-batch   391: 0.370\n",
      "Training Loss: 0.083 \t\t Validation Loss:0.273\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 0.034\n",
      "Loss after mini-batch    11: 3.717\n",
      "Loss after mini-batch    21: 0.468\n",
      "Loss after mini-batch    31: 34.576\n",
      "Loss after mini-batch    41: 0.386\n",
      "Loss after mini-batch    51: 2.519\n",
      "Loss after mini-batch    61: 0.583\n",
      "Loss after mini-batch    71: 0.352\n",
      "Loss after mini-batch    81: 10.923\n",
      "Loss after mini-batch    91: 0.134\n",
      "Loss after mini-batch   101: 1.177\n",
      "Loss after mini-batch   111: 13.920\n",
      "Loss after mini-batch   121: 0.301\n",
      "Loss after mini-batch   131: 4.150\n",
      "Loss after mini-batch   141: 0.811\n",
      "Loss after mini-batch   151: 37.437\n",
      "Loss after mini-batch   161: 2.328\n",
      "Loss after mini-batch   171: 0.064\n",
      "Loss after mini-batch   181: 0.463\n",
      "Loss after mini-batch   191: 0.461\n",
      "Loss after mini-batch   201: 18.468\n",
      "Loss after mini-batch   211: 0.430\n",
      "Loss after mini-batch   221: 0.271\n",
      "Loss after mini-batch   231: 0.196\n",
      "Loss after mini-batch   241: 0.630\n",
      "Loss after mini-batch   251: 0.200\n",
      "Loss after mini-batch   261: 0.639\n",
      "Loss after mini-batch   271: 0.325\n",
      "Loss after mini-batch   281: 6.606\n",
      "Loss after mini-batch   291: 2.531\n",
      "Loss after mini-batch   301: 0.022\n",
      "Loss after mini-batch   311: 1.070\n",
      "Loss after mini-batch   321: 1.714\n",
      "Loss after mini-batch   331: 0.480\n",
      "Loss after mini-batch   341: 0.454\n",
      "Loss after mini-batch   351: 9.885\n",
      "Loss after mini-batch   361: 16.341\n",
      "Loss after mini-batch   371: 2.709\n",
      "Loss after mini-batch   381: 0.715\n",
      "Loss after mini-batch   391: 0.470\n",
      "Training Loss: 0.664 \t\t Validation Loss:0.691\n",
      "Starting epoch 29\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch     1: 0.807\n",
      "Loss after mini-batch    11: 0.790\n",
      "Loss after mini-batch    21: 0.181\n",
      "Loss after mini-batch    31: 0.157\n",
      "Loss after mini-batch    41: 94.815\n",
      "Loss after mini-batch    51: 0.377\n",
      "Loss after mini-batch    61: 0.284\n",
      "Loss after mini-batch    71: 14.784\n",
      "Loss after mini-batch    81: 0.148\n",
      "Loss after mini-batch    91: 0.190\n",
      "Loss after mini-batch   101: 1.302\n",
      "Loss after mini-batch   111: 3.370\n",
      "Loss after mini-batch   121: 4.541\n",
      "Loss after mini-batch   131: 0.464\n",
      "Loss after mini-batch   141: 4.029\n",
      "Loss after mini-batch   151: 0.603\n",
      "Loss after mini-batch   161: 9.318\n",
      "Loss after mini-batch   171: 0.428\n",
      "Loss after mini-batch   181: 0.401\n",
      "Loss after mini-batch   191: 0.460\n",
      "Loss after mini-batch   201: 0.226\n",
      "Loss after mini-batch   211: 16.511\n",
      "Loss after mini-batch   221: 0.486\n",
      "Loss after mini-batch   231: 1.876\n",
      "Loss after mini-batch   241: 0.017\n",
      "Loss after mini-batch   251: 1.695\n",
      "Loss after mini-batch   261: 0.314\n",
      "Loss after mini-batch   271: 0.202\n",
      "Loss after mini-batch   281: 9.200\n",
      "Loss after mini-batch   291: 0.808\n",
      "Loss after mini-batch   301: 0.282\n",
      "Loss after mini-batch   311: 0.517\n",
      "Loss after mini-batch   321: 30.274\n",
      "Loss after mini-batch   331: 12.632\n",
      "Loss after mini-batch   341: 0.208\n",
      "Loss after mini-batch   351: 0.960\n",
      "Loss after mini-batch   361: 15.529\n",
      "Loss after mini-batch   371: 43.143\n",
      "Loss after mini-batch   381: 0.278\n",
      "Loss after mini-batch   391: 0.869\n",
      "Training Loss: 0.784 \t\t Validation Loss:5.647\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 17.595\n",
      "Loss after mini-batch    11: 3.543\n",
      "Loss after mini-batch    21: 0.934\n",
      "Loss after mini-batch    31: 2.631\n",
      "Loss after mini-batch    41: 1.027\n",
      "Loss after mini-batch    51: 0.522\n",
      "Loss after mini-batch    61: 56.646\n",
      "Loss after mini-batch    71: 0.415\n",
      "Loss after mini-batch    81: 0.577\n",
      "Loss after mini-batch    91: 0.592\n",
      "Loss after mini-batch   101: 0.151\n",
      "Loss after mini-batch   111: 0.277\n",
      "Loss after mini-batch   121: 9.021\n",
      "Loss after mini-batch   131: 2.537\n",
      "Loss after mini-batch   141: 2.489\n",
      "Loss after mini-batch   151: 0.671\n",
      "Loss after mini-batch   161: 0.437\n",
      "Loss after mini-batch   171: 34.021\n",
      "Loss after mini-batch   181: 0.527\n",
      "Loss after mini-batch   191: 1.969\n",
      "Loss after mini-batch   201: 5.566\n",
      "Loss after mini-batch   211: 8.704\n",
      "Loss after mini-batch   221: 20.913\n",
      "Loss after mini-batch   231: 0.380\n",
      "Loss after mini-batch   241: 0.570\n",
      "Loss after mini-batch   251: 0.376\n",
      "Loss after mini-batch   261: 2.972\n",
      "Loss after mini-batch   271: 0.161\n",
      "Loss after mini-batch   281: 0.480\n",
      "Loss after mini-batch   291: 0.397\n",
      "Loss after mini-batch   301: 0.032\n",
      "Loss after mini-batch   311: 0.347\n",
      "Loss after mini-batch   321: 3.702\n",
      "Loss after mini-batch   331: 0.424\n",
      "Loss after mini-batch   341: 0.222\n",
      "Loss after mini-batch   351: 0.096\n",
      "Loss after mini-batch   361: 16.040\n",
      "Loss after mini-batch   371: 33.786\n",
      "Loss after mini-batch   381: 1.034\n",
      "Loss after mini-batch   391: 0.439\n",
      "Training Loss: 0.668 \t\t Validation Loss:11.400\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 8.292\n",
      "Loss after mini-batch    11: 35.440\n",
      "Loss after mini-batch    21: 0.366\n",
      "Loss after mini-batch    31: 0.280\n",
      "Loss after mini-batch    41: 2.342\n",
      "Loss after mini-batch    51: 0.526\n",
      "Loss after mini-batch    61: 0.507\n",
      "Loss after mini-batch    71: 0.376\n",
      "Loss after mini-batch    81: 0.241\n",
      "Loss after mini-batch    91: 0.579\n",
      "Loss after mini-batch   101: 24.029\n",
      "Loss after mini-batch   111: 0.505\n",
      "Loss after mini-batch   121: 2.549\n",
      "Loss after mini-batch   131: 0.424\n",
      "Loss after mini-batch   141: 0.600\n",
      "Loss after mini-batch   151: 5.850\n",
      "Loss after mini-batch   161: 0.359\n",
      "Loss after mini-batch   171: 0.321\n",
      "Loss after mini-batch   181: 0.258\n",
      "Loss after mini-batch   191: 0.238\n",
      "Loss after mini-batch   201: 14.567\n",
      "Loss after mini-batch   211: 9.618\n",
      "Loss after mini-batch   221: 4.314\n",
      "Loss after mini-batch   231: 0.103\n",
      "Loss after mini-batch   241: 18.499\n",
      "Loss after mini-batch   251: 0.354\n",
      "Loss after mini-batch   261: 0.208\n",
      "Loss after mini-batch   271: 0.277\n",
      "Loss after mini-batch   281: 15.691\n",
      "Loss after mini-batch   291: 0.015\n",
      "Loss after mini-batch   301: 0.361\n",
      "Loss after mini-batch   311: 2.495\n",
      "Loss after mini-batch   321: 6.573\n",
      "Loss after mini-batch   331: 0.693\n",
      "Loss after mini-batch   341: 0.741\n",
      "Loss after mini-batch   351: 0.281\n",
      "Loss after mini-batch   361: 0.306\n",
      "Loss after mini-batch   371: 0.235\n",
      "Loss after mini-batch   381: 1.497\n",
      "Loss after mini-batch   391: 0.584\n",
      "Training Loss: 5.339 \t\t Validation Loss:5.845\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 6.401\n",
      "Loss after mini-batch    11: 7.612\n",
      "Loss after mini-batch    21: 0.043\n",
      "Loss after mini-batch    31: 4.169\n",
      "Loss after mini-batch    41: 5.513\n",
      "Loss after mini-batch    51: 69.644\n",
      "Loss after mini-batch    61: 4.734\n",
      "Loss after mini-batch    71: 0.706\n",
      "Loss after mini-batch    81: 3.395\n",
      "Loss after mini-batch    91: 1.030\n",
      "Loss after mini-batch   101: 12.823\n",
      "Loss after mini-batch   111: 0.198\n",
      "Loss after mini-batch   121: 1.140\n",
      "Loss after mini-batch   131: 1.873\n",
      "Loss after mini-batch   141: 0.740\n",
      "Loss after mini-batch   151: 15.606\n",
      "Loss after mini-batch   161: 0.904\n",
      "Loss after mini-batch   171: 1.583\n",
      "Loss after mini-batch   181: 6.219\n",
      "Loss after mini-batch   191: 0.137\n",
      "Loss after mini-batch   201: 0.219\n",
      "Loss after mini-batch   211: 4.217\n",
      "Loss after mini-batch   221: 22.227\n",
      "Loss after mini-batch   231: 0.070\n",
      "Loss after mini-batch   241: 57.979\n",
      "Loss after mini-batch   251: 0.262\n",
      "Loss after mini-batch   261: 0.248\n",
      "Loss after mini-batch   271: 0.037\n",
      "Loss after mini-batch   281: 0.250\n",
      "Loss after mini-batch   291: 0.651\n",
      "Loss after mini-batch   301: 0.228\n",
      "Loss after mini-batch   311: 0.616\n",
      "Loss after mini-batch   321: 40.087\n",
      "Loss after mini-batch   331: 15.463\n",
      "Loss after mini-batch   341: 5.646\n",
      "Loss after mini-batch   351: 0.627\n",
      "Loss after mini-batch   361: 13.090\n",
      "Loss after mini-batch   371: 0.126\n",
      "Loss after mini-batch   381: 0.617\n",
      "Loss after mini-batch   391: 25.152\n",
      "Training Loss: 0.044 \t\t Validation Loss:2.045\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 0.629\n",
      "Loss after mini-batch    11: 6.295\n",
      "Loss after mini-batch    21: 0.241\n",
      "Loss after mini-batch    31: 0.434\n",
      "Loss after mini-batch    41: 0.323\n",
      "Loss after mini-batch    51: 45.624\n",
      "Loss after mini-batch    61: 0.451\n",
      "Loss after mini-batch    71: 3.616\n",
      "Loss after mini-batch    81: 0.188\n",
      "Loss after mini-batch    91: 0.371\n",
      "Loss after mini-batch   101: 0.578\n",
      "Loss after mini-batch   111: 2.711\n",
      "Loss after mini-batch   121: 0.099\n",
      "Loss after mini-batch   131: 2.058\n",
      "Loss after mini-batch   141: 0.483\n",
      "Loss after mini-batch   151: 5.604\n",
      "Loss after mini-batch   161: 0.555\n",
      "Loss after mini-batch   171: 13.509\n",
      "Loss after mini-batch   181: 7.069\n",
      "Loss after mini-batch   191: 6.146\n",
      "Loss after mini-batch   201: 0.694\n",
      "Loss after mini-batch   211: 46.932\n",
      "Loss after mini-batch   221: 7.983\n",
      "Loss after mini-batch   231: 0.283\n",
      "Loss after mini-batch   241: 1.142\n",
      "Loss after mini-batch   251: 1.124\n",
      "Loss after mini-batch   261: 0.618\n",
      "Loss after mini-batch   271: 0.667\n",
      "Loss after mini-batch   281: 0.417\n",
      "Loss after mini-batch   291: 0.025\n",
      "Loss after mini-batch   301: 0.430\n",
      "Loss after mini-batch   311: 17.669\n",
      "Loss after mini-batch   321: 0.667\n",
      "Loss after mini-batch   331: 0.289\n",
      "Loss after mini-batch   341: 0.112\n",
      "Loss after mini-batch   351: 33.660\n",
      "Loss after mini-batch   361: 21.963\n",
      "Loss after mini-batch   371: 0.053\n",
      "Loss after mini-batch   381: 0.252\n",
      "Loss after mini-batch   391: 0.258\n",
      "Training Loss: 52.607 \t\t Validation Loss:71.681\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 1.905\n",
      "Loss after mini-batch    11: 0.255\n",
      "Loss after mini-batch    21: 1.523\n",
      "Loss after mini-batch    31: 3.130\n",
      "Loss after mini-batch    41: 0.141\n",
      "Loss after mini-batch    51: 0.358\n",
      "Loss after mini-batch    61: 1.043\n",
      "Loss after mini-batch    71: 0.393\n",
      "Loss after mini-batch    81: 0.545\n",
      "Loss after mini-batch    91: 7.561\n",
      "Loss after mini-batch   101: 0.468\n",
      "Loss after mini-batch   111: 0.473\n",
      "Loss after mini-batch   121: 4.263\n",
      "Loss after mini-batch   131: 0.496\n",
      "Loss after mini-batch   141: 0.641\n",
      "Loss after mini-batch   151: 0.696\n",
      "Loss after mini-batch   161: 2.915\n",
      "Loss after mini-batch   171: 0.033\n",
      "Loss after mini-batch   181: 43.050\n",
      "Loss after mini-batch   191: 0.269\n",
      "Loss after mini-batch   201: 0.186\n",
      "Loss after mini-batch   211: 0.520\n",
      "Loss after mini-batch   221: 22.115\n",
      "Loss after mini-batch   231: 0.325\n",
      "Loss after mini-batch   241: 93.608\n",
      "Loss after mini-batch   251: 0.352\n",
      "Loss after mini-batch   261: 8.693\n",
      "Loss after mini-batch   271: 1.556\n",
      "Loss after mini-batch   281: 0.786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   291: 0.341\n",
      "Loss after mini-batch   301: 5.030\n",
      "Loss after mini-batch   311: 53.953\n",
      "Loss after mini-batch   321: 0.463\n",
      "Loss after mini-batch   331: 3.912\n",
      "Loss after mini-batch   341: 0.164\n",
      "Loss after mini-batch   351: 0.778\n",
      "Loss after mini-batch   361: 0.442\n",
      "Loss after mini-batch   371: 0.360\n",
      "Loss after mini-batch   381: 0.574\n",
      "Loss after mini-batch   391: 0.316\n",
      "Training Loss: 17.089 \t\t Validation Loss:17.463\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 2.233\n",
      "Loss after mini-batch    11: 2.284\n",
      "Loss after mini-batch    21: 0.323\n",
      "Loss after mini-batch    31: 0.340\n",
      "Loss after mini-batch    41: 0.314\n",
      "Loss after mini-batch    51: 2.784\n",
      "Loss after mini-batch    61: 0.485\n",
      "Loss after mini-batch    71: 0.320\n",
      "Loss after mini-batch    81: 0.481\n",
      "Loss after mini-batch    91: 0.206\n",
      "Loss after mini-batch   101: 0.284\n",
      "Loss after mini-batch   111: 0.430\n",
      "Loss after mini-batch   121: 0.244\n",
      "Loss after mini-batch   131: 1.893\n",
      "Loss after mini-batch   141: 13.537\n",
      "Loss after mini-batch   151: 0.351\n",
      "Loss after mini-batch   161: 7.116\n",
      "Loss after mini-batch   171: 13.919\n",
      "Loss after mini-batch   181: 19.072\n",
      "Loss after mini-batch   191: 0.936\n",
      "Loss after mini-batch   201: 1.939\n",
      "Loss after mini-batch   211: 11.515\n",
      "Loss after mini-batch   221: 0.617\n",
      "Loss after mini-batch   231: 93.153\n",
      "Loss after mini-batch   241: 12.969\n",
      "Loss after mini-batch   251: 1.827\n",
      "Loss after mini-batch   261: 6.791\n",
      "Loss after mini-batch   271: 57.407\n",
      "Loss after mini-batch   281: 0.124\n",
      "Loss after mini-batch   291: 0.583\n",
      "Loss after mini-batch   301: 0.024\n",
      "Loss after mini-batch   311: 0.530\n",
      "Loss after mini-batch   321: 6.600\n",
      "Loss after mini-batch   331: 0.366\n",
      "Loss after mini-batch   341: 0.238\n",
      "Loss after mini-batch   351: 0.148\n",
      "Loss after mini-batch   361: 33.097\n",
      "Loss after mini-batch   371: 0.491\n",
      "Loss after mini-batch   381: 0.703\n",
      "Loss after mini-batch   391: 0.425\n",
      "Training Loss: 0.450 \t\t Validation Loss:0.749\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 0.383\n",
      "Loss after mini-batch    11: 1.858\n",
      "Loss after mini-batch    21: 0.334\n",
      "Loss after mini-batch    31: 14.091\n",
      "Loss after mini-batch    41: 0.515\n",
      "Loss after mini-batch    51: 1.003\n",
      "Loss after mini-batch    61: 6.296\n",
      "Loss after mini-batch    71: 2.183\n",
      "Loss after mini-batch    81: 0.276\n",
      "Loss after mini-batch    91: 57.156\n",
      "Loss after mini-batch   101: 3.622\n",
      "Loss after mini-batch   111: 0.057\n",
      "Loss after mini-batch   121: 0.539\n",
      "Loss after mini-batch   131: 0.490\n",
      "Loss after mini-batch   141: 6.187\n",
      "Loss after mini-batch   151: 0.981\n",
      "Loss after mini-batch   161: 0.610\n",
      "Loss after mini-batch   171: 0.920\n",
      "Loss after mini-batch   181: 0.935\n",
      "Loss after mini-batch   191: 1.213\n",
      "Loss after mini-batch   201: 11.177\n",
      "Loss after mini-batch   211: 0.622\n",
      "Loss after mini-batch   221: 0.672\n",
      "Loss after mini-batch   231: 0.500\n",
      "Loss after mini-batch   241: 0.494\n",
      "Loss after mini-batch   251: 1.956\n",
      "Loss after mini-batch   261: 0.219\n",
      "Loss after mini-batch   271: 0.127\n",
      "Loss after mini-batch   281: 1.730\n",
      "Loss after mini-batch   291: 0.519\n",
      "Loss after mini-batch   301: 0.097\n",
      "Loss after mini-batch   311: 0.057\n",
      "Loss after mini-batch   321: 3.084\n",
      "Loss after mini-batch   331: 0.331\n",
      "Loss after mini-batch   341: 0.352\n",
      "Loss after mini-batch   351: 0.437\n",
      "Loss after mini-batch   361: 0.483\n",
      "Loss after mini-batch   371: 0.371\n",
      "Loss after mini-batch   381: 0.905\n",
      "Loss after mini-batch   391: 0.502\n",
      "Training Loss: 0.624 \t\t Validation Loss:3.596\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 0.377\n",
      "Loss after mini-batch    11: 0.427\n",
      "Loss after mini-batch    21: 0.465\n",
      "Loss after mini-batch    31: 2.225\n",
      "Loss after mini-batch    41: 0.453\n",
      "Loss after mini-batch    51: 0.730\n",
      "Loss after mini-batch    61: 4.775\n",
      "Loss after mini-batch    71: 0.485\n",
      "Loss after mini-batch    81: 0.287\n",
      "Loss after mini-batch    91: 0.433\n",
      "Loss after mini-batch   101: 0.488\n",
      "Loss after mini-batch   111: 1.683\n",
      "Loss after mini-batch   121: 6.167\n",
      "Loss after mini-batch   131: 0.276\n",
      "Loss after mini-batch   141: 1.237\n",
      "Loss after mini-batch   151: 3.092\n",
      "Loss after mini-batch   161: 0.265\n",
      "Loss after mini-batch   171: 20.058\n",
      "Loss after mini-batch   181: 7.263\n",
      "Loss after mini-batch   191: 16.477\n",
      "Loss after mini-batch   201: 0.078\n",
      "Loss after mini-batch   211: 0.066\n",
      "Loss after mini-batch   221: 0.639\n",
      "Loss after mini-batch   231: 6.498\n",
      "Loss after mini-batch   241: 0.103\n",
      "Loss after mini-batch   251: 0.226\n",
      "Loss after mini-batch   261: 51.784\n",
      "Loss after mini-batch   271: 0.412\n",
      "Loss after mini-batch   281: 12.343\n",
      "Loss after mini-batch   291: 0.160\n",
      "Loss after mini-batch   301: 0.544\n",
      "Loss after mini-batch   311: 2.094\n",
      "Loss after mini-batch   321: 3.225\n",
      "Loss after mini-batch   331: 32.153\n",
      "Loss after mini-batch   341: 0.561\n",
      "Loss after mini-batch   351: 0.389\n",
      "Loss after mini-batch   361: 0.592\n",
      "Loss after mini-batch   371: 0.986\n",
      "Loss after mini-batch   381: 1.806\n",
      "Loss after mini-batch   391: 0.198\n",
      "Training Loss: 0.193 \t\t Validation Loss:2.587\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 0.826\n",
      "Loss after mini-batch    11: 0.530\n",
      "Loss after mini-batch    21: 21.310\n",
      "Loss after mini-batch    31: 0.733\n",
      "Loss after mini-batch    41: 0.150\n",
      "Loss after mini-batch    51: 13.094\n",
      "Loss after mini-batch    61: 92.749\n",
      "Loss after mini-batch    71: 0.384\n",
      "Loss after mini-batch    81: 52.488\n",
      "Loss after mini-batch    91: 0.379\n",
      "Loss after mini-batch   101: 0.396\n",
      "Loss after mini-batch   111: 0.338\n",
      "Loss after mini-batch   121: 3.690\n",
      "Loss after mini-batch   131: 0.154\n",
      "Loss after mini-batch   141: 0.176\n",
      "Loss after mini-batch   151: 0.263\n",
      "Loss after mini-batch   161: 0.494\n",
      "Loss after mini-batch   171: 0.106\n",
      "Loss after mini-batch   181: 0.219\n",
      "Loss after mini-batch   191: 0.200\n",
      "Loss after mini-batch   201: 1.850\n",
      "Loss after mini-batch   211: 0.061\n",
      "Loss after mini-batch   221: 0.210\n",
      "Loss after mini-batch   231: 0.339\n",
      "Loss after mini-batch   241: 0.151\n",
      "Loss after mini-batch   251: 0.419\n",
      "Loss after mini-batch   261: 0.416\n",
      "Loss after mini-batch   271: 5.765\n",
      "Loss after mini-batch   281: 46.862\n",
      "Loss after mini-batch   291: 0.339\n",
      "Loss after mini-batch   301: 5.490\n",
      "Loss after mini-batch   311: 0.545\n",
      "Loss after mini-batch   321: 0.096\n",
      "Loss after mini-batch   331: 0.254\n",
      "Loss after mini-batch   341: 14.052\n",
      "Loss after mini-batch   351: 0.290\n",
      "Loss after mini-batch   361: 0.356\n",
      "Loss after mini-batch   371: 0.387\n",
      "Loss after mini-batch   381: 0.076\n",
      "Loss after mini-batch   391: 12.473\n",
      "Training Loss: 0.056 \t\t Validation Loss:0.970\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 0.897\n",
      "Loss after mini-batch    11: 0.808\n",
      "Loss after mini-batch    21: 0.093\n",
      "Loss after mini-batch    31: 4.251\n",
      "Loss after mini-batch    41: 0.843\n",
      "Loss after mini-batch    51: 15.110\n",
      "Loss after mini-batch    61: 1.724\n",
      "Loss after mini-batch    71: 0.476\n",
      "Loss after mini-batch    81: 0.523\n",
      "Loss after mini-batch    91: 14.446\n",
      "Loss after mini-batch   101: 0.940\n",
      "Loss after mini-batch   111: 0.410\n",
      "Loss after mini-batch   121: 6.286\n",
      "Loss after mini-batch   131: 0.206\n",
      "Loss after mini-batch   141: 0.632\n",
      "Loss after mini-batch   151: 0.646\n",
      "Loss after mini-batch   161: 0.385\n",
      "Loss after mini-batch   171: 0.163\n",
      "Loss after mini-batch   181: 0.312\n",
      "Loss after mini-batch   191: 0.055\n",
      "Loss after mini-batch   201: 0.342\n",
      "Loss after mini-batch   211: 0.214\n",
      "Loss after mini-batch   221: 7.461\n",
      "Loss after mini-batch   231: 0.516\n",
      "Loss after mini-batch   241: 0.304\n",
      "Loss after mini-batch   251: 1.205\n",
      "Loss after mini-batch   261: 32.406\n",
      "Loss after mini-batch   271: 0.497\n",
      "Loss after mini-batch   281: 19.523\n",
      "Loss after mini-batch   291: 0.527\n",
      "Loss after mini-batch   301: 6.891\n",
      "Loss after mini-batch   311: 0.677\n",
      "Loss after mini-batch   321: 7.747\n",
      "Loss after mini-batch   331: 0.187\n",
      "Loss after mini-batch   341: 1.673\n",
      "Loss after mini-batch   351: 0.343\n",
      "Loss after mini-batch   361: 0.426\n",
      "Loss after mini-batch   371: 24.950\n",
      "Loss after mini-batch   381: 0.586\n",
      "Loss after mini-batch   391: 0.550\n",
      "Training Loss: 8.821 \t\t Validation Loss:13.384\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 7.245\n",
      "Loss after mini-batch    11: 0.302\n",
      "Loss after mini-batch    21: 14.072\n",
      "Loss after mini-batch    31: 0.735\n",
      "Loss after mini-batch    41: 0.607\n",
      "Loss after mini-batch    51: 0.442\n",
      "Loss after mini-batch    61: 41.956\n",
      "Loss after mini-batch    71: 0.308\n",
      "Loss after mini-batch    81: 0.443\n",
      "Loss after mini-batch    91: 52.969\n",
      "Loss after mini-batch   101: 0.508\n",
      "Loss after mini-batch   111: 0.950\n",
      "Loss after mini-batch   121: 2.881\n",
      "Loss after mini-batch   131: 1.746\n",
      "Loss after mini-batch   141: 9.281\n",
      "Loss after mini-batch   151: 2.472\n",
      "Loss after mini-batch   161: 0.360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   171: 0.529\n",
      "Loss after mini-batch   181: 0.691\n",
      "Loss after mini-batch   191: 0.485\n",
      "Loss after mini-batch   201: 0.012\n",
      "Loss after mini-batch   211: 0.645\n",
      "Loss after mini-batch   221: 0.052\n",
      "Loss after mini-batch   231: 0.492\n",
      "Loss after mini-batch   241: 0.301\n",
      "Loss after mini-batch   251: 0.019\n",
      "Loss after mini-batch   261: 0.336\n",
      "Loss after mini-batch   271: 0.791\n",
      "Loss after mini-batch   281: 0.274\n",
      "Loss after mini-batch   291: 0.260\n",
      "Loss after mini-batch   301: 13.016\n",
      "Loss after mini-batch   311: 44.237\n",
      "Loss after mini-batch   321: 0.362\n",
      "Loss after mini-batch   331: 0.271\n",
      "Loss after mini-batch   341: 0.321\n",
      "Loss after mini-batch   351: 0.647\n",
      "Loss after mini-batch   361: 0.642\n",
      "Loss after mini-batch   371: 41.703\n",
      "Loss after mini-batch   381: 0.259\n",
      "Loss after mini-batch   391: 0.132\n",
      "Training Loss: 2.343 \t\t Validation Loss:2.709\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 0.429\n",
      "Loss after mini-batch    11: 8.053\n",
      "Loss after mini-batch    21: 2.005\n",
      "Loss after mini-batch    31: 0.185\n",
      "Loss after mini-batch    41: 12.577\n",
      "Loss after mini-batch    51: 67.806\n",
      "Loss after mini-batch    61: 4.403\n",
      "Loss after mini-batch    71: 0.401\n",
      "Loss after mini-batch    81: 0.250\n",
      "Loss after mini-batch    91: 46.384\n",
      "Loss after mini-batch   101: 0.239\n",
      "Loss after mini-batch   111: 0.138\n",
      "Loss after mini-batch   121: 2.799\n",
      "Loss after mini-batch   131: 0.367\n",
      "Loss after mini-batch   141: 0.205\n",
      "Loss after mini-batch   151: 0.213\n",
      "Loss after mini-batch   161: 0.030\n",
      "Loss after mini-batch   171: 0.660\n",
      "Loss after mini-batch   181: 0.225\n",
      "Loss after mini-batch   191: 0.481\n",
      "Loss after mini-batch   201: 0.412\n",
      "Loss after mini-batch   211: 0.143\n",
      "Loss after mini-batch   221: 1.970\n",
      "Loss after mini-batch   231: 0.184\n",
      "Loss after mini-batch   241: 15.648\n",
      "Loss after mini-batch   251: 5.439\n",
      "Loss after mini-batch   261: 0.449\n",
      "Loss after mini-batch   271: 8.712\n",
      "Loss after mini-batch   281: 0.536\n",
      "Loss after mini-batch   291: 13.142\n",
      "Loss after mini-batch   301: 3.755\n",
      "Loss after mini-batch   311: 3.125\n",
      "Loss after mini-batch   321: 0.490\n",
      "Loss after mini-batch   331: 3.913\n",
      "Loss after mini-batch   341: 8.265\n",
      "Loss after mini-batch   351: 0.286\n",
      "Loss after mini-batch   361: 0.372\n",
      "Loss after mini-batch   371: 0.518\n",
      "Loss after mini-batch   381: 2.705\n",
      "Loss after mini-batch   391: 0.383\n",
      "Training Loss: 0.118 \t\t Validation Loss:5.654\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 0.373\n",
      "Loss after mini-batch    11: 0.440\n",
      "Loss after mini-batch    21: 19.033\n",
      "Loss after mini-batch    31: 0.126\n",
      "Loss after mini-batch    41: 0.110\n",
      "Loss after mini-batch    51: 0.015\n",
      "Loss after mini-batch    61: 0.356\n",
      "Loss after mini-batch    71: 0.509\n",
      "Loss after mini-batch    81: 0.555\n",
      "Loss after mini-batch    91: 7.941\n",
      "Loss after mini-batch   101: 0.444\n",
      "Loss after mini-batch   111: 7.943\n",
      "Loss after mini-batch   121: 0.767\n",
      "Loss after mini-batch   131: 0.672\n",
      "Loss after mini-batch   141: 0.652\n",
      "Loss after mini-batch   151: 1.467\n",
      "Loss after mini-batch   161: 3.928\n",
      "Loss after mini-batch   171: 91.043\n",
      "Loss after mini-batch   181: 0.593\n",
      "Loss after mini-batch   191: 0.360\n",
      "Loss after mini-batch   201: 0.638\n",
      "Loss after mini-batch   211: 0.792\n",
      "Loss after mini-batch   221: 0.315\n",
      "Loss after mini-batch   231: 2.193\n",
      "Loss after mini-batch   241: 0.062\n",
      "Loss after mini-batch   251: 0.294\n",
      "Loss after mini-batch   261: 5.684\n",
      "Loss after mini-batch   271: 0.735\n",
      "Loss after mini-batch   281: 0.647\n",
      "Loss after mini-batch   291: 0.248\n",
      "Loss after mini-batch   301: 2.672\n",
      "Loss after mini-batch   311: 0.176\n",
      "Loss after mini-batch   321: 0.219\n",
      "Loss after mini-batch   331: 0.325\n",
      "Loss after mini-batch   341: 0.036\n",
      "Loss after mini-batch   351: 5.303\n",
      "Loss after mini-batch   361: 0.276\n",
      "Loss after mini-batch   371: 34.328\n",
      "Loss after mini-batch   381: 0.509\n",
      "Loss after mini-batch   391: 18.974\n",
      "Training Loss: 0.251 \t\t Validation Loss:0.404\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 0.556\n",
      "Loss after mini-batch    11: 13.257\n",
      "Loss after mini-batch    21: 0.447\n",
      "Loss after mini-batch    31: 4.336\n",
      "Loss after mini-batch    41: 0.209\n",
      "Loss after mini-batch    51: 8.214\n",
      "Loss after mini-batch    61: 1.466\n",
      "Loss after mini-batch    71: 0.047\n",
      "Loss after mini-batch    81: 0.894\n",
      "Loss after mini-batch    91: 1.109\n",
      "Loss after mini-batch   101: 0.510\n",
      "Loss after mini-batch   111: 0.877\n",
      "Loss after mini-batch   121: 51.185\n",
      "Loss after mini-batch   131: 31.773\n",
      "Loss after mini-batch   141: 23.796\n",
      "Loss after mini-batch   151: 7.084\n",
      "Loss after mini-batch   161: 0.061\n",
      "Loss after mini-batch   171: 0.092\n",
      "Loss after mini-batch   181: 0.223\n",
      "Loss after mini-batch   191: 0.111\n",
      "Loss after mini-batch   201: 0.011\n",
      "Loss after mini-batch   211: 0.813\n",
      "Loss after mini-batch   221: 0.202\n",
      "Loss after mini-batch   231: 0.149\n",
      "Loss after mini-batch   241: 6.076\n",
      "Loss after mini-batch   251: 0.711\n",
      "Loss after mini-batch   261: 3.321\n",
      "Loss after mini-batch   271: 0.363\n",
      "Loss after mini-batch   281: 0.188\n",
      "Loss after mini-batch   291: 19.669\n",
      "Loss after mini-batch   301: 0.177\n",
      "Loss after mini-batch   311: 0.089\n",
      "Loss after mini-batch   321: 0.182\n",
      "Loss after mini-batch   331: 0.003\n",
      "Loss after mini-batch   341: 1.650\n",
      "Loss after mini-batch   351: 0.200\n",
      "Loss after mini-batch   361: 0.593\n",
      "Loss after mini-batch   371: 14.767\n",
      "Loss after mini-batch   381: 5.688\n",
      "Loss after mini-batch   391: 9.110\n",
      "Training Loss: 0.606 \t\t Validation Loss:8.662\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 2.406\n",
      "Loss after mini-batch    11: 1.858\n",
      "Loss after mini-batch    21: 0.202\n",
      "Loss after mini-batch    31: 3.511\n",
      "Loss after mini-batch    41: 0.071\n",
      "Loss after mini-batch    51: 0.083\n",
      "Loss after mini-batch    61: 32.096\n",
      "Loss after mini-batch    71: 0.421\n",
      "Loss after mini-batch    81: 6.038\n",
      "Loss after mini-batch    91: 0.509\n",
      "Loss after mini-batch   101: 5.343\n",
      "Loss after mini-batch   111: 0.339\n",
      "Loss after mini-batch   121: 43.514\n",
      "Loss after mini-batch   131: 0.013\n",
      "Loss after mini-batch   141: 1.230\n",
      "Loss after mini-batch   151: 0.963\n",
      "Loss after mini-batch   161: 0.274\n",
      "Loss after mini-batch   171: 0.091\n",
      "Loss after mini-batch   181: 51.858\n",
      "Loss after mini-batch   191: 0.390\n",
      "Loss after mini-batch   201: 0.188\n",
      "Loss after mini-batch   211: 0.728\n",
      "Loss after mini-batch   221: 0.231\n",
      "Loss after mini-batch   231: 0.260\n",
      "Loss after mini-batch   241: 1.666\n",
      "Loss after mini-batch   251: 0.161\n",
      "Loss after mini-batch   261: 3.309\n",
      "Loss after mini-batch   271: 0.282\n",
      "Loss after mini-batch   281: 0.441\n",
      "Loss after mini-batch   291: 1.288\n",
      "Loss after mini-batch   301: 0.057\n",
      "Loss after mini-batch   311: 0.193\n",
      "Loss after mini-batch   321: 0.426\n",
      "Loss after mini-batch   331: 0.006\n",
      "Loss after mini-batch   341: 0.565\n",
      "Loss after mini-batch   351: 2.344\n",
      "Loss after mini-batch   361: 55.950\n",
      "Loss after mini-batch   371: 0.306\n",
      "Loss after mini-batch   381: 0.207\n",
      "Loss after mini-batch   391: 0.601\n",
      "Training Loss: 3.855 \t\t Validation Loss:4.357\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 55.962\n",
      "Loss after mini-batch    11: 1.043\n",
      "Loss after mini-batch    21: 11.568\n",
      "Loss after mini-batch    31: 1.001\n",
      "Loss after mini-batch    41: 4.247\n",
      "Loss after mini-batch    51: 31.564\n",
      "Loss after mini-batch    61: 16.702\n",
      "Loss after mini-batch    71: 1.075\n",
      "Loss after mini-batch    81: 0.291\n",
      "Loss after mini-batch    91: 6.080\n",
      "Loss after mini-batch   101: 0.357\n",
      "Loss after mini-batch   111: 0.658\n",
      "Loss after mini-batch   121: 13.281\n",
      "Loss after mini-batch   131: 0.707\n",
      "Loss after mini-batch   141: 0.213\n",
      "Loss after mini-batch   151: 66.639\n",
      "Loss after mini-batch   161: 0.839\n",
      "Loss after mini-batch   171: 0.308\n",
      "Loss after mini-batch   181: 0.515\n",
      "Loss after mini-batch   191: 0.278\n",
      "Loss after mini-batch   201: 12.715\n",
      "Loss after mini-batch   211: 0.357\n",
      "Loss after mini-batch   221: 0.061\n",
      "Loss after mini-batch   231: 0.404\n",
      "Loss after mini-batch   241: 6.573\n",
      "Loss after mini-batch   251: 0.358\n",
      "Loss after mini-batch   261: 0.123\n",
      "Loss after mini-batch   271: 3.359\n",
      "Loss after mini-batch   281: 0.001\n",
      "Loss after mini-batch   291: 0.112\n",
      "Loss after mini-batch   301: 0.420\n",
      "Loss after mini-batch   311: 0.061\n",
      "Loss after mini-batch   321: 1.347\n",
      "Loss after mini-batch   331: 15.342\n",
      "Loss after mini-batch   341: 0.440\n",
      "Loss after mini-batch   351: 0.042\n",
      "Loss after mini-batch   361: 0.130\n",
      "Loss after mini-batch   371: 1.943\n",
      "Loss after mini-batch   381: 23.644\n",
      "Loss after mini-batch   391: 8.929\n",
      "Training Loss: 4.374 \t\t Validation Loss:15.482\n",
      "Starting epoch 46\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch     1: 0.155\n",
      "Loss after mini-batch    11: 1.115\n",
      "Loss after mini-batch    21: 5.763\n",
      "Loss after mini-batch    31: 50.634\n",
      "Loss after mini-batch    41: 0.572\n",
      "Loss after mini-batch    51: 5.118\n",
      "Loss after mini-batch    61: 0.535\n",
      "Loss after mini-batch    71: 1.380\n",
      "Loss after mini-batch    81: 55.021\n",
      "Loss after mini-batch    91: 0.261\n",
      "Loss after mini-batch   101: 0.509\n",
      "Loss after mini-batch   111: 57.657\n",
      "Loss after mini-batch   121: 0.342\n",
      "Loss after mini-batch   131: 0.091\n",
      "Loss after mini-batch   141: 0.436\n",
      "Loss after mini-batch   151: 12.028\n",
      "Loss after mini-batch   161: 0.156\n",
      "Loss after mini-batch   171: 4.934\n",
      "Loss after mini-batch   181: 0.081\n",
      "Loss after mini-batch   191: 0.537\n",
      "Loss after mini-batch   201: 0.009\n",
      "Loss after mini-batch   211: 0.611\n",
      "Loss after mini-batch   221: 0.516\n",
      "Loss after mini-batch   231: 0.566\n",
      "Loss after mini-batch   241: 4.482\n",
      "Loss after mini-batch   251: 0.415\n",
      "Loss after mini-batch   261: 0.250\n",
      "Loss after mini-batch   271: 0.209\n",
      "Loss after mini-batch   281: 0.058\n",
      "Loss after mini-batch   291: 0.795\n",
      "Loss after mini-batch   301: 0.505\n",
      "Loss after mini-batch   311: 0.520\n",
      "Loss after mini-batch   321: 0.172\n",
      "Loss after mini-batch   331: 0.119\n",
      "Loss after mini-batch   341: 0.563\n",
      "Loss after mini-batch   351: 0.085\n",
      "Loss after mini-batch   361: 0.352\n",
      "Loss after mini-batch   371: 0.726\n",
      "Loss after mini-batch   381: 0.276\n",
      "Loss after mini-batch   391: 0.482\n",
      "Training Loss: 0.352 \t\t Validation Loss:0.736\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 54.543\n",
      "Loss after mini-batch    11: 4.408\n",
      "Loss after mini-batch    21: 0.406\n",
      "Loss after mini-batch    31: 0.401\n",
      "Loss after mini-batch    41: 13.970\n",
      "Loss after mini-batch    51: 0.071\n",
      "Loss after mini-batch    61: 0.248\n",
      "Loss after mini-batch    71: 0.363\n",
      "Loss after mini-batch    81: 0.404\n",
      "Loss after mini-batch    91: 13.299\n",
      "Loss after mini-batch   101: 0.563\n",
      "Loss after mini-batch   111: 0.521\n",
      "Loss after mini-batch   121: 37.705\n",
      "Loss after mini-batch   131: 0.429\n",
      "Loss after mini-batch   141: 0.537\n",
      "Loss after mini-batch   151: 0.481\n",
      "Loss after mini-batch   161: 1.486\n",
      "Loss after mini-batch   171: 0.495\n",
      "Loss after mini-batch   181: 12.634\n",
      "Loss after mini-batch   191: 11.805\n",
      "Loss after mini-batch   201: 12.153\n",
      "Loss after mini-batch   211: 7.463\n",
      "Loss after mini-batch   221: 0.687\n",
      "Loss after mini-batch   231: 0.082\n",
      "Loss after mini-batch   241: 0.106\n",
      "Loss after mini-batch   251: 0.774\n",
      "Loss after mini-batch   261: 0.470\n",
      "Loss after mini-batch   271: 0.130\n",
      "Loss after mini-batch   281: 1.944\n",
      "Loss after mini-batch   291: 0.430\n",
      "Loss after mini-batch   301: 51.110\n",
      "Loss after mini-batch   311: 0.130\n",
      "Loss after mini-batch   321: 0.290\n",
      "Loss after mini-batch   331: 0.950\n",
      "Loss after mini-batch   341: 0.130\n",
      "Loss after mini-batch   351: 0.286\n",
      "Loss after mini-batch   361: 0.532\n",
      "Loss after mini-batch   371: 1.531\n",
      "Loss after mini-batch   381: 0.184\n",
      "Loss after mini-batch   391: 0.789\n",
      "Training Loss: 0.096 \t\t Validation Loss:0.319\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.009\n",
      "Loss after mini-batch    11: 0.070\n",
      "Loss after mini-batch    21: 0.253\n",
      "Loss after mini-batch    31: 0.105\n",
      "Loss after mini-batch    41: 33.092\n",
      "Loss after mini-batch    51: 1.656\n",
      "Loss after mini-batch    61: 0.300\n",
      "Loss after mini-batch    71: 0.086\n",
      "Loss after mini-batch    81: 2.807\n",
      "Loss after mini-batch    91: 1.613\n",
      "Loss after mini-batch   101: 2.378\n",
      "Loss after mini-batch   111: 1.397\n",
      "Loss after mini-batch   121: 1.380\n",
      "Loss after mini-batch   131: 0.235\n",
      "Loss after mini-batch   141: 2.310\n",
      "Loss after mini-batch   151: 1.403\n",
      "Loss after mini-batch   161: 5.571\n",
      "Loss after mini-batch   171: 2.853\n",
      "Loss after mini-batch   181: 0.823\n",
      "Loss after mini-batch   191: 0.032\n",
      "Loss after mini-batch   201: 0.007\n",
      "Loss after mini-batch   211: 0.110\n",
      "Loss after mini-batch   221: 23.156\n",
      "Loss after mini-batch   231: 21.457\n",
      "Loss after mini-batch   241: 42.533\n",
      "Loss after mini-batch   251: 55.834\n",
      "Loss after mini-batch   261: 0.376\n",
      "Loss after mini-batch   271: 0.130\n",
      "Loss after mini-batch   281: 0.230\n",
      "Loss after mini-batch   291: 0.122\n",
      "Loss after mini-batch   301: 0.554\n",
      "Loss after mini-batch   311: 0.327\n",
      "Loss after mini-batch   321: 4.862\n",
      "Loss after mini-batch   331: 0.504\n",
      "Loss after mini-batch   341: 4.544\n",
      "Loss after mini-batch   351: 0.010\n",
      "Loss after mini-batch   361: 0.286\n",
      "Loss after mini-batch   371: 4.503\n",
      "Loss after mini-batch   381: 0.773\n",
      "Loss after mini-batch   391: 0.291\n",
      "Training Loss: 0.623 \t\t Validation Loss:0.827\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 0.048\n",
      "Loss after mini-batch    11: 2.517\n",
      "Loss after mini-batch    21: 0.308\n",
      "Loss after mini-batch    31: 0.329\n",
      "Loss after mini-batch    41: 0.191\n",
      "Loss after mini-batch    51: 0.164\n",
      "Loss after mini-batch    61: 0.069\n",
      "Loss after mini-batch    71: 6.300\n",
      "Loss after mini-batch    81: 0.709\n",
      "Loss after mini-batch    91: 1.784\n",
      "Loss after mini-batch   101: 0.589\n",
      "Loss after mini-batch   111: 1.610\n",
      "Loss after mini-batch   121: 0.978\n",
      "Loss after mini-batch   131: 0.457\n",
      "Loss after mini-batch   141: 11.168\n",
      "Loss after mini-batch   151: 0.123\n",
      "Loss after mini-batch   161: 0.790\n",
      "Loss after mini-batch   171: 0.371\n",
      "Loss after mini-batch   181: 0.516\n",
      "Loss after mini-batch   191: 42.425\n",
      "Loss after mini-batch   201: 11.956\n",
      "Loss after mini-batch   211: 0.595\n",
      "Loss after mini-batch   221: 13.169\n",
      "Loss after mini-batch   231: 4.844\n",
      "Loss after mini-batch   241: 1.067\n",
      "Loss after mini-batch   251: 0.498\n",
      "Loss after mini-batch   261: 0.240\n",
      "Loss after mini-batch   271: 0.275\n",
      "Loss after mini-batch   281: 50.074\n",
      "Loss after mini-batch   291: 11.622\n",
      "Loss after mini-batch   301: 0.283\n",
      "Loss after mini-batch   311: 1.188\n",
      "Loss after mini-batch   321: 0.295\n",
      "Loss after mini-batch   331: 0.298\n",
      "Loss after mini-batch   341: 0.328\n",
      "Loss after mini-batch   351: 11.796\n",
      "Loss after mini-batch   361: 0.222\n",
      "Loss after mini-batch   371: 0.484\n",
      "Loss after mini-batch   381: 20.046\n",
      "Loss after mini-batch   391: 12.200\n",
      "Training Loss: 0.303 \t\t Validation Loss:70.220\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 1.452\n",
      "Loss after mini-batch    11: 0.776\n",
      "Loss after mini-batch    21: 0.283\n",
      "Loss after mini-batch    31: 1.058\n",
      "Loss after mini-batch    41: 17.742\n",
      "Loss after mini-batch    51: 1.676\n",
      "Loss after mini-batch    61: 0.098\n",
      "Loss after mini-batch    71: 7.301\n",
      "Loss after mini-batch    81: 0.161\n",
      "Loss after mini-batch    91: 32.639\n",
      "Loss after mini-batch   101: 0.865\n",
      "Loss after mini-batch   111: 3.942\n",
      "Loss after mini-batch   121: 1.069\n",
      "Loss after mini-batch   131: 1.942\n",
      "Loss after mini-batch   141: 11.151\n",
      "Loss after mini-batch   151: 0.221\n",
      "Loss after mini-batch   161: 0.759\n",
      "Loss after mini-batch   171: 5.791\n",
      "Loss after mini-batch   181: 39.882\n",
      "Loss after mini-batch   191: 0.296\n",
      "Loss after mini-batch   201: 0.286\n",
      "Loss after mini-batch   211: 0.694\n",
      "Loss after mini-batch   221: 2.985\n",
      "Loss after mini-batch   231: 0.051\n",
      "Loss after mini-batch   241: 29.692\n",
      "Loss after mini-batch   251: 48.604\n",
      "Loss after mini-batch   261: 13.283\n",
      "Loss after mini-batch   271: 0.111\n",
      "Loss after mini-batch   281: 0.084\n",
      "Loss after mini-batch   291: 7.741\n",
      "Loss after mini-batch   301: 0.623\n",
      "Loss after mini-batch   311: 4.486\n",
      "Loss after mini-batch   321: 1.806\n",
      "Loss after mini-batch   331: 0.141\n",
      "Loss after mini-batch   341: 0.060\n",
      "Loss after mini-batch   351: 12.236\n",
      "Loss after mini-batch   361: 19.961\n",
      "Loss after mini-batch   371: 31.852\n",
      "Loss after mini-batch   381: 0.474\n",
      "Loss after mini-batch   391: 0.366\n",
      "Training Loss: 0.262 \t\t Validation Loss:0.469\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "history_train = np.empty((1,))\n",
    "history_val = np.empty((1,))\n",
    "nepochs=50\n",
    "for epoch in range(0, nepochs): # 5 epochs at maximum  \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "          # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "#                 (i + 1, current_loss / 500))\n",
    "                  (i + 1, loss.item()))\n",
    "            current_loss = 0.0\n",
    "    history_train = np.append(history_train, current_loss)\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    mlp.eval()     # Optional when not using Model Specific layer\n",
    "    for i, data in enumerate(validloader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        output_val = mlp(inputs)\n",
    "        valid_loss = loss_function(output_val, targets)\n",
    "    \n",
    "        valid_loss += loss.item()\n",
    "    history_val = np.append(history_val, valid_loss.item())\n",
    "    print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "         '{:.3f}'.format(loss.item(), valid_loss.item()))\n",
    "#     print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "#           '{:.3f}'.format(current_loss / len(trainloader), valid_loss / len(validloader)))\n",
    "#     if min_valid_loss > valid_loss:\n",
    "#         print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "#         min_valid_loss = valid_loss\n",
    "#         # Saving State Dict\n",
    "#         torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7b9d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.random.randn(13)\n",
    "# torch usa tensores de torch y no numpy.darrays\n",
    "dtype = torch.float\n",
    "test = torch.randn((1, 3), device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3332d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62c6a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.371753692626953"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "27fc4848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0, 0], X_test[0]\n",
    "# xtest = [x[0] for x in X_test]\n",
    "ypred = [y[0].item() for y in y_pred]\n",
    "ytest = [y[0].item() for y in y_test]\n",
    "diff=np.array(ytest)-np.array(ypred)\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87b7e8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diff,bins=100)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0,4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f558b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6a8d52ecd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = np.arange(0, nepochs+1)\n",
    "plt.plot(ep, history_train, label='train', color='green')\n",
    "plt.plot(ep, history_val, label='val', color='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e1355f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[87.23693] [16.030924]\n",
      "[7.2511854] [7.2660136]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.displot(y_test)\n",
    "plt.plot(np.sort(y_test.detach().numpy()), color='blue', label='true likelihood')\n",
    "plt.plot(np.sort(y_pred.detach().numpy()), color='red', label='neural likelihood')\n",
    "plt.ylabel('-loglikelihood')\n",
    "plt.legend()\n",
    "print(max(y_test.detach().numpy()), max(y_pred.detach().numpy()))\n",
    "print(min(y_test.detach().numpy()), min(y_pred.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47ff911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1]), torch.Size([100, 1]), (100, 1))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_pred), np.shape(y_test), np.shape(y_pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58b48c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51,), (51,))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(history_train), np.shape(history_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4744941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 60.0)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ytest, bins=len(ytest), color='red')\n",
    "# plt.hist(ypred, bins=len(ypred), color='green')\n",
    "plt.ylim(0,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a45db1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)==len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a07e1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d767f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> 8.371753692626953\n",
      "<class 'torch.Tensor'> 7.754067897796631\n",
      "<class 'torch.Tensor'> 15.272122383117676\n",
      "<class 'torch.Tensor'> 15.526750564575195\n",
      "<class 'torch.Tensor'> 10.680397033691406\n",
      "<class 'torch.Tensor'> 8.487668991088867\n",
      "<class 'torch.Tensor'> 7.3282670974731445\n",
      "<class 'torch.Tensor'> 9.643688201904297\n",
      "<class 'torch.Tensor'> 10.67636489868164\n",
      "<class 'torch.Tensor'> 14.384824752807617\n",
      "<class 'torch.Tensor'> 7.302188396453857\n",
      "<class 'torch.Tensor'> 7.273252487182617\n",
      "<class 'torch.Tensor'> 10.827005386352539\n",
      "<class 'torch.Tensor'> 7.436983108520508\n",
      "<class 'torch.Tensor'> 7.379617214202881\n",
      "<class 'torch.Tensor'> 7.6102681159973145\n",
      "<class 'torch.Tensor'> 8.467998504638672\n",
      "<class 'torch.Tensor'> 7.266648769378662\n",
      "<class 'torch.Tensor'> 8.279807090759277\n",
      "<class 'torch.Tensor'> 8.23650074005127\n",
      "<class 'torch.Tensor'> 9.531845092773438\n",
      "<class 'torch.Tensor'> 7.351142406463623\n",
      "<class 'torch.Tensor'> 8.762679100036621\n",
      "<class 'torch.Tensor'> 7.62294864654541\n",
      "<class 'torch.Tensor'> 7.513554096221924\n",
      "<class 'torch.Tensor'> 9.460086822509766\n",
      "<class 'torch.Tensor'> 7.4443535804748535\n",
      "<class 'torch.Tensor'> 9.460926055908203\n",
      "<class 'torch.Tensor'> 7.290613651275635\n",
      "<class 'torch.Tensor'> 7.839132308959961\n",
      "<class 'torch.Tensor'> 8.557653427124023\n",
      "<class 'torch.Tensor'> 7.860507488250732\n",
      "<class 'torch.Tensor'> 7.66343879699707\n",
      "<class 'torch.Tensor'> 9.09138011932373\n",
      "<class 'torch.Tensor'> 10.658835411071777\n",
      "<class 'torch.Tensor'> 9.646514892578125\n",
      "<class 'torch.Tensor'> 8.749593734741211\n",
      "<class 'torch.Tensor'> 12.216090202331543\n",
      "<class 'torch.Tensor'> 7.465112209320068\n",
      "<class 'torch.Tensor'> 7.386097431182861\n",
      "<class 'torch.Tensor'> 11.301834106445312\n",
      "<class 'torch.Tensor'> 12.21337890625\n",
      "<class 'torch.Tensor'> 7.8582234382629395\n",
      "<class 'torch.Tensor'> 7.291317462921143\n",
      "<class 'torch.Tensor'> 8.066720008850098\n",
      "<class 'torch.Tensor'> 8.015661239624023\n",
      "<class 'torch.Tensor'> 7.50720739364624\n",
      "<class 'torch.Tensor'> 7.396446228027344\n",
      "<class 'torch.Tensor'> 8.107847213745117\n",
      "<class 'torch.Tensor'> 7.813002586364746\n",
      "<class 'torch.Tensor'> 11.14883804321289\n",
      "<class 'torch.Tensor'> 9.841473579406738\n",
      "<class 'torch.Tensor'> 7.319411754608154\n",
      "<class 'torch.Tensor'> 7.365909576416016\n",
      "<class 'torch.Tensor'> 7.347702503204346\n",
      "<class 'torch.Tensor'> 7.311785697937012\n",
      "<class 'torch.Tensor'> 8.975939750671387\n",
      "<class 'torch.Tensor'> 7.343194484710693\n",
      "<class 'torch.Tensor'> 7.334024906158447\n",
      "<class 'torch.Tensor'> 10.177040100097656\n",
      "<class 'torch.Tensor'> 7.268357753753662\n",
      "<class 'torch.Tensor'> 7.965272426605225\n",
      "<class 'torch.Tensor'> 10.404216766357422\n",
      "<class 'torch.Tensor'> 7.3329176902771\n",
      "<class 'torch.Tensor'> 7.637204647064209\n",
      "<class 'torch.Tensor'> 8.415684700012207\n",
      "<class 'torch.Tensor'> 7.705535411834717\n",
      "<class 'torch.Tensor'> 7.865816593170166\n",
      "<class 'torch.Tensor'> 16.030925750732422\n",
      "<class 'torch.Tensor'> 7.679427623748779\n",
      "<class 'torch.Tensor'> 7.715827465057373\n",
      "<class 'torch.Tensor'> 11.651971817016602\n",
      "<class 'torch.Tensor'> 10.830741882324219\n",
      "<class 'torch.Tensor'> 8.124611854553223\n",
      "<class 'torch.Tensor'> 12.388503074645996\n",
      "<class 'torch.Tensor'> 7.796209335327148\n",
      "<class 'torch.Tensor'> 8.764434814453125\n",
      "<class 'torch.Tensor'> 7.507289409637451\n",
      "<class 'torch.Tensor'> 7.668491840362549\n",
      "<class 'torch.Tensor'> 7.412364482879639\n",
      "<class 'torch.Tensor'> 8.146117210388184\n",
      "<class 'torch.Tensor'> 7.332937717437744\n",
      "<class 'torch.Tensor'> 7.466918468475342\n",
      "<class 'torch.Tensor'> 9.942120552062988\n",
      "<class 'torch.Tensor'> 7.347519397735596\n",
      "<class 'torch.Tensor'> 13.338489532470703\n",
      "<class 'torch.Tensor'> 7.366827964782715\n",
      "<class 'torch.Tensor'> 8.841922760009766\n",
      "<class 'torch.Tensor'> 7.74915885925293\n",
      "<class 'torch.Tensor'> 7.601943016052246\n",
      "<class 'torch.Tensor'> 8.577651977539062\n",
      "<class 'torch.Tensor'> 7.655454158782959\n",
      "<class 'torch.Tensor'> 7.337539196014404\n",
      "<class 'torch.Tensor'> 11.035409927368164\n",
      "<class 'torch.Tensor'> 7.3403825759887695\n",
      "<class 'torch.Tensor'> 7.299585819244385\n",
      "<class 'torch.Tensor'> 7.330558776855469\n",
      "<class 'torch.Tensor'> 7.594698429107666\n",
      "<class 'torch.Tensor'> 9.487106323242188\n",
      "<class 'torch.Tensor'> 7.2660136222839355\n"
     ]
    }
   ],
   "source": [
    "pred = map(mlp.forward, X_test)\n",
    "for res in pred:\n",
    "    print(type(res), res.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799234a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
