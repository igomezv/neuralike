{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1e1bae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee35f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fd99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om\t\t\t\\Omega_m\n",
    "# Obh2\t\t\t\\Omega_{b}h^2\n",
    "# h\t\n",
    "datafile = 'chains/LCDM_phy_HD_nested_dynesty_multi_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc69788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataSet(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Prepare the dataset for regression\n",
    "    '''\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c80912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ncols = 3\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(ncols, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e24b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1036, 3) (1036, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Load Boston dataset\n",
    "    X = np.loadtxt(datafile, usecols=(2,3,4))\n",
    "    y = np.loadtxt(datafile, usecols=1).reshape(-1, 1)\n",
    "    randomize = np.random.permutation(len(X))\n",
    "    X = X[randomize]\n",
    "    y = y[randomize]\n",
    "    print(np.shape(X), np.shape(y))\n",
    "    X_test, y_test = X[:100, :], y[:100, :]\n",
    "    X, y = X[100:, :], y[100:, :]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f173672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LoadDataSet(X_train, y_train)\n",
    "dataset_val = LoadDataSet(X_val, y_val)\n",
    "# dataset_test = LoadDataSet(X_test, y_test)\n",
    "X_test, y_test = torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=1)\n",
    "validloader = torch.utils.data.DataLoader(dataset_val, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ea89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "mlp.float()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f533ceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       800\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       40,200\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       201\n",
       "=================================================================\n",
       "Total params: 41,201\n",
       "Trainable params: 41,201\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mlp, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00af4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 19.430\n",
      "Loss after mini-batch    11: 15.020\n",
      "Loss after mini-batch    21: 11.495\n",
      "Loss after mini-batch    31: 13.113\n",
      "Loss after mini-batch    41: 9.104\n",
      "Loss after mini-batch    51: 14.056\n",
      "Loss after mini-batch    61: 12.764\n",
      "Loss after mini-batch    71: 18.327\n",
      "Training Loss: 9.136 \t\t Validation Loss:19.586\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 10.538\n",
      "Loss after mini-batch    11: 6.027\n",
      "Loss after mini-batch    21: 7.252\n",
      "Loss after mini-batch    31: 16.961\n",
      "Loss after mini-batch    41: 5.681\n",
      "Loss after mini-batch    51: 14.572\n",
      "Loss after mini-batch    61: 6.854\n",
      "Loss after mini-batch    71: 11.067\n",
      "Training Loss: 7.556 \t\t Validation Loss:18.803\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 4.196\n",
      "Loss after mini-batch    11: 4.367\n",
      "Loss after mini-batch    21: 9.973\n",
      "Loss after mini-batch    31: 3.377\n",
      "Loss after mini-batch    41: 12.085\n",
      "Loss after mini-batch    51: 3.592\n",
      "Loss after mini-batch    61: 5.360\n",
      "Loss after mini-batch    71: 14.002\n",
      "Training Loss: 0.800 \t\t Validation Loss:2.235\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 3.481\n",
      "Loss after mini-batch    11: 2.082\n",
      "Loss after mini-batch    21: 1.152\n",
      "Loss after mini-batch    31: 7.868\n",
      "Loss after mini-batch    41: 1.421\n",
      "Loss after mini-batch    51: 1.147\n",
      "Loss after mini-batch    61: 7.834\n",
      "Loss after mini-batch    71: 5.268\n",
      "Training Loss: 9.399 \t\t Validation Loss:10.174\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 1.407\n",
      "Loss after mini-batch    11: 4.835\n",
      "Loss after mini-batch    21: 0.243\n",
      "Loss after mini-batch    31: 1.330\n",
      "Loss after mini-batch    41: 0.971\n",
      "Loss after mini-batch    51: 0.815\n",
      "Loss after mini-batch    61: 1.151\n",
      "Loss after mini-batch    71: 1.700\n",
      "Training Loss: 1.191 \t\t Validation Loss:6.322\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.804\n",
      "Loss after mini-batch    11: 3.311\n",
      "Loss after mini-batch    21: 0.364\n",
      "Loss after mini-batch    31: 1.834\n",
      "Loss after mini-batch    41: 1.177\n",
      "Loss after mini-batch    51: 1.874\n",
      "Loss after mini-batch    61: 7.703\n",
      "Loss after mini-batch    71: 7.971\n",
      "Training Loss: 2.848 \t\t Validation Loss:10.753\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 1.321\n",
      "Loss after mini-batch    11: 5.043\n",
      "Loss after mini-batch    21: 0.496\n",
      "Loss after mini-batch    31: 1.635\n",
      "Loss after mini-batch    41: 0.786\n",
      "Loss after mini-batch    51: 1.045\n",
      "Loss after mini-batch    61: 2.750\n",
      "Loss after mini-batch    71: 0.735\n",
      "Training Loss: 9.206 \t\t Validation Loss:11.434\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 1.743\n",
      "Loss after mini-batch    11: 2.661\n",
      "Loss after mini-batch    21: 2.538\n",
      "Loss after mini-batch    31: 0.710\n",
      "Loss after mini-batch    41: 0.997\n",
      "Loss after mini-batch    51: 1.873\n",
      "Loss after mini-batch    61: 1.010\n",
      "Loss after mini-batch    71: 3.357\n",
      "Training Loss: 10.451 \t\t Validation Loss:13.869\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 1.189\n",
      "Loss after mini-batch    11: 2.254\n",
      "Loss after mini-batch    21: 0.366\n",
      "Loss after mini-batch    31: 0.858\n",
      "Loss after mini-batch    41: 7.467\n",
      "Loss after mini-batch    51: 8.517\n",
      "Loss after mini-batch    61: 1.663\n",
      "Loss after mini-batch    71: 1.146\n",
      "Training Loss: 0.336 \t\t Validation Loss:0.977\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 5.989\n",
      "Loss after mini-batch    11: 4.990\n",
      "Loss after mini-batch    21: 4.616\n",
      "Loss after mini-batch    31: 2.781\n",
      "Loss after mini-batch    41: 1.477\n",
      "Loss after mini-batch    51: 3.249\n",
      "Loss after mini-batch    61: 2.058\n",
      "Loss after mini-batch    71: 0.487\n",
      "Training Loss: 5.949 \t\t Validation Loss:12.067\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 2.996\n",
      "Loss after mini-batch    11: 3.133\n",
      "Loss after mini-batch    21: 7.520\n",
      "Loss after mini-batch    31: 0.680\n",
      "Loss after mini-batch    41: 4.510\n",
      "Loss after mini-batch    51: 0.756\n",
      "Loss after mini-batch    61: 0.867\n",
      "Loss after mini-batch    71: 4.625\n",
      "Training Loss: 0.704 \t\t Validation Loss:1.080\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 4.340\n",
      "Loss after mini-batch    11: 0.651\n",
      "Loss after mini-batch    21: 1.410\n",
      "Loss after mini-batch    31: 1.482\n",
      "Loss after mini-batch    41: 1.200\n",
      "Loss after mini-batch    51: 0.948\n",
      "Loss after mini-batch    61: 1.374\n",
      "Loss after mini-batch    71: 1.774\n",
      "Training Loss: 0.732 \t\t Validation Loss:3.373\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.658\n",
      "Loss after mini-batch    11: 4.550\n",
      "Loss after mini-batch    21: 0.661\n",
      "Loss after mini-batch    31: 0.596\n",
      "Loss after mini-batch    41: 1.351\n",
      "Loss after mini-batch    51: 0.441\n",
      "Loss after mini-batch    61: 0.328\n",
      "Loss after mini-batch    71: 0.771\n",
      "Training Loss: 0.204 \t\t Validation Loss:3.893\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 0.441\n",
      "Loss after mini-batch    11: 1.025\n",
      "Loss after mini-batch    21: 1.942\n",
      "Loss after mini-batch    31: 0.637\n",
      "Loss after mini-batch    41: 0.773\n",
      "Loss after mini-batch    51: 0.636\n",
      "Loss after mini-batch    61: 6.786\n",
      "Loss after mini-batch    71: 0.711\n",
      "Training Loss: 4.560 \t\t Validation Loss:8.840\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 0.579\n",
      "Loss after mini-batch    11: 1.225\n",
      "Loss after mini-batch    21: 0.702\n",
      "Loss after mini-batch    31: 1.178\n",
      "Loss after mini-batch    41: 5.951\n",
      "Loss after mini-batch    51: 3.197\n",
      "Loss after mini-batch    61: 2.303\n",
      "Loss after mini-batch    71: 0.319\n",
      "Training Loss: 0.867 \t\t Validation Loss:5.925\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 1.051\n",
      "Loss after mini-batch    11: 0.957\n",
      "Loss after mini-batch    21: 2.321\n",
      "Loss after mini-batch    31: 1.633\n",
      "Loss after mini-batch    41: 0.718\n",
      "Loss after mini-batch    51: 0.372\n",
      "Loss after mini-batch    61: 0.683\n",
      "Loss after mini-batch    71: 0.719\n",
      "Training Loss: 4.184 \t\t Validation Loss:4.721\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 1.784\n",
      "Loss after mini-batch    11: 0.824\n",
      "Loss after mini-batch    21: 0.459\n",
      "Loss after mini-batch    31: 0.813\n",
      "Loss after mini-batch    41: 0.377\n",
      "Loss after mini-batch    51: 2.950\n",
      "Loss after mini-batch    61: 0.594\n",
      "Loss after mini-batch    71: 0.660\n",
      "Training Loss: 0.220 \t\t Validation Loss:0.747\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 1.345\n",
      "Loss after mini-batch    11: 5.039\n",
      "Loss after mini-batch    21: 0.725\n",
      "Loss after mini-batch    31: 6.936\n",
      "Loss after mini-batch    41: 0.743\n",
      "Loss after mini-batch    51: 0.234\n",
      "Loss after mini-batch    61: 0.715\n",
      "Loss after mini-batch    71: 0.437\n",
      "Training Loss: 1.172 \t\t Validation Loss:1.604\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.301\n",
      "Loss after mini-batch    11: 1.303\n",
      "Loss after mini-batch    21: 0.740\n",
      "Loss after mini-batch    31: 1.372\n",
      "Loss after mini-batch    41: 0.824\n",
      "Loss after mini-batch    51: 0.238\n",
      "Loss after mini-batch    61: 2.426\n",
      "Loss after mini-batch    71: 5.797\n",
      "Training Loss: 1.168 \t\t Validation Loss:2.164\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.149\n",
      "Loss after mini-batch    11: 0.234\n",
      "Loss after mini-batch    21: 0.471\n",
      "Loss after mini-batch    31: 0.738\n",
      "Loss after mini-batch    41: 1.088\n",
      "Loss after mini-batch    51: 0.911\n",
      "Loss after mini-batch    61: 1.338\n",
      "Loss after mini-batch    71: 0.555\n",
      "Training Loss: 0.526 \t\t Validation Loss:3.205\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 1.746\n",
      "Loss after mini-batch    11: 0.511\n",
      "Loss after mini-batch    21: 1.790\n",
      "Loss after mini-batch    31: 1.261\n",
      "Loss after mini-batch    41: 2.432\n",
      "Loss after mini-batch    51: 0.701\n",
      "Loss after mini-batch    61: 3.115\n",
      "Loss after mini-batch    71: 0.597\n",
      "Training Loss: 0.441 \t\t Validation Loss:1.788\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.400\n",
      "Loss after mini-batch    11: 0.439\n",
      "Loss after mini-batch    21: 4.594\n",
      "Loss after mini-batch    31: 0.621\n",
      "Loss after mini-batch    41: 1.244\n",
      "Loss after mini-batch    51: 0.573\n",
      "Loss after mini-batch    61: 0.685\n",
      "Loss after mini-batch    71: 0.298\n",
      "Training Loss: 0.255 \t\t Validation Loss:2.949\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 0.772\n",
      "Loss after mini-batch    11: 0.576\n",
      "Loss after mini-batch    21: 1.206\n",
      "Loss after mini-batch    31: 3.657\n",
      "Loss after mini-batch    41: 0.526\n",
      "Loss after mini-batch    51: 0.620\n",
      "Loss after mini-batch    61: 0.954\n",
      "Loss after mini-batch    71: 0.596\n",
      "Training Loss: 1.736 \t\t Validation Loss:1.969\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.741\n",
      "Loss after mini-batch    11: 3.520\n",
      "Loss after mini-batch    21: 0.514\n",
      "Loss after mini-batch    31: 0.605\n",
      "Loss after mini-batch    41: 2.058\n",
      "Loss after mini-batch    51: 0.456\n",
      "Loss after mini-batch    61: 0.407\n",
      "Loss after mini-batch    71: 0.160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.124 \t\t Validation Loss:1.018\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 0.692\n",
      "Loss after mini-batch    11: 0.808\n",
      "Loss after mini-batch    21: 1.529\n",
      "Loss after mini-batch    31: 2.162\n",
      "Loss after mini-batch    41: 0.548\n",
      "Loss after mini-batch    51: 0.755\n",
      "Loss after mini-batch    61: 1.501\n",
      "Loss after mini-batch    71: 3.761\n",
      "Training Loss: 1.355 \t\t Validation Loss:3.163\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 0.755\n",
      "Loss after mini-batch    11: 1.039\n",
      "Loss after mini-batch    21: 1.863\n",
      "Loss after mini-batch    31: 0.179\n",
      "Loss after mini-batch    41: 0.956\n",
      "Loss after mini-batch    51: 0.160\n",
      "Loss after mini-batch    61: 2.009\n",
      "Loss after mini-batch    71: 0.320\n",
      "Training Loss: 0.560 \t\t Validation Loss:1.702\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.335\n",
      "Loss after mini-batch    11: 6.090\n",
      "Loss after mini-batch    21: 1.271\n",
      "Loss after mini-batch    31: 0.985\n",
      "Loss after mini-batch    41: 2.934\n",
      "Loss after mini-batch    51: 1.292\n",
      "Loss after mini-batch    61: 2.101\n",
      "Loss after mini-batch    71: 1.142\n",
      "Training Loss: 0.438 \t\t Validation Loss:0.973\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 2.544\n",
      "Loss after mini-batch    11: 4.395\n",
      "Loss after mini-batch    21: 1.291\n",
      "Loss after mini-batch    31: 0.954\n",
      "Loss after mini-batch    41: 1.165\n",
      "Loss after mini-batch    51: 1.983\n",
      "Loss after mini-batch    61: 0.603\n",
      "Loss after mini-batch    71: 1.045\n",
      "Training Loss: 0.494 \t\t Validation Loss:2.165\n",
      "Starting epoch 29\n",
      "Loss after mini-batch     1: 0.671\n",
      "Loss after mini-batch    11: 0.609\n",
      "Loss after mini-batch    21: 1.293\n",
      "Loss after mini-batch    31: 0.199\n",
      "Loss after mini-batch    41: 0.368\n",
      "Loss after mini-batch    51: 0.321\n",
      "Loss after mini-batch    61: 0.207\n",
      "Loss after mini-batch    71: 1.848\n",
      "Training Loss: 2.054 \t\t Validation Loss:4.359\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 2.455\n",
      "Loss after mini-batch    11: 0.363\n",
      "Loss after mini-batch    21: 1.036\n",
      "Loss after mini-batch    31: 1.306\n",
      "Loss after mini-batch    41: 1.499\n",
      "Loss after mini-batch    51: 2.024\n",
      "Loss after mini-batch    61: 0.916\n",
      "Loss after mini-batch    71: 0.128\n",
      "Training Loss: 0.338 \t\t Validation Loss:0.538\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 0.689\n",
      "Loss after mini-batch    11: 1.855\n",
      "Loss after mini-batch    21: 0.292\n",
      "Loss after mini-batch    31: 1.450\n",
      "Loss after mini-batch    41: 1.229\n",
      "Loss after mini-batch    51: 0.485\n",
      "Loss after mini-batch    61: 1.293\n",
      "Loss after mini-batch    71: 0.402\n",
      "Training Loss: 0.300 \t\t Validation Loss:0.780\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 0.726\n",
      "Loss after mini-batch    11: 0.530\n",
      "Loss after mini-batch    21: 0.725\n",
      "Loss after mini-batch    31: 0.261\n",
      "Loss after mini-batch    41: 1.290\n",
      "Loss after mini-batch    51: 0.604\n",
      "Loss after mini-batch    61: 0.330\n",
      "Loss after mini-batch    71: 0.991\n",
      "Training Loss: 1.100 \t\t Validation Loss:1.292\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 1.228\n",
      "Loss after mini-batch    11: 0.579\n",
      "Loss after mini-batch    21: 0.332\n",
      "Loss after mini-batch    31: 0.920\n",
      "Loss after mini-batch    41: 1.078\n",
      "Loss after mini-batch    51: 0.036\n",
      "Loss after mini-batch    61: 3.798\n",
      "Loss after mini-batch    71: 0.229\n",
      "Training Loss: 1.011 \t\t Validation Loss:1.380\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 0.475\n",
      "Loss after mini-batch    11: 0.241\n",
      "Loss after mini-batch    21: 0.851\n",
      "Loss after mini-batch    31: 0.815\n",
      "Loss after mini-batch    41: 2.224\n",
      "Loss after mini-batch    51: 0.969\n",
      "Loss after mini-batch    61: 0.848\n",
      "Loss after mini-batch    71: 0.374\n",
      "Training Loss: 0.797 \t\t Validation Loss:1.454\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 1.799\n",
      "Loss after mini-batch    11: 1.075\n",
      "Loss after mini-batch    21: 2.606\n",
      "Loss after mini-batch    31: 0.173\n",
      "Loss after mini-batch    41: 0.284\n",
      "Loss after mini-batch    51: 0.179\n",
      "Loss after mini-batch    61: 1.366\n",
      "Loss after mini-batch    71: 0.901\n",
      "Training Loss: 0.122 \t\t Validation Loss:0.867\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 1.025\n",
      "Loss after mini-batch    11: 1.416\n",
      "Loss after mini-batch    21: 0.978\n",
      "Loss after mini-batch    31: 1.043\n",
      "Loss after mini-batch    41: 0.507\n",
      "Loss after mini-batch    51: 4.904\n",
      "Loss after mini-batch    61: 1.199\n",
      "Loss after mini-batch    71: 0.113\n",
      "Training Loss: 0.112 \t\t Validation Loss:1.742\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 0.163\n",
      "Loss after mini-batch    11: 0.827\n",
      "Loss after mini-batch    21: 0.546\n",
      "Loss after mini-batch    31: 1.034\n",
      "Loss after mini-batch    41: 0.908\n",
      "Loss after mini-batch    51: 1.452\n",
      "Loss after mini-batch    61: 0.268\n",
      "Loss after mini-batch    71: 0.627\n",
      "Training Loss: 2.181 \t\t Validation Loss:2.724\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 0.346\n",
      "Loss after mini-batch    11: 0.536\n",
      "Loss after mini-batch    21: 0.405\n",
      "Loss after mini-batch    31: 0.521\n",
      "Loss after mini-batch    41: 0.144\n",
      "Loss after mini-batch    51: 1.146\n",
      "Loss after mini-batch    61: 0.424\n",
      "Loss after mini-batch    71: 0.090\n",
      "Training Loss: 0.780 \t\t Validation Loss:1.805\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 2.164\n",
      "Loss after mini-batch    11: 0.903\n",
      "Loss after mini-batch    21: 0.172\n",
      "Loss after mini-batch    31: 0.514\n",
      "Loss after mini-batch    41: 0.228\n",
      "Loss after mini-batch    51: 0.774\n",
      "Loss after mini-batch    61: 2.632\n",
      "Loss after mini-batch    71: 0.760\n",
      "Training Loss: 2.130 \t\t Validation Loss:2.862\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 0.468\n",
      "Loss after mini-batch    11: 0.432\n",
      "Loss after mini-batch    21: 0.205\n",
      "Loss after mini-batch    31: 1.192\n",
      "Loss after mini-batch    41: 0.243\n",
      "Loss after mini-batch    51: 0.232\n",
      "Loss after mini-batch    61: 0.168\n",
      "Loss after mini-batch    71: 0.131\n",
      "Training Loss: 0.444 \t\t Validation Loss:0.579\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 0.261\n",
      "Loss after mini-batch    11: 0.585\n",
      "Loss after mini-batch    21: 0.470\n",
      "Loss after mini-batch    31: 0.101\n",
      "Loss after mini-batch    41: 0.384\n",
      "Loss after mini-batch    51: 1.578\n",
      "Loss after mini-batch    61: 0.254\n",
      "Loss after mini-batch    71: 0.319\n",
      "Training Loss: 1.307 \t\t Validation Loss:1.508\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 1.797\n",
      "Loss after mini-batch    11: 0.327\n",
      "Loss after mini-batch    21: 1.045\n",
      "Loss after mini-batch    31: 0.140\n",
      "Loss after mini-batch    41: 1.294\n",
      "Loss after mini-batch    51: 0.995\n",
      "Loss after mini-batch    61: 0.176\n",
      "Loss after mini-batch    71: 0.342\n",
      "Training Loss: 0.207 \t\t Validation Loss:1.035\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 0.550\n",
      "Loss after mini-batch    11: 0.449\n",
      "Loss after mini-batch    21: 0.149\n",
      "Loss after mini-batch    31: 0.395\n",
      "Loss after mini-batch    41: 0.256\n",
      "Loss after mini-batch    51: 1.651\n",
      "Loss after mini-batch    61: 1.787\n",
      "Loss after mini-batch    71: 3.077\n",
      "Training Loss: 0.775 \t\t Validation Loss:1.445\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 0.480\n",
      "Loss after mini-batch    11: 0.453\n",
      "Loss after mini-batch    21: 0.583\n",
      "Loss after mini-batch    31: 0.511\n",
      "Loss after mini-batch    41: 1.467\n",
      "Loss after mini-batch    51: 0.314\n",
      "Loss after mini-batch    61: 0.313\n",
      "Loss after mini-batch    71: 0.588\n",
      "Training Loss: 0.198 \t\t Validation Loss:1.239\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 0.374\n",
      "Loss after mini-batch    11: 2.547\n",
      "Loss after mini-batch    21: 1.077\n",
      "Loss after mini-batch    31: 1.431\n",
      "Loss after mini-batch    41: 1.282\n",
      "Loss after mini-batch    51: 0.937\n",
      "Loss after mini-batch    61: 0.736\n",
      "Loss after mini-batch    71: 0.640\n",
      "Training Loss: 1.708 \t\t Validation Loss:1.962\n",
      "Starting epoch 46\n",
      "Loss after mini-batch     1: 2.258\n",
      "Loss after mini-batch    11: 0.764\n",
      "Loss after mini-batch    21: 0.679\n",
      "Loss after mini-batch    31: 0.156\n",
      "Loss after mini-batch    41: 0.658\n",
      "Loss after mini-batch    51: 0.887\n",
      "Loss after mini-batch    61: 0.516\n",
      "Loss after mini-batch    71: 0.859\n",
      "Training Loss: 0.219 \t\t Validation Loss:2.677\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 1.580\n",
      "Loss after mini-batch    11: 0.843\n",
      "Loss after mini-batch    21: 0.169\n",
      "Loss after mini-batch    31: 2.183\n",
      "Loss after mini-batch    41: 0.246\n",
      "Loss after mini-batch    51: 4.499\n",
      "Loss after mini-batch    61: 0.085\n",
      "Loss after mini-batch    71: 0.557\n",
      "Training Loss: 0.692 \t\t Validation Loss:1.017\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.191\n",
      "Loss after mini-batch    11: 0.331\n",
      "Loss after mini-batch    21: 0.172\n",
      "Loss after mini-batch    31: 4.732\n",
      "Loss after mini-batch    41: 0.170\n",
      "Loss after mini-batch    51: 0.495\n",
      "Loss after mini-batch    61: 1.887\n",
      "Loss after mini-batch    71: 0.290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.143 \t\t Validation Loss:2.097\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 0.394\n",
      "Loss after mini-batch    11: 1.818\n",
      "Loss after mini-batch    21: 0.186\n",
      "Loss after mini-batch    31: 0.058\n",
      "Loss after mini-batch    41: 0.529\n",
      "Loss after mini-batch    51: 0.445\n",
      "Loss after mini-batch    61: 0.287\n",
      "Loss after mini-batch    71: 0.154\n",
      "Training Loss: 0.196 \t\t Validation Loss:0.598\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 0.299\n",
      "Loss after mini-batch    11: 0.510\n",
      "Loss after mini-batch    21: 0.273\n",
      "Loss after mini-batch    31: 1.490\n",
      "Loss after mini-batch    41: 0.666\n",
      "Loss after mini-batch    51: 0.323\n",
      "Loss after mini-batch    61: 0.560\n",
      "Loss after mini-batch    71: 0.266\n",
      "Training Loss: 0.059 \t\t Validation Loss:0.202\n",
      "Starting epoch 51\n",
      "Loss after mini-batch     1: 0.847\n",
      "Loss after mini-batch    11: 0.646\n",
      "Loss after mini-batch    21: 0.463\n",
      "Loss after mini-batch    31: 0.269\n",
      "Loss after mini-batch    41: 0.110\n",
      "Loss after mini-batch    51: 0.939\n",
      "Loss after mini-batch    61: 1.445\n",
      "Loss after mini-batch    71: 2.334\n",
      "Training Loss: 0.374 \t\t Validation Loss:2.698\n",
      "Starting epoch 52\n",
      "Loss after mini-batch     1: 0.686\n",
      "Loss after mini-batch    11: 1.133\n",
      "Loss after mini-batch    21: 0.333\n",
      "Loss after mini-batch    31: 0.697\n",
      "Loss after mini-batch    41: 0.095\n",
      "Loss after mini-batch    51: 0.109\n",
      "Loss after mini-batch    61: 0.120\n",
      "Loss after mini-batch    71: 2.712\n",
      "Training Loss: 2.344 \t\t Validation Loss:2.534\n",
      "Starting epoch 53\n",
      "Loss after mini-batch     1: 0.413\n",
      "Loss after mini-batch    11: 0.500\n",
      "Loss after mini-batch    21: 0.291\n",
      "Loss after mini-batch    31: 1.199\n",
      "Loss after mini-batch    41: 0.074\n",
      "Loss after mini-batch    51: 0.061\n",
      "Loss after mini-batch    61: 0.541\n",
      "Loss after mini-batch    71: 0.111\n",
      "Training Loss: 1.332 \t\t Validation Loss:2.505\n",
      "Starting epoch 54\n",
      "Loss after mini-batch     1: 0.248\n",
      "Loss after mini-batch    11: 0.632\n",
      "Loss after mini-batch    21: 0.164\n",
      "Loss after mini-batch    31: 1.836\n",
      "Loss after mini-batch    41: 0.093\n",
      "Loss after mini-batch    51: 0.780\n",
      "Loss after mini-batch    61: 0.343\n",
      "Loss after mini-batch    71: 0.580\n",
      "Training Loss: 0.092 \t\t Validation Loss:0.227\n",
      "Starting epoch 55\n",
      "Loss after mini-batch     1: 0.356\n",
      "Loss after mini-batch    11: 0.423\n",
      "Loss after mini-batch    21: 1.209\n",
      "Loss after mini-batch    31: 0.568\n",
      "Loss after mini-batch    41: 2.456\n",
      "Loss after mini-batch    51: 0.210\n",
      "Loss after mini-batch    61: 0.109\n",
      "Loss after mini-batch    71: 1.037\n",
      "Training Loss: 0.966 \t\t Validation Loss:4.560\n",
      "Starting epoch 56\n",
      "Loss after mini-batch     1: 0.157\n",
      "Loss after mini-batch    11: 0.341\n",
      "Loss after mini-batch    21: 1.345\n",
      "Loss after mini-batch    31: 0.179\n",
      "Loss after mini-batch    41: 0.341\n",
      "Loss after mini-batch    51: 0.364\n",
      "Loss after mini-batch    61: 0.353\n",
      "Loss after mini-batch    71: 0.981\n",
      "Training Loss: 0.606 \t\t Validation Loss:1.755\n",
      "Starting epoch 57\n",
      "Loss after mini-batch     1: 0.912\n",
      "Loss after mini-batch    11: 0.135\n",
      "Loss after mini-batch    21: 0.145\n",
      "Loss after mini-batch    31: 0.616\n",
      "Loss after mini-batch    41: 0.044\n",
      "Loss after mini-batch    51: 1.046\n",
      "Loss after mini-batch    61: 0.773\n",
      "Loss after mini-batch    71: 0.191\n",
      "Training Loss: 0.181 \t\t Validation Loss:0.483\n",
      "Starting epoch 58\n",
      "Loss after mini-batch     1: 0.122\n",
      "Loss after mini-batch    11: 1.000\n",
      "Loss after mini-batch    21: 0.456\n",
      "Loss after mini-batch    31: 0.158\n",
      "Loss after mini-batch    41: 0.217\n",
      "Loss after mini-batch    51: 0.588\n",
      "Loss after mini-batch    61: 0.477\n",
      "Loss after mini-batch    71: 0.093\n",
      "Training Loss: 0.329 \t\t Validation Loss:0.862\n",
      "Starting epoch 59\n",
      "Loss after mini-batch     1: 0.479\n",
      "Loss after mini-batch    11: 0.749\n",
      "Loss after mini-batch    21: 0.333\n",
      "Loss after mini-batch    31: 0.362\n",
      "Loss after mini-batch    41: 0.332\n",
      "Loss after mini-batch    51: 0.120\n",
      "Loss after mini-batch    61: 0.287\n",
      "Loss after mini-batch    71: 0.610\n",
      "Training Loss: 0.260 \t\t Validation Loss:4.082\n",
      "Starting epoch 60\n",
      "Loss after mini-batch     1: 0.749\n",
      "Loss after mini-batch    11: 0.245\n",
      "Loss after mini-batch    21: 1.100\n",
      "Loss after mini-batch    31: 0.247\n",
      "Loss after mini-batch    41: 0.300\n",
      "Loss after mini-batch    51: 0.210\n",
      "Loss after mini-batch    61: 0.639\n",
      "Loss after mini-batch    71: 0.232\n",
      "Training Loss: 2.192 \t\t Validation Loss:2.992\n",
      "Starting epoch 61\n",
      "Loss after mini-batch     1: 0.093\n",
      "Loss after mini-batch    11: 0.361\n",
      "Loss after mini-batch    21: 0.934\n",
      "Loss after mini-batch    31: 0.256\n",
      "Loss after mini-batch    41: 0.636\n",
      "Loss after mini-batch    51: 0.867\n",
      "Loss after mini-batch    61: 0.093\n",
      "Loss after mini-batch    71: 0.614\n",
      "Training Loss: 0.876 \t\t Validation Loss:1.689\n",
      "Starting epoch 62\n",
      "Loss after mini-batch     1: 0.983\n",
      "Loss after mini-batch    11: 0.908\n",
      "Loss after mini-batch    21: 0.884\n",
      "Loss after mini-batch    31: 0.795\n",
      "Loss after mini-batch    41: 0.086\n",
      "Loss after mini-batch    51: 0.378\n",
      "Loss after mini-batch    61: 0.198\n",
      "Loss after mini-batch    71: 0.207\n",
      "Training Loss: 0.354 \t\t Validation Loss:1.936\n",
      "Starting epoch 63\n",
      "Loss after mini-batch     1: 0.665\n",
      "Loss after mini-batch    11: 0.127\n",
      "Loss after mini-batch    21: 0.224\n",
      "Loss after mini-batch    31: 0.158\n",
      "Loss after mini-batch    41: 0.286\n",
      "Loss after mini-batch    51: 1.039\n",
      "Loss after mini-batch    61: 0.311\n",
      "Loss after mini-batch    71: 0.208\n",
      "Training Loss: 0.405 \t\t Validation Loss:3.254\n",
      "Starting epoch 64\n",
      "Loss after mini-batch     1: 0.150\n",
      "Loss after mini-batch    11: 1.012\n",
      "Loss after mini-batch    21: 4.910\n",
      "Loss after mini-batch    31: 0.262\n",
      "Loss after mini-batch    41: 0.283\n",
      "Loss after mini-batch    51: 0.288\n",
      "Loss after mini-batch    61: 0.272\n",
      "Loss after mini-batch    71: 0.270\n",
      "Training Loss: 0.219 \t\t Validation Loss:1.644\n",
      "Starting epoch 65\n",
      "Loss after mini-batch     1: 0.894\n",
      "Loss after mini-batch    11: 0.182\n",
      "Loss after mini-batch    21: 0.409\n",
      "Loss after mini-batch    31: 0.252\n",
      "Loss after mini-batch    41: 0.147\n",
      "Loss after mini-batch    51: 0.628\n",
      "Loss after mini-batch    61: 0.130\n",
      "Loss after mini-batch    71: 0.934\n",
      "Training Loss: 0.233 \t\t Validation Loss:0.997\n",
      "Starting epoch 66\n",
      "Loss after mini-batch     1: 0.218\n",
      "Loss after mini-batch    11: 1.728\n",
      "Loss after mini-batch    21: 0.358\n",
      "Loss after mini-batch    31: 0.269\n",
      "Loss after mini-batch    41: 0.110\n",
      "Loss after mini-batch    51: 0.379\n",
      "Loss after mini-batch    61: 0.247\n",
      "Loss after mini-batch    71: 1.959\n",
      "Training Loss: 0.190 \t\t Validation Loss:2.992\n",
      "Starting epoch 67\n",
      "Loss after mini-batch     1: 0.596\n",
      "Loss after mini-batch    11: 0.126\n",
      "Loss after mini-batch    21: 0.194\n",
      "Loss after mini-batch    31: 0.751\n",
      "Loss after mini-batch    41: 0.209\n",
      "Loss after mini-batch    51: 0.380\n",
      "Loss after mini-batch    61: 0.098\n",
      "Loss after mini-batch    71: 0.465\n",
      "Training Loss: 1.973 \t\t Validation Loss:2.125\n",
      "Starting epoch 68\n",
      "Loss after mini-batch     1: 0.149\n",
      "Loss after mini-batch    11: 0.224\n",
      "Loss after mini-batch    21: 0.252\n",
      "Loss after mini-batch    31: 0.511\n",
      "Loss after mini-batch    41: 0.543\n",
      "Loss after mini-batch    51: 0.273\n",
      "Loss after mini-batch    61: 0.266\n",
      "Loss after mini-batch    71: 0.729\n",
      "Training Loss: 0.464 \t\t Validation Loss:0.742\n",
      "Starting epoch 69\n",
      "Loss after mini-batch     1: 0.098\n",
      "Loss after mini-batch    11: 0.370\n",
      "Loss after mini-batch    21: 0.097\n",
      "Loss after mini-batch    31: 0.213\n",
      "Loss after mini-batch    41: 0.174\n",
      "Loss after mini-batch    51: 1.912\n",
      "Loss after mini-batch    61: 1.097\n",
      "Loss after mini-batch    71: 0.085\n",
      "Training Loss: 2.019 \t\t Validation Loss:2.461\n",
      "Starting epoch 70\n",
      "Loss after mini-batch     1: 0.172\n",
      "Loss after mini-batch    11: 0.300\n",
      "Loss after mini-batch    21: 0.308\n",
      "Loss after mini-batch    31: 0.566\n",
      "Loss after mini-batch    41: 0.101\n",
      "Loss after mini-batch    51: 0.493\n",
      "Loss after mini-batch    61: 0.540\n",
      "Loss after mini-batch    71: 0.106\n",
      "Training Loss: 0.537 \t\t Validation Loss:0.739\n",
      "Starting epoch 71\n",
      "Loss after mini-batch     1: 0.866\n",
      "Loss after mini-batch    11: 0.267\n",
      "Loss after mini-batch    21: 0.174\n",
      "Loss after mini-batch    31: 0.226\n",
      "Loss after mini-batch    41: 1.096\n",
      "Loss after mini-batch    51: 1.010\n",
      "Loss after mini-batch    61: 0.349\n",
      "Loss after mini-batch    71: 0.058\n",
      "Training Loss: 0.234 \t\t Validation Loss:0.443\n",
      "Starting epoch 72\n",
      "Loss after mini-batch     1: 0.211\n",
      "Loss after mini-batch    11: 1.529\n",
      "Loss after mini-batch    21: 0.225\n",
      "Loss after mini-batch    31: 0.151\n",
      "Loss after mini-batch    41: 0.414\n",
      "Loss after mini-batch    51: 0.117\n",
      "Loss after mini-batch    61: 0.226\n",
      "Loss after mini-batch    71: 1.423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 3.624 \t\t Validation Loss:5.205\n",
      "Starting epoch 73\n",
      "Loss after mini-batch     1: 0.070\n",
      "Loss after mini-batch    11: 0.158\n",
      "Loss after mini-batch    21: 0.813\n",
      "Loss after mini-batch    31: 0.066\n",
      "Loss after mini-batch    41: 0.251\n",
      "Loss after mini-batch    51: 0.200\n",
      "Loss after mini-batch    61: 0.161\n",
      "Loss after mini-batch    71: 0.279\n",
      "Training Loss: 0.717 \t\t Validation Loss:1.731\n",
      "Starting epoch 74\n",
      "Loss after mini-batch     1: 0.186\n",
      "Loss after mini-batch    11: 0.174\n",
      "Loss after mini-batch    21: 0.135\n",
      "Loss after mini-batch    31: 0.057\n",
      "Loss after mini-batch    41: 3.688\n",
      "Loss after mini-batch    51: 0.071\n",
      "Loss after mini-batch    61: 0.738\n",
      "Loss after mini-batch    71: 0.050\n",
      "Training Loss: 0.651 \t\t Validation Loss:1.031\n",
      "Starting epoch 75\n",
      "Loss after mini-batch     1: 0.089\n",
      "Loss after mini-batch    11: 0.132\n",
      "Loss after mini-batch    21: 1.230\n",
      "Loss after mini-batch    31: 1.096\n",
      "Loss after mini-batch    41: 0.849\n",
      "Loss after mini-batch    51: 0.147\n",
      "Loss after mini-batch    61: 0.194\n",
      "Loss after mini-batch    71: 0.578\n",
      "Training Loss: 0.229 \t\t Validation Loss:2.162\n",
      "Starting epoch 76\n",
      "Loss after mini-batch     1: 0.521\n",
      "Loss after mini-batch    11: 0.111\n",
      "Loss after mini-batch    21: 0.623\n",
      "Loss after mini-batch    31: 0.362\n",
      "Loss after mini-batch    41: 0.529\n",
      "Loss after mini-batch    51: 0.814\n",
      "Loss after mini-batch    61: 0.079\n",
      "Loss after mini-batch    71: 0.289\n",
      "Training Loss: 0.357 \t\t Validation Loss:2.094\n",
      "Starting epoch 77\n",
      "Loss after mini-batch     1: 0.257\n",
      "Loss after mini-batch    11: 1.391\n",
      "Loss after mini-batch    21: 3.731\n",
      "Loss after mini-batch    31: 0.492\n",
      "Loss after mini-batch    41: 0.070\n",
      "Loss after mini-batch    51: 0.066\n",
      "Loss after mini-batch    61: 0.304\n",
      "Loss after mini-batch    71: 0.189\n",
      "Training Loss: 0.281 \t\t Validation Loss:1.764\n",
      "Starting epoch 78\n",
      "Loss after mini-batch     1: 0.222\n",
      "Loss after mini-batch    11: 1.438\n",
      "Loss after mini-batch    21: 0.036\n",
      "Loss after mini-batch    31: 0.243\n",
      "Loss after mini-batch    41: 0.279\n",
      "Loss after mini-batch    51: 0.511\n",
      "Loss after mini-batch    61: 0.085\n",
      "Loss after mini-batch    71: 0.214\n",
      "Training Loss: 0.099 \t\t Validation Loss:1.820\n",
      "Starting epoch 79\n",
      "Loss after mini-batch     1: 0.633\n",
      "Loss after mini-batch    11: 0.254\n",
      "Loss after mini-batch    21: 0.135\n",
      "Loss after mini-batch    31: 0.380\n",
      "Loss after mini-batch    41: 0.489\n",
      "Loss after mini-batch    51: 0.057\n",
      "Loss after mini-batch    61: 0.055\n",
      "Loss after mini-batch    71: 0.206\n",
      "Training Loss: 0.645 \t\t Validation Loss:2.340\n",
      "Starting epoch 80\n",
      "Loss after mini-batch     1: 0.370\n",
      "Loss after mini-batch    11: 0.715\n",
      "Loss after mini-batch    21: 0.242\n",
      "Loss after mini-batch    31: 0.338\n",
      "Loss after mini-batch    41: 0.259\n",
      "Loss after mini-batch    51: 0.322\n",
      "Loss after mini-batch    61: 0.167\n",
      "Loss after mini-batch    71: 0.334\n",
      "Training Loss: 0.277 \t\t Validation Loss:0.842\n",
      "Starting epoch 81\n",
      "Loss after mini-batch     1: 0.211\n",
      "Loss after mini-batch    11: 0.291\n",
      "Loss after mini-batch    21: 0.239\n",
      "Loss after mini-batch    31: 0.093\n",
      "Loss after mini-batch    41: 0.194\n",
      "Loss after mini-batch    51: 0.296\n",
      "Loss after mini-batch    61: 0.220\n",
      "Loss after mini-batch    71: 0.029\n",
      "Training Loss: 1.554 \t\t Validation Loss:2.362\n",
      "Starting epoch 82\n",
      "Loss after mini-batch     1: 0.125\n",
      "Loss after mini-batch    11: 0.887\n",
      "Loss after mini-batch    21: 1.318\n",
      "Loss after mini-batch    31: 0.355\n",
      "Loss after mini-batch    41: 0.139\n",
      "Loss after mini-batch    51: 0.094\n",
      "Loss after mini-batch    61: 0.161\n",
      "Loss after mini-batch    71: 0.234\n",
      "Training Loss: 0.336 \t\t Validation Loss:0.668\n",
      "Starting epoch 83\n",
      "Loss after mini-batch     1: 0.151\n",
      "Loss after mini-batch    11: 0.165\n",
      "Loss after mini-batch    21: 0.244\n",
      "Loss after mini-batch    31: 0.184\n",
      "Loss after mini-batch    41: 0.172\n",
      "Loss after mini-batch    51: 0.308\n",
      "Loss after mini-batch    61: 0.137\n",
      "Loss after mini-batch    71: 0.080\n",
      "Training Loss: 0.915 \t\t Validation Loss:1.051\n",
      "Starting epoch 84\n",
      "Loss after mini-batch     1: 0.046\n",
      "Loss after mini-batch    11: 0.709\n",
      "Loss after mini-batch    21: 0.419\n",
      "Loss after mini-batch    31: 0.396\n",
      "Loss after mini-batch    41: 0.354\n",
      "Loss after mini-batch    51: 0.742\n",
      "Loss after mini-batch    61: 0.062\n",
      "Loss after mini-batch    71: 0.334\n",
      "Training Loss: 0.293 \t\t Validation Loss:0.824\n",
      "Starting epoch 85\n",
      "Loss after mini-batch     1: 0.111\n",
      "Loss after mini-batch    11: 0.086\n",
      "Loss after mini-batch    21: 0.075\n",
      "Loss after mini-batch    31: 0.137\n",
      "Loss after mini-batch    41: 0.212\n",
      "Loss after mini-batch    51: 0.073\n",
      "Loss after mini-batch    61: 0.678\n",
      "Loss after mini-batch    71: 0.113\n",
      "Training Loss: 0.348 \t\t Validation Loss:2.194\n",
      "Starting epoch 86\n",
      "Loss after mini-batch     1: 0.135\n",
      "Loss after mini-batch    11: 0.078\n",
      "Loss after mini-batch    21: 0.140\n",
      "Loss after mini-batch    31: 0.366\n",
      "Loss after mini-batch    41: 0.188\n",
      "Loss after mini-batch    51: 0.845\n",
      "Loss after mini-batch    61: 0.111\n",
      "Loss after mini-batch    71: 0.161\n",
      "Training Loss: 0.349 \t\t Validation Loss:0.773\n",
      "Starting epoch 87\n",
      "Loss after mini-batch     1: 0.091\n",
      "Loss after mini-batch    11: 0.325\n",
      "Loss after mini-batch    21: 0.360\n",
      "Loss after mini-batch    31: 0.919\n",
      "Loss after mini-batch    41: 0.189\n",
      "Loss after mini-batch    51: 0.497\n",
      "Loss after mini-batch    61: 0.166\n",
      "Loss after mini-batch    71: 0.312\n",
      "Training Loss: 0.838 \t\t Validation Loss:1.158\n",
      "Starting epoch 88\n",
      "Loss after mini-batch     1: 0.141\n",
      "Loss after mini-batch    11: 0.228\n",
      "Loss after mini-batch    21: 0.595\n",
      "Loss after mini-batch    31: 0.291\n",
      "Loss after mini-batch    41: 0.214\n",
      "Loss after mini-batch    51: 0.534\n",
      "Loss after mini-batch    61: 0.119\n",
      "Loss after mini-batch    71: 0.152\n",
      "Training Loss: 0.111 \t\t Validation Loss:0.324\n",
      "Starting epoch 89\n",
      "Loss after mini-batch     1: 0.123\n",
      "Loss after mini-batch    11: 0.211\n",
      "Loss after mini-batch    21: 0.199\n",
      "Loss after mini-batch    31: 0.399\n",
      "Loss after mini-batch    41: 0.411\n",
      "Loss after mini-batch    51: 3.836\n",
      "Loss after mini-batch    61: 0.200\n",
      "Loss after mini-batch    71: 0.555\n",
      "Training Loss: 0.509 \t\t Validation Loss:0.691\n",
      "Starting epoch 90\n",
      "Loss after mini-batch     1: 0.102\n",
      "Loss after mini-batch    11: 0.334\n",
      "Loss after mini-batch    21: 0.373\n",
      "Loss after mini-batch    31: 0.693\n",
      "Loss after mini-batch    41: 3.299\n",
      "Loss after mini-batch    51: 0.202\n",
      "Loss after mini-batch    61: 0.155\n",
      "Loss after mini-batch    71: 0.066\n",
      "Training Loss: 0.278 \t\t Validation Loss:0.870\n",
      "Starting epoch 91\n",
      "Loss after mini-batch     1: 0.508\n",
      "Loss after mini-batch    11: 0.532\n",
      "Loss after mini-batch    21: 0.558\n",
      "Loss after mini-batch    31: 0.470\n",
      "Loss after mini-batch    41: 0.161\n",
      "Loss after mini-batch    51: 1.013\n",
      "Loss after mini-batch    61: 0.400\n",
      "Loss after mini-batch    71: 0.084\n",
      "Training Loss: 0.129 \t\t Validation Loss:0.570\n",
      "Starting epoch 92\n",
      "Loss after mini-batch     1: 0.094\n",
      "Loss after mini-batch    11: 0.299\n",
      "Loss after mini-batch    21: 1.343\n",
      "Loss after mini-batch    31: 0.458\n",
      "Loss after mini-batch    41: 0.078\n",
      "Loss after mini-batch    51: 0.285\n",
      "Loss after mini-batch    61: 0.503\n",
      "Loss after mini-batch    71: 3.259\n",
      "Training Loss: 1.676 \t\t Validation Loss:3.248\n",
      "Starting epoch 93\n",
      "Loss after mini-batch     1: 0.145\n",
      "Loss after mini-batch    11: 0.154\n",
      "Loss after mini-batch    21: 1.355\n",
      "Loss after mini-batch    31: 0.961\n",
      "Loss after mini-batch    41: 0.549\n",
      "Loss after mini-batch    51: 0.028\n",
      "Loss after mini-batch    61: 0.561\n",
      "Loss after mini-batch    71: 0.522\n",
      "Training Loss: 0.562 \t\t Validation Loss:2.039\n",
      "Starting epoch 94\n",
      "Loss after mini-batch     1: 0.064\n",
      "Loss after mini-batch    11: 1.048\n",
      "Loss after mini-batch    21: 0.074\n",
      "Loss after mini-batch    31: 0.705\n",
      "Loss after mini-batch    41: 0.220\n",
      "Loss after mini-batch    51: 0.233\n",
      "Loss after mini-batch    61: 0.416\n",
      "Loss after mini-batch    71: 1.321\n",
      "Training Loss: 0.654 \t\t Validation Loss:0.968\n",
      "Starting epoch 95\n",
      "Loss after mini-batch     1: 0.246\n",
      "Loss after mini-batch    11: 0.282\n",
      "Loss after mini-batch    21: 0.391\n",
      "Loss after mini-batch    31: 0.153\n",
      "Loss after mini-batch    41: 0.093\n",
      "Loss after mini-batch    51: 0.064\n",
      "Loss after mini-batch    61: 3.661\n",
      "Loss after mini-batch    71: 0.249\n",
      "Training Loss: 0.969 \t\t Validation Loss:1.952\n",
      "Starting epoch 96\n",
      "Loss after mini-batch     1: 0.703\n",
      "Loss after mini-batch    11: 0.035\n",
      "Loss after mini-batch    21: 0.113\n",
      "Loss after mini-batch    31: 0.118\n",
      "Loss after mini-batch    41: 0.673\n",
      "Loss after mini-batch    51: 0.374\n",
      "Loss after mini-batch    61: 0.061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch    71: 0.162\n",
      "Training Loss: 3.967 \t\t Validation Loss:4.154\n",
      "Starting epoch 97\n",
      "Loss after mini-batch     1: 0.500\n",
      "Loss after mini-batch    11: 0.166\n",
      "Loss after mini-batch    21: 0.134\n",
      "Loss after mini-batch    31: 0.156\n",
      "Loss after mini-batch    41: 0.344\n",
      "Loss after mini-batch    51: 0.065\n",
      "Loss after mini-batch    61: 0.518\n",
      "Loss after mini-batch    71: 0.395\n",
      "Training Loss: 0.027 \t\t Validation Loss:0.720\n",
      "Starting epoch 98\n",
      "Loss after mini-batch     1: 0.211\n",
      "Loss after mini-batch    11: 1.521\n",
      "Loss after mini-batch    21: 0.288\n",
      "Loss after mini-batch    31: 0.402\n",
      "Loss after mini-batch    41: 0.605\n",
      "Loss after mini-batch    51: 0.944\n",
      "Loss after mini-batch    61: 0.119\n",
      "Loss after mini-batch    71: 0.080\n",
      "Training Loss: 0.480 \t\t Validation Loss:1.379\n",
      "Starting epoch 99\n",
      "Loss after mini-batch     1: 0.072\n",
      "Loss after mini-batch    11: 0.360\n",
      "Loss after mini-batch    21: 0.106\n",
      "Loss after mini-batch    31: 0.068\n",
      "Loss after mini-batch    41: 0.290\n",
      "Loss after mini-batch    51: 0.049\n",
      "Loss after mini-batch    61: 0.039\n",
      "Loss after mini-batch    71: 0.466\n",
      "Training Loss: 0.095 \t\t Validation Loss:2.362\n",
      "Starting epoch 100\n",
      "Loss after mini-batch     1: 0.103\n",
      "Loss after mini-batch    11: 0.106\n",
      "Loss after mini-batch    21: 0.167\n",
      "Loss after mini-batch    31: 0.763\n",
      "Loss after mini-batch    41: 0.263\n",
      "Loss after mini-batch    51: 1.183\n",
      "Loss after mini-batch    61: 0.208\n",
      "Loss after mini-batch    71: 0.716\n",
      "Training Loss: 0.148 \t\t Validation Loss:0.330\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "history_train = np.empty((1,))\n",
    "history_val = np.empty((1,))\n",
    "for epoch in range(0, 100): # 5 epochs at maximum  \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "          # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "#                 (i + 1, current_loss / 500))\n",
    "                  (i + 1, loss.item()))\n",
    "            current_loss = 0.0\n",
    "    history_train = np.append(history_train, current_loss)\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    mlp.eval()     # Optional when not using Model Specific layer\n",
    "    for i, data in enumerate(validloader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        output_val = mlp(inputs)\n",
    "        valid_loss = loss_function(output_val, targets)\n",
    "    \n",
    "        valid_loss += loss.item()\n",
    "    history_val = np.append(history_val, valid_loss.item())\n",
    "    print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "         '{:.3f}'.format(loss.item(), valid_loss.item()))\n",
    "#     print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "#           '{:.3f}'.format(current_loss / len(trainloader), valid_loss / len(validloader)))\n",
    "#     if min_valid_loss > valid_loss:\n",
    "#         print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "#         min_valid_loss = valid_loss\n",
    "#         # Saving State Dict\n",
    "#         torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b9d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.random.randn(13)\n",
    "# torch usa tensores de torch y no numpy.darrays\n",
    "dtype = torch.float\n",
    "test = torch.randn((1, 3), device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3332d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c6a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.502007484436035"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27fc4848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0, 0], X_test[0]\n",
    "# xtest = [x[0] for x in X_test]\n",
    "ypred = [y[0].item() for y in y_pred]\n",
    "ytest = [y[0].item() for y in y_test]\n",
    "diff=np.array(ytest)-np.array(ypred)\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9bb27d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "87b7e8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.5)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ys, _, _ = plt.hist(diff,bins=100)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0,4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbab3796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([51.,  4.,  3.,  3.,  4.,  3.,  0.,  2.,  3.,  0.,  2.,  1.,  0.,\n",
       "        1.,  2.,  1.,  0.,  1.,  2.,  0.,  0.,  1.,  1.,  1.,  0.,  0.,\n",
       "        1.,  0.,  0.,  0.,  0.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "        0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
       "        0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f558b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc17bf24190>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = np.arange(0, 101)\n",
    "plt.plot(ep, history_train, label='train')\n",
    "plt.plot(ep, history_val, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1355f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58b48c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101,), (101,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(history_train), np.shape(history_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4744941b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
