{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee35f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fd99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om\t\t\t\\Omega_m\n",
    "# Obh2\t\t\t\\Omega_{b}h^2\n",
    "# h\t\n",
    "datafile = 'chains/LCDM_phy_HD_nested_dynesty_multi_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc69788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataSet(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Prepare the dataset for regression\n",
    "    '''\n",
    "    def __init__(self, X, y, scale_data=False):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c80912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ncols = 3\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(ncols, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e24b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1036, 3) (1036, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Load Boston dataset\n",
    "    X = np.loadtxt(datafile, usecols=(2,3,4))\n",
    "    y = np.loadtxt(datafile, usecols=1).reshape(-1, 1)\n",
    "    randomize = np.random.permutation(len(X))\n",
    "    X = X[randomize]\n",
    "    y = y[randomize]\n",
    "    print(np.shape(X), np.shape(y))\n",
    "    X_test, y_test = X[:100, :], y[:100, :]\n",
    "    X, y = X[100:, :], y[100:, :]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f173672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LoadDataSet(X_train, y_train)\n",
    "dataset_val = LoadDataSet(X_val, y_val)\n",
    "# dataset_test = LoadDataSet(X_test, y_test)\n",
    "X_test, y_test = torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=1, shuffle=True, num_workers=1)\n",
    "validloader = torch.utils.data.DataLoader(dataset_val, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ea89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "mlp.float()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f533ceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       800\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       40,200\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       201\n",
       "=================================================================\n",
       "Total params: 41,201\n",
       "Trainable params: 41,201\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mlp, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00af4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 26.837\n",
      "Loss after mini-batch    11: 65.791\n",
      "Loss after mini-batch    21: 15.771\n",
      "Loss after mini-batch    31: 18.729\n",
      "Loss after mini-batch    41: 6.745\n",
      "Loss after mini-batch    51: 11.396\n",
      "Loss after mini-batch    61: 7.034\n",
      "Loss after mini-batch    71: 9.380\n",
      "Loss after mini-batch    81: 7.546\n",
      "Loss after mini-batch    91: 6.056\n",
      "Loss after mini-batch   101: 14.874\n",
      "Loss after mini-batch   111: 5.768\n",
      "Loss after mini-batch   121: 5.607\n",
      "Loss after mini-batch   131: 5.324\n",
      "Loss after mini-batch   141: 10.645\n",
      "Loss after mini-batch   151: 5.002\n",
      "Loss after mini-batch   161: 99.939\n",
      "Loss after mini-batch   171: 4.500\n",
      "Loss after mini-batch   181: 4.247\n",
      "Loss after mini-batch   191: 6.383\n",
      "Loss after mini-batch   201: 4.043\n",
      "Loss after mini-batch   211: 16.813\n",
      "Loss after mini-batch   221: 3.354\n",
      "Loss after mini-batch   231: 3.548\n",
      "Loss after mini-batch   241: 2.289\n",
      "Loss after mini-batch   251: 17.298\n",
      "Loss after mini-batch   261: 1.567\n",
      "Loss after mini-batch   271: 0.980\n",
      "Loss after mini-batch   281: 23.955\n",
      "Loss after mini-batch   291: 1.027\n",
      "Loss after mini-batch   301: 4.583\n",
      "Loss after mini-batch   311: 0.430\n",
      "Loss after mini-batch   321: 0.435\n",
      "Loss after mini-batch   331: 0.177\n",
      "Loss after mini-batch   341: 0.563\n",
      "Loss after mini-batch   351: 6.667\n",
      "Loss after mini-batch   361: 0.424\n",
      "Loss after mini-batch   371: 0.531\n",
      "Loss after mini-batch   381: 1.099\n",
      "Loss after mini-batch   391: 0.583\n",
      "Loss after mini-batch   401: 0.148\n",
      "Loss after mini-batch   411: 14.157\n",
      "Loss after mini-batch   421: 0.070\n",
      "Loss after mini-batch   431: 0.866\n",
      "Loss after mini-batch   441: 0.675\n",
      "Loss after mini-batch   451: 0.481\n",
      "Loss after mini-batch   461: 0.267\n",
      "Loss after mini-batch   471: 0.357\n",
      "Loss after mini-batch   481: 25.249\n",
      "Loss after mini-batch   491: 1.124\n",
      "Loss after mini-batch   501: 37.994\n",
      "Loss after mini-batch   511: 31.314\n",
      "Loss after mini-batch   521: 0.620\n",
      "Loss after mini-batch   531: 2.632\n",
      "Loss after mini-batch   541: 0.563\n",
      "Loss after mini-batch   551: 29.260\n",
      "Loss after mini-batch   561: 0.100\n",
      "Loss after mini-batch   571: 0.215\n",
      "Loss after mini-batch   581: 1.025\n",
      "Loss after mini-batch   591: 0.315\n",
      "Loss after mini-batch   601: 1.047\n",
      "Loss after mini-batch   611: 0.484\n",
      "Loss after mini-batch   621: 0.100\n",
      "Loss after mini-batch   631: 0.109\n",
      "Loss after mini-batch   641: 0.567\n",
      "Loss after mini-batch   651: 18.561\n",
      "Loss after mini-batch   661: 0.681\n",
      "Loss after mini-batch   671: 1.066\n",
      "Loss after mini-batch   681: 51.292\n",
      "Loss after mini-batch   691: 0.051\n",
      "Loss after mini-batch   701: 0.489\n",
      "Loss after mini-batch   711: 27.721\n",
      "Loss after mini-batch   721: 0.549\n",
      "Loss after mini-batch   731: 0.584\n",
      "Loss after mini-batch   741: 3.185\n",
      "Training Loss: 0.532 \t\t Validation Loss:1.106\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 20.978\n",
      "Loss after mini-batch    11: 0.711\n",
      "Loss after mini-batch    21: 7.334\n",
      "Loss after mini-batch    31: 0.264\n",
      "Loss after mini-batch    41: 1.905\n",
      "Loss after mini-batch    51: 0.106\n",
      "Loss after mini-batch    61: 0.424\n",
      "Loss after mini-batch    71: 64.054\n",
      "Loss after mini-batch    81: 2.423\n",
      "Loss after mini-batch    91: 13.823\n",
      "Loss after mini-batch   101: 2.418\n",
      "Loss after mini-batch   111: 0.680\n",
      "Loss after mini-batch   121: 0.063\n",
      "Loss after mini-batch   131: 1.062\n",
      "Loss after mini-batch   141: 0.553\n",
      "Loss after mini-batch   151: 0.666\n",
      "Loss after mini-batch   161: 0.213\n",
      "Loss after mini-batch   171: 3.861\n",
      "Loss after mini-batch   181: 0.846\n",
      "Loss after mini-batch   191: 0.761\n",
      "Loss after mini-batch   201: 1.987\n",
      "Loss after mini-batch   211: 0.926\n",
      "Loss after mini-batch   221: 0.299\n",
      "Loss after mini-batch   231: 0.649\n",
      "Loss after mini-batch   241: 0.604\n",
      "Loss after mini-batch   251: 0.321\n",
      "Loss after mini-batch   261: 0.442\n",
      "Loss after mini-batch   271: 4.299\n",
      "Loss after mini-batch   281: 0.061\n",
      "Loss after mini-batch   291: 4.805\n",
      "Loss after mini-batch   301: 0.391\n",
      "Loss after mini-batch   311: 1.938\n",
      "Loss after mini-batch   321: 0.253\n",
      "Loss after mini-batch   331: 0.392\n",
      "Loss after mini-batch   341: 1.031\n",
      "Loss after mini-batch   351: 75.822\n",
      "Loss after mini-batch   361: 2.800\n",
      "Loss after mini-batch   371: 0.136\n",
      "Loss after mini-batch   381: 79.529\n",
      "Loss after mini-batch   391: 0.929\n",
      "Loss after mini-batch   401: 56.021\n",
      "Loss after mini-batch   411: 23.482\n",
      "Loss after mini-batch   421: 5.945\n",
      "Loss after mini-batch   431: 0.555\n",
      "Loss after mini-batch   441: 0.377\n",
      "Loss after mini-batch   451: 0.913\n",
      "Loss after mini-batch   461: 1.986\n",
      "Loss after mini-batch   471: 0.737\n",
      "Loss after mini-batch   481: 24.353\n",
      "Loss after mini-batch   491: 0.738\n",
      "Loss after mini-batch   501: 0.768\n",
      "Loss after mini-batch   511: 7.160\n",
      "Loss after mini-batch   521: 0.935\n",
      "Loss after mini-batch   531: 5.175\n",
      "Loss after mini-batch   541: 0.883\n",
      "Loss after mini-batch   551: 0.806\n",
      "Loss after mini-batch   561: 0.865\n",
      "Loss after mini-batch   571: 1.786\n",
      "Loss after mini-batch   581: 0.663\n",
      "Loss after mini-batch   591: 0.069\n",
      "Loss after mini-batch   601: 0.695\n",
      "Loss after mini-batch   611: 0.186\n",
      "Loss after mini-batch   621: 0.646\n",
      "Loss after mini-batch   631: 14.784\n",
      "Loss after mini-batch   641: 0.905\n",
      "Loss after mini-batch   651: 0.840\n",
      "Loss after mini-batch   661: 0.491\n",
      "Loss after mini-batch   671: 0.748\n",
      "Loss after mini-batch   681: 8.643\n",
      "Loss after mini-batch   691: 36.138\n",
      "Loss after mini-batch   701: 0.381\n",
      "Loss after mini-batch   711: 0.964\n",
      "Loss after mini-batch   721: 0.540\n",
      "Loss after mini-batch   731: 0.365\n",
      "Loss after mini-batch   741: 1.035\n",
      "Training Loss: 0.686 \t\t Validation Loss:21.478\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 1.009\n",
      "Loss after mini-batch    11: 1.319\n",
      "Loss after mini-batch    21: 0.452\n",
      "Loss after mini-batch    31: 0.723\n",
      "Loss after mini-batch    41: 0.794\n",
      "Loss after mini-batch    51: 0.272\n",
      "Loss after mini-batch    61: 9.196\n",
      "Loss after mini-batch    71: 0.057\n",
      "Loss after mini-batch    81: 0.286\n",
      "Loss after mini-batch    91: 0.644\n",
      "Loss after mini-batch   101: 66.220\n",
      "Loss after mini-batch   111: 0.349\n",
      "Loss after mini-batch   121: 0.606\n",
      "Loss after mini-batch   131: 56.270\n",
      "Loss after mini-batch   141: 13.500\n",
      "Loss after mini-batch   151: 0.052\n",
      "Loss after mini-batch   161: 0.871\n",
      "Loss after mini-batch   171: 0.291\n",
      "Loss after mini-batch   181: 9.367\n",
      "Loss after mini-batch   191: 2.173\n",
      "Loss after mini-batch   201: 0.303\n",
      "Loss after mini-batch   211: 0.469\n",
      "Loss after mini-batch   221: 1.254\n",
      "Loss after mini-batch   231: 39.476\n",
      "Loss after mini-batch   241: 1.766\n",
      "Loss after mini-batch   251: 15.078\n",
      "Loss after mini-batch   261: 1.037\n",
      "Loss after mini-batch   271: 8.789\n",
      "Loss after mini-batch   281: 0.817\n",
      "Loss after mini-batch   291: 0.373\n",
      "Loss after mini-batch   301: 0.148\n",
      "Loss after mini-batch   311: 0.993\n",
      "Loss after mini-batch   321: 0.782\n",
      "Loss after mini-batch   331: 39.508\n",
      "Loss after mini-batch   341: 0.639\n",
      "Loss after mini-batch   351: 0.875\n",
      "Loss after mini-batch   361: 0.343\n",
      "Loss after mini-batch   371: 0.081\n",
      "Loss after mini-batch   381: 13.753\n",
      "Loss after mini-batch   391: 0.828\n",
      "Loss after mini-batch   401: 31.829\n",
      "Loss after mini-batch   411: 0.781\n",
      "Loss after mini-batch   421: 0.017\n",
      "Loss after mini-batch   431: 0.144\n",
      "Loss after mini-batch   441: 7.339\n",
      "Loss after mini-batch   451: 0.006\n",
      "Loss after mini-batch   461: 0.148\n",
      "Loss after mini-batch   471: 0.545\n",
      "Loss after mini-batch   481: 0.846\n",
      "Loss after mini-batch   491: 0.507\n",
      "Loss after mini-batch   501: 0.391\n",
      "Loss after mini-batch   511: 1.776\n",
      "Loss after mini-batch   521: 0.556\n",
      "Loss after mini-batch   531: 63.597\n",
      "Loss after mini-batch   541: 0.896\n",
      "Loss after mini-batch   551: 0.070\n",
      "Loss after mini-batch   561: 0.753\n",
      "Loss after mini-batch   571: 0.673\n",
      "Loss after mini-batch   581: 0.669\n",
      "Loss after mini-batch   591: 0.154\n",
      "Loss after mini-batch   601: 16.393\n",
      "Loss after mini-batch   611: 0.598\n",
      "Loss after mini-batch   621: 0.500\n",
      "Loss after mini-batch   631: 0.519\n",
      "Loss after mini-batch   641: 4.357\n",
      "Loss after mini-batch   651: 4.527\n",
      "Loss after mini-batch   661: 35.043\n",
      "Loss after mini-batch   671: 0.522\n",
      "Loss after mini-batch   681: 3.720\n",
      "Loss after mini-batch   691: 0.470\n",
      "Loss after mini-batch   701: 0.914\n",
      "Loss after mini-batch   711: 0.496\n",
      "Loss after mini-batch   721: 5.154\n",
      "Loss after mini-batch   731: 0.766\n",
      "Loss after mini-batch   741: 0.224\n",
      "Training Loss: 11.060 \t\t Validation Loss:11.121\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 0.112\n",
      "Loss after mini-batch    11: 14.240\n",
      "Loss after mini-batch    21: 0.117\n",
      "Loss after mini-batch    31: 1.298\n",
      "Loss after mini-batch    41: 0.715\n",
      "Loss after mini-batch    51: 0.020\n",
      "Loss after mini-batch    61: 5.995\n",
      "Loss after mini-batch    71: 0.486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch    81: 0.989\n",
      "Loss after mini-batch    91: 0.555\n",
      "Loss after mini-batch   101: 0.620\n",
      "Loss after mini-batch   111: 0.588\n",
      "Loss after mini-batch   121: 4.675\n",
      "Loss after mini-batch   131: 0.706\n",
      "Loss after mini-batch   141: 0.782\n",
      "Loss after mini-batch   151: 0.382\n",
      "Loss after mini-batch   161: 11.947\n",
      "Loss after mini-batch   171: 5.167\n",
      "Loss after mini-batch   181: 0.809\n",
      "Loss after mini-batch   191: 0.298\n",
      "Loss after mini-batch   201: 2.461\n",
      "Loss after mini-batch   211: 0.188\n",
      "Loss after mini-batch   221: 0.790\n",
      "Loss after mini-batch   231: 16.800\n",
      "Loss after mini-batch   241: 22.988\n",
      "Loss after mini-batch   251: 0.164\n",
      "Loss after mini-batch   261: 3.977\n",
      "Loss after mini-batch   271: 0.477\n",
      "Loss after mini-batch   281: 0.380\n",
      "Loss after mini-batch   291: 0.090\n",
      "Loss after mini-batch   301: 9.129\n",
      "Loss after mini-batch   311: 9.792\n",
      "Loss after mini-batch   321: 0.546\n",
      "Loss after mini-batch   331: 0.532\n",
      "Loss after mini-batch   341: 1.481\n",
      "Loss after mini-batch   351: 0.388\n",
      "Loss after mini-batch   361: 0.569\n",
      "Loss after mini-batch   371: 0.600\n",
      "Loss after mini-batch   381: 0.649\n",
      "Loss after mini-batch   391: 0.866\n",
      "Loss after mini-batch   401: 0.635\n",
      "Loss after mini-batch   411: 4.207\n",
      "Loss after mini-batch   421: 1.199\n",
      "Loss after mini-batch   431: 9.014\n",
      "Loss after mini-batch   441: 1.384\n",
      "Loss after mini-batch   451: 0.911\n",
      "Loss after mini-batch   461: 12.940\n",
      "Loss after mini-batch   471: 2.484\n",
      "Loss after mini-batch   481: 14.477\n",
      "Loss after mini-batch   491: 1.029\n",
      "Loss after mini-batch   501: 1.336\n",
      "Loss after mini-batch   511: 16.478\n",
      "Loss after mini-batch   521: 0.837\n",
      "Loss after mini-batch   531: 10.593\n",
      "Loss after mini-batch   541: 30.585\n",
      "Loss after mini-batch   551: 0.691\n",
      "Loss after mini-batch   561: 0.844\n",
      "Loss after mini-batch   571: 0.579\n",
      "Loss after mini-batch   581: 3.973\n",
      "Loss after mini-batch   591: 35.717\n",
      "Loss after mini-batch   601: 0.896\n",
      "Loss after mini-batch   611: 0.494\n",
      "Loss after mini-batch   621: 17.133\n",
      "Loss after mini-batch   631: 1.626\n",
      "Loss after mini-batch   641: 1.487\n",
      "Loss after mini-batch   651: 8.257\n",
      "Loss after mini-batch   661: 1.272\n",
      "Loss after mini-batch   671: 1.123\n",
      "Loss after mini-batch   681: 4.968\n",
      "Loss after mini-batch   691: 0.811\n",
      "Loss after mini-batch   701: 0.434\n",
      "Loss after mini-batch   711: 0.498\n",
      "Loss after mini-batch   721: 0.905\n",
      "Loss after mini-batch   731: 0.457\n",
      "Loss after mini-batch   741: 0.584\n",
      "Training Loss: 0.130 \t\t Validation Loss:1.997\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 0.447\n",
      "Loss after mini-batch    11: 0.789\n",
      "Loss after mini-batch    21: 0.403\n",
      "Loss after mini-batch    31: 0.364\n",
      "Loss after mini-batch    41: 0.198\n",
      "Loss after mini-batch    51: 0.381\n",
      "Loss after mini-batch    61: 0.076\n",
      "Loss after mini-batch    71: 0.338\n",
      "Loss after mini-batch    81: 0.105\n",
      "Loss after mini-batch    91: 0.292\n",
      "Loss after mini-batch   101: 0.555\n",
      "Loss after mini-batch   111: 3.786\n",
      "Loss after mini-batch   121: 4.706\n",
      "Loss after mini-batch   131: 0.438\n",
      "Loss after mini-batch   141: 2.485\n",
      "Loss after mini-batch   151: 17.184\n",
      "Loss after mini-batch   161: 25.675\n",
      "Loss after mini-batch   171: 0.390\n",
      "Loss after mini-batch   181: 0.721\n",
      "Loss after mini-batch   191: 0.186\n",
      "Loss after mini-batch   201: 0.367\n",
      "Loss after mini-batch   211: 36.620\n",
      "Loss after mini-batch   221: 0.060\n",
      "Loss after mini-batch   231: 0.109\n",
      "Loss after mini-batch   241: 0.540\n",
      "Loss after mini-batch   251: 0.152\n",
      "Loss after mini-batch   261: 0.657\n",
      "Loss after mini-batch   271: 0.653\n",
      "Loss after mini-batch   281: 14.963\n",
      "Loss after mini-batch   291: 0.761\n",
      "Loss after mini-batch   301: 2.932\n",
      "Loss after mini-batch   311: 0.964\n",
      "Loss after mini-batch   321: 0.439\n",
      "Loss after mini-batch   331: 0.615\n",
      "Loss after mini-batch   341: 12.818\n",
      "Loss after mini-batch   351: 18.635\n",
      "Loss after mini-batch   361: 0.763\n",
      "Loss after mini-batch   371: 0.012\n",
      "Loss after mini-batch   381: 0.828\n",
      "Loss after mini-batch   391: 15.536\n",
      "Loss after mini-batch   401: 0.823\n",
      "Loss after mini-batch   411: 0.736\n",
      "Loss after mini-batch   421: 2.061\n",
      "Loss after mini-batch   431: 0.943\n",
      "Loss after mini-batch   441: 0.678\n",
      "Loss after mini-batch   451: 0.759\n",
      "Loss after mini-batch   461: 20.365\n",
      "Loss after mini-batch   471: 11.571\n",
      "Loss after mini-batch   481: 0.491\n",
      "Loss after mini-batch   491: 1.790\n",
      "Loss after mini-batch   501: 0.417\n",
      "Loss after mini-batch   511: 0.696\n",
      "Loss after mini-batch   521: 2.157\n",
      "Loss after mini-batch   531: 3.332\n",
      "Loss after mini-batch   541: 0.811\n",
      "Loss after mini-batch   551: 0.418\n",
      "Loss after mini-batch   561: 0.976\n",
      "Loss after mini-batch   571: 0.271\n",
      "Loss after mini-batch   581: 0.036\n",
      "Loss after mini-batch   591: 0.398\n",
      "Loss after mini-batch   601: 0.832\n",
      "Loss after mini-batch   611: 63.284\n",
      "Loss after mini-batch   621: 0.977\n",
      "Loss after mini-batch   631: 0.994\n",
      "Loss after mini-batch   641: 1.330\n",
      "Loss after mini-batch   651: 0.391\n",
      "Loss after mini-batch   661: 0.285\n",
      "Loss after mini-batch   671: 2.431\n",
      "Loss after mini-batch   681: 1.028\n",
      "Loss after mini-batch   691: 12.655\n",
      "Loss after mini-batch   701: 0.666\n",
      "Loss after mini-batch   711: 0.446\n",
      "Loss after mini-batch   721: 0.473\n",
      "Loss after mini-batch   731: 0.445\n",
      "Loss after mini-batch   741: 0.535\n",
      "Training Loss: 0.469 \t\t Validation Loss:0.855\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.403\n",
      "Loss after mini-batch    11: 0.065\n",
      "Loss after mini-batch    21: 1.183\n",
      "Loss after mini-batch    31: 0.630\n",
      "Loss after mini-batch    41: 0.578\n",
      "Loss after mini-batch    51: 0.659\n",
      "Loss after mini-batch    61: 0.416\n",
      "Loss after mini-batch    71: 9.033\n",
      "Loss after mini-batch    81: 0.670\n",
      "Loss after mini-batch    91: 0.359\n",
      "Loss after mini-batch   101: 0.400\n",
      "Loss after mini-batch   111: 0.617\n",
      "Loss after mini-batch   121: 0.231\n",
      "Loss after mini-batch   131: 0.563\n",
      "Loss after mini-batch   141: 0.663\n",
      "Loss after mini-batch   151: 0.650\n",
      "Loss after mini-batch   161: 0.353\n",
      "Loss after mini-batch   171: 0.311\n",
      "Loss after mini-batch   181: 0.826\n",
      "Loss after mini-batch   191: 39.114\n",
      "Loss after mini-batch   201: 0.417\n",
      "Loss after mini-batch   211: 0.388\n",
      "Loss after mini-batch   221: 0.576\n",
      "Loss after mini-batch   231: 0.674\n",
      "Loss after mini-batch   241: 0.588\n",
      "Loss after mini-batch   251: 25.677\n",
      "Loss after mini-batch   261: 0.525\n",
      "Loss after mini-batch   271: 1.050\n",
      "Loss after mini-batch   281: 2.148\n",
      "Loss after mini-batch   291: 6.629\n",
      "Loss after mini-batch   301: 0.443\n",
      "Loss after mini-batch   311: 0.643\n",
      "Loss after mini-batch   321: 0.694\n",
      "Loss after mini-batch   331: 0.498\n",
      "Loss after mini-batch   341: 0.288\n",
      "Loss after mini-batch   351: 0.061\n",
      "Loss after mini-batch   361: 0.729\n",
      "Loss after mini-batch   371: 23.808\n",
      "Loss after mini-batch   381: 0.662\n",
      "Loss after mini-batch   391: 0.207\n",
      "Loss after mini-batch   401: 0.377\n",
      "Loss after mini-batch   411: 0.383\n",
      "Loss after mini-batch   421: 2.006\n",
      "Loss after mini-batch   431: 0.639\n",
      "Loss after mini-batch   441: 0.691\n",
      "Loss after mini-batch   451: 0.427\n",
      "Loss after mini-batch   461: 0.343\n",
      "Loss after mini-batch   471: 0.507\n",
      "Loss after mini-batch   481: 1.097\n",
      "Loss after mini-batch   491: 56.202\n",
      "Loss after mini-batch   501: 0.933\n",
      "Loss after mini-batch   511: 0.318\n",
      "Loss after mini-batch   521: 1.150\n",
      "Loss after mini-batch   531: 15.159\n",
      "Loss after mini-batch   541: 7.041\n",
      "Loss after mini-batch   551: 1.111\n",
      "Loss after mini-batch   561: 1.214\n",
      "Loss after mini-batch   571: 0.620\n",
      "Loss after mini-batch   581: 0.294\n",
      "Loss after mini-batch   591: 0.319\n",
      "Loss after mini-batch   601: 0.140\n",
      "Loss after mini-batch   611: 0.098\n",
      "Loss after mini-batch   621: 1.665\n",
      "Loss after mini-batch   631: 0.639\n",
      "Loss after mini-batch   641: 0.596\n",
      "Loss after mini-batch   651: 14.958\n",
      "Loss after mini-batch   661: 0.644\n",
      "Loss after mini-batch   671: 6.428\n",
      "Loss after mini-batch   681: 0.980\n",
      "Loss after mini-batch   691: 0.031\n",
      "Loss after mini-batch   701: 0.501\n",
      "Loss after mini-batch   711: 14.248\n",
      "Loss after mini-batch   721: 1.466\n",
      "Loss after mini-batch   731: 6.154\n",
      "Loss after mini-batch   741: 0.532\n",
      "Training Loss: 3.233 \t\t Validation Loss:3.807\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 1.111\n",
      "Loss after mini-batch    11: 0.007\n",
      "Loss after mini-batch    21: 1.524\n",
      "Loss after mini-batch    31: 0.636\n",
      "Loss after mini-batch    41: 0.583\n",
      "Loss after mini-batch    51: 7.655\n",
      "Loss after mini-batch    61: 0.522\n",
      "Loss after mini-batch    71: 0.307\n",
      "Loss after mini-batch    81: 0.860\n",
      "Loss after mini-batch    91: 8.850\n",
      "Loss after mini-batch   101: 0.196\n",
      "Loss after mini-batch   111: 0.205\n",
      "Loss after mini-batch   121: 0.525\n",
      "Loss after mini-batch   131: 14.316\n",
      "Loss after mini-batch   141: 15.341\n",
      "Loss after mini-batch   151: 0.134\n",
      "Loss after mini-batch   161: 5.561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   171: 2.248\n",
      "Loss after mini-batch   181: 0.087\n",
      "Loss after mini-batch   191: 24.152\n",
      "Loss after mini-batch   201: 1.188\n",
      "Loss after mini-batch   211: 0.502\n",
      "Loss after mini-batch   221: 0.459\n",
      "Loss after mini-batch   231: 4.043\n",
      "Loss after mini-batch   241: 4.303\n",
      "Loss after mini-batch   251: 0.501\n",
      "Loss after mini-batch   261: 0.645\n",
      "Loss after mini-batch   271: 0.187\n",
      "Loss after mini-batch   281: 1.485\n",
      "Loss after mini-batch   291: 0.797\n",
      "Loss after mini-batch   301: 0.028\n",
      "Loss after mini-batch   311: 29.063\n",
      "Loss after mini-batch   321: 0.471\n",
      "Loss after mini-batch   331: 0.577\n",
      "Loss after mini-batch   341: 0.501\n",
      "Loss after mini-batch   351: 0.275\n",
      "Loss after mini-batch   361: 0.570\n",
      "Loss after mini-batch   371: 0.598\n",
      "Loss after mini-batch   381: 0.571\n",
      "Loss after mini-batch   391: 0.090\n",
      "Loss after mini-batch   401: 0.509\n",
      "Loss after mini-batch   411: 0.100\n",
      "Loss after mini-batch   421: 2.182\n",
      "Loss after mini-batch   431: 0.942\n",
      "Loss after mini-batch   441: 7.921\n",
      "Loss after mini-batch   451: 0.652\n",
      "Loss after mini-batch   461: 31.444\n",
      "Loss after mini-batch   471: 0.482\n",
      "Loss after mini-batch   481: 56.214\n",
      "Loss after mini-batch   491: 14.572\n",
      "Loss after mini-batch   501: 0.893\n",
      "Loss after mini-batch   511: 2.447\n",
      "Loss after mini-batch   521: 23.107\n",
      "Loss after mini-batch   531: 1.225\n",
      "Loss after mini-batch   541: 1.202\n",
      "Loss after mini-batch   551: 1.003\n",
      "Loss after mini-batch   561: 21.955\n",
      "Loss after mini-batch   571: 5.579\n",
      "Loss after mini-batch   581: 0.584\n",
      "Loss after mini-batch   591: 56.373\n",
      "Loss after mini-batch   601: 0.155\n",
      "Loss after mini-batch   611: 38.497\n",
      "Loss after mini-batch   621: 0.670\n",
      "Loss after mini-batch   631: 21.045\n",
      "Loss after mini-batch   641: 0.175\n",
      "Loss after mini-batch   651: 0.259\n",
      "Loss after mini-batch   661: 0.650\n",
      "Loss after mini-batch   671: 3.840\n",
      "Loss after mini-batch   681: 0.215\n",
      "Loss after mini-batch   691: 3.185\n",
      "Loss after mini-batch   701: 0.403\n",
      "Loss after mini-batch   711: 0.545\n",
      "Loss after mini-batch   721: 0.672\n",
      "Loss after mini-batch   731: 0.825\n",
      "Loss after mini-batch   741: 0.283\n",
      "Training Loss: 0.613 \t\t Validation Loss:0.625\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 14.706\n",
      "Loss after mini-batch    11: 57.191\n",
      "Loss after mini-batch    21: 0.023\n",
      "Loss after mini-batch    31: 0.493\n",
      "Loss after mini-batch    41: 4.947\n",
      "Loss after mini-batch    51: 0.419\n",
      "Loss after mini-batch    61: 0.492\n",
      "Loss after mini-batch    71: 2.186\n",
      "Loss after mini-batch    81: 1.561\n",
      "Loss after mini-batch    91: 0.722\n",
      "Loss after mini-batch   101: 0.735\n",
      "Loss after mini-batch   111: 0.755\n",
      "Loss after mini-batch   121: 0.730\n",
      "Loss after mini-batch   131: 0.747\n",
      "Loss after mini-batch   141: 0.504\n",
      "Loss after mini-batch   151: 12.268\n",
      "Loss after mini-batch   161: 26.725\n",
      "Loss after mini-batch   171: 38.364\n",
      "Loss after mini-batch   181: 6.747\n",
      "Loss after mini-batch   191: 13.950\n",
      "Loss after mini-batch   201: 0.602\n",
      "Loss after mini-batch   211: 23.494\n",
      "Loss after mini-batch   221: 28.895\n",
      "Loss after mini-batch   231: 0.636\n",
      "Loss after mini-batch   241: 1.538\n",
      "Loss after mini-batch   251: 0.094\n",
      "Loss after mini-batch   261: 15.516\n",
      "Loss after mini-batch   271: 8.762\n",
      "Loss after mini-batch   281: 1.441\n",
      "Loss after mini-batch   291: 17.939\n",
      "Loss after mini-batch   301: 0.929\n",
      "Loss after mini-batch   311: 30.505\n",
      "Loss after mini-batch   321: 0.955\n",
      "Loss after mini-batch   331: 16.330\n",
      "Loss after mini-batch   341: 0.655\n",
      "Loss after mini-batch   351: 8.371\n",
      "Loss after mini-batch   361: 1.155\n",
      "Loss after mini-batch   371: 5.073\n",
      "Loss after mini-batch   381: 23.541\n",
      "Loss after mini-batch   391: 1.251\n",
      "Loss after mini-batch   401: 2.273\n",
      "Loss after mini-batch   411: 28.105\n",
      "Loss after mini-batch   421: 1.689\n",
      "Loss after mini-batch   431: 1.417\n",
      "Loss after mini-batch   441: 1.570\n",
      "Loss after mini-batch   451: 0.083\n",
      "Loss after mini-batch   461: 1.223\n",
      "Loss after mini-batch   471: 0.563\n",
      "Loss after mini-batch   481: 1.080\n",
      "Loss after mini-batch   491: 9.247\n",
      "Loss after mini-batch   501: 0.309\n",
      "Loss after mini-batch   511: 0.758\n",
      "Loss after mini-batch   521: 0.754\n",
      "Loss after mini-batch   531: 3.884\n",
      "Loss after mini-batch   541: 0.580\n",
      "Loss after mini-batch   551: 0.604\n",
      "Loss after mini-batch   561: 0.478\n",
      "Loss after mini-batch   571: 0.596\n",
      "Loss after mini-batch   581: 0.504\n",
      "Loss after mini-batch   591: 4.301\n",
      "Loss after mini-batch   601: 2.972\n",
      "Loss after mini-batch   611: 9.613\n",
      "Loss after mini-batch   621: 21.219\n",
      "Loss after mini-batch   631: 0.558\n",
      "Loss after mini-batch   641: 0.604\n",
      "Loss after mini-batch   651: 5.851\n",
      "Loss after mini-batch   661: 14.022\n",
      "Loss after mini-batch   671: 0.579\n",
      "Loss after mini-batch   681: 0.546\n",
      "Loss after mini-batch   691: 0.601\n",
      "Loss after mini-batch   701: 0.452\n",
      "Loss after mini-batch   711: 0.775\n",
      "Loss after mini-batch   721: 2.738\n",
      "Loss after mini-batch   731: 0.697\n",
      "Loss after mini-batch   741: 3.484\n",
      "Training Loss: 0.452 \t\t Validation Loss:0.960\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.049\n",
      "Loss after mini-batch    11: 0.061\n",
      "Loss after mini-batch    21: 0.384\n",
      "Loss after mini-batch    31: 0.174\n",
      "Loss after mini-batch    41: 0.173\n",
      "Loss after mini-batch    51: 0.002\n",
      "Loss after mini-batch    61: 1.850\n",
      "Loss after mini-batch    71: 0.430\n",
      "Loss after mini-batch    81: 0.262\n",
      "Loss after mini-batch    91: 0.272\n",
      "Loss after mini-batch   101: 6.123\n",
      "Loss after mini-batch   111: 1.800\n",
      "Loss after mini-batch   121: 3.024\n",
      "Loss after mini-batch   131: 3.946\n",
      "Loss after mini-batch   141: 0.561\n",
      "Loss after mini-batch   151: 0.258\n",
      "Loss after mini-batch   161: 2.047\n",
      "Loss after mini-batch   171: 0.575\n",
      "Loss after mini-batch   181: 1.214\n",
      "Loss after mini-batch   191: 0.098\n",
      "Loss after mini-batch   201: 10.171\n",
      "Loss after mini-batch   211: 7.168\n",
      "Loss after mini-batch   221: 56.057\n",
      "Loss after mini-batch   231: 35.930\n",
      "Loss after mini-batch   241: 9.210\n",
      "Loss after mini-batch   251: 24.441\n",
      "Loss after mini-batch   261: 0.515\n",
      "Loss after mini-batch   271: 0.700\n",
      "Loss after mini-batch   281: 0.585\n",
      "Loss after mini-batch   291: 0.575\n",
      "Loss after mini-batch   301: 0.463\n",
      "Loss after mini-batch   311: 0.964\n",
      "Loss after mini-batch   321: 13.063\n",
      "Loss after mini-batch   331: 0.905\n",
      "Loss after mini-batch   341: 0.604\n",
      "Loss after mini-batch   351: 0.077\n",
      "Loss after mini-batch   361: 12.360\n",
      "Loss after mini-batch   371: 1.701\n",
      "Loss after mini-batch   381: 1.141\n",
      "Loss after mini-batch   391: 24.130\n",
      "Loss after mini-batch   401: 0.001\n",
      "Loss after mini-batch   411: 0.547\n",
      "Loss after mini-batch   421: 1.174\n",
      "Loss after mini-batch   431: 1.124\n",
      "Loss after mini-batch   441: 17.914\n",
      "Loss after mini-batch   451: 8.402\n",
      "Loss after mini-batch   461: 11.198\n",
      "Loss after mini-batch   471: 0.833\n",
      "Loss after mini-batch   481: 3.925\n",
      "Loss after mini-batch   491: 3.176\n",
      "Loss after mini-batch   501: 1.024\n",
      "Loss after mini-batch   511: 28.004\n",
      "Loss after mini-batch   521: 0.956\n",
      "Loss after mini-batch   531: 1.017\n",
      "Loss after mini-batch   541: 1.175\n",
      "Loss after mini-batch   551: 13.085\n",
      "Loss after mini-batch   561: 8.419\n",
      "Loss after mini-batch   571: 1.203\n",
      "Loss after mini-batch   581: 1.156\n",
      "Loss after mini-batch   591: 0.926\n",
      "Loss after mini-batch   601: 0.854\n",
      "Loss after mini-batch   611: 0.725\n",
      "Loss after mini-batch   621: 0.811\n",
      "Loss after mini-batch   631: 56.094\n",
      "Loss after mini-batch   641: 0.294\n",
      "Loss after mini-batch   651: 0.180\n",
      "Loss after mini-batch   661: 0.434\n",
      "Loss after mini-batch   671: 0.329\n",
      "Loss after mini-batch   681: 38.055\n",
      "Loss after mini-batch   691: 0.624\n",
      "Loss after mini-batch   701: 0.657\n",
      "Loss after mini-batch   711: 0.002\n",
      "Loss after mini-batch   721: 24.971\n",
      "Loss after mini-batch   731: 0.145\n",
      "Loss after mini-batch   741: 11.294\n",
      "Training Loss: 0.195 \t\t Validation Loss:0.769\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 1.700\n",
      "Loss after mini-batch    11: 7.414\n",
      "Loss after mini-batch    21: 0.446\n",
      "Loss after mini-batch    31: 0.509\n",
      "Loss after mini-batch    41: 9.854\n",
      "Loss after mini-batch    51: 1.338\n",
      "Loss after mini-batch    61: 0.661\n",
      "Loss after mini-batch    71: 0.618\n",
      "Loss after mini-batch    81: 18.267\n",
      "Loss after mini-batch    91: 0.784\n",
      "Loss after mini-batch   101: 0.619\n",
      "Loss after mini-batch   111: 0.281\n",
      "Loss after mini-batch   121: 1.482\n",
      "Loss after mini-batch   131: 0.448\n",
      "Loss after mini-batch   141: 1.224\n",
      "Loss after mini-batch   151: 2.783\n",
      "Loss after mini-batch   161: 0.162\n",
      "Loss after mini-batch   171: 0.738\n",
      "Loss after mini-batch   181: 0.580\n",
      "Loss after mini-batch   191: 0.212\n",
      "Loss after mini-batch   201: 0.907\n",
      "Loss after mini-batch   211: 1.010\n",
      "Loss after mini-batch   221: 14.735\n",
      "Loss after mini-batch   231: 0.775\n",
      "Loss after mini-batch   241: 0.598\n",
      "Loss after mini-batch   251: 0.460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   261: 0.427\n",
      "Loss after mini-batch   271: 0.305\n",
      "Loss after mini-batch   281: 0.358\n",
      "Loss after mini-batch   291: 0.359\n",
      "Loss after mini-batch   301: 0.463\n",
      "Loss after mini-batch   311: 2.251\n",
      "Loss after mini-batch   321: 5.493\n",
      "Loss after mini-batch   331: 0.405\n",
      "Loss after mini-batch   341: 0.952\n",
      "Loss after mini-batch   351: 19.973\n",
      "Loss after mini-batch   361: 0.592\n",
      "Loss after mini-batch   371: 0.335\n",
      "Loss after mini-batch   381: 11.377\n",
      "Loss after mini-batch   391: 15.305\n",
      "Loss after mini-batch   401: 0.495\n",
      "Loss after mini-batch   411: 1.008\n",
      "Loss after mini-batch   421: 45.516\n",
      "Loss after mini-batch   431: 1.006\n",
      "Loss after mini-batch   441: 0.573\n",
      "Loss after mini-batch   451: 0.871\n",
      "Loss after mini-batch   461: 0.329\n",
      "Loss after mini-batch   471: 0.671\n",
      "Loss after mini-batch   481: 0.135\n",
      "Loss after mini-batch   491: 4.505\n",
      "Loss after mini-batch   501: 0.768\n",
      "Loss after mini-batch   511: 0.307\n",
      "Loss after mini-batch   521: 0.388\n",
      "Loss after mini-batch   531: 5.040\n",
      "Loss after mini-batch   541: 0.864\n",
      "Loss after mini-batch   551: 0.921\n",
      "Loss after mini-batch   561: 1.044\n",
      "Loss after mini-batch   571: 0.668\n",
      "Loss after mini-batch   581: 2.048\n",
      "Loss after mini-batch   591: 0.879\n",
      "Loss after mini-batch   601: 25.973\n",
      "Loss after mini-batch   611: 28.328\n",
      "Loss after mini-batch   621: 0.742\n",
      "Loss after mini-batch   631: 1.098\n",
      "Loss after mini-batch   641: 5.582\n",
      "Loss after mini-batch   651: 3.398\n",
      "Loss after mini-batch   661: 4.282\n",
      "Loss after mini-batch   671: 0.888\n",
      "Loss after mini-batch   681: 0.970\n",
      "Loss after mini-batch   691: 0.749\n",
      "Loss after mini-batch   701: 0.434\n",
      "Loss after mini-batch   711: 0.602\n",
      "Loss after mini-batch   721: 18.069\n",
      "Loss after mini-batch   731: 0.644\n",
      "Loss after mini-batch   741: 4.043\n",
      "Training Loss: 0.284 \t\t Validation Loss:5.962\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 6.373\n",
      "Loss after mini-batch    11: 1.107\n",
      "Loss after mini-batch    21: 0.035\n",
      "Loss after mini-batch    31: 0.559\n",
      "Loss after mini-batch    41: 0.365\n",
      "Loss after mini-batch    51: 19.181\n",
      "Loss after mini-batch    61: 1.412\n",
      "Loss after mini-batch    71: 0.513\n",
      "Loss after mini-batch    81: 0.049\n",
      "Loss after mini-batch    91: 0.555\n",
      "Loss after mini-batch   101: 0.625\n",
      "Loss after mini-batch   111: 1.888\n",
      "Loss after mini-batch   121: 0.468\n",
      "Loss after mini-batch   131: 0.222\n",
      "Loss after mini-batch   141: 1.851\n",
      "Loss after mini-batch   151: 0.523\n",
      "Loss after mini-batch   161: 0.219\n",
      "Loss after mini-batch   171: 5.471\n",
      "Loss after mini-batch   181: 0.811\n",
      "Loss after mini-batch   191: 6.892\n",
      "Loss after mini-batch   201: 0.699\n",
      "Loss after mini-batch   211: 0.610\n",
      "Loss after mini-batch   221: 93.327\n",
      "Loss after mini-batch   231: 0.886\n",
      "Loss after mini-batch   241: 2.510\n",
      "Loss after mini-batch   251: 0.928\n",
      "Loss after mini-batch   261: 0.608\n",
      "Loss after mini-batch   271: 0.315\n",
      "Loss after mini-batch   281: 0.318\n",
      "Loss after mini-batch   291: 1.207\n",
      "Loss after mini-batch   301: 0.637\n",
      "Loss after mini-batch   311: 33.078\n",
      "Loss after mini-batch   321: 0.514\n",
      "Loss after mini-batch   331: 2.381\n",
      "Loss after mini-batch   341: 1.299\n",
      "Loss after mini-batch   351: 0.690\n",
      "Loss after mini-batch   361: 3.667\n",
      "Loss after mini-batch   371: 0.550\n",
      "Loss after mini-batch   381: 5.221\n",
      "Loss after mini-batch   391: 0.234\n",
      "Loss after mini-batch   401: 2.674\n",
      "Loss after mini-batch   411: 0.173\n",
      "Loss after mini-batch   421: 0.000\n",
      "Loss after mini-batch   431: 0.448\n",
      "Loss after mini-batch   441: 0.596\n",
      "Loss after mini-batch   451: 0.584\n",
      "Loss after mini-batch   461: 0.612\n",
      "Loss after mini-batch   471: 0.450\n",
      "Loss after mini-batch   481: 3.751\n",
      "Loss after mini-batch   491: 2.388\n",
      "Loss after mini-batch   501: 0.251\n",
      "Loss after mini-batch   511: 1.269\n",
      "Loss after mini-batch   521: 0.385\n",
      "Loss after mini-batch   531: 1.373\n",
      "Loss after mini-batch   541: 62.783\n",
      "Loss after mini-batch   551: 0.422\n",
      "Loss after mini-batch   561: 19.669\n",
      "Loss after mini-batch   571: 0.558\n",
      "Loss after mini-batch   581: 0.485\n",
      "Loss after mini-batch   591: 0.012\n",
      "Loss after mini-batch   601: 13.591\n",
      "Loss after mini-batch   611: 0.439\n",
      "Loss after mini-batch   621: 8.525\n",
      "Loss after mini-batch   631: 0.758\n",
      "Loss after mini-batch   641: 6.264\n",
      "Loss after mini-batch   651: 2.391\n",
      "Loss after mini-batch   661: 5.467\n",
      "Loss after mini-batch   671: 0.977\n",
      "Loss after mini-batch   681: 6.555\n",
      "Loss after mini-batch   691: 2.232\n",
      "Loss after mini-batch   701: 0.873\n",
      "Loss after mini-batch   711: 0.422\n",
      "Loss after mini-batch   721: 0.749\n",
      "Loss after mini-batch   731: 0.584\n",
      "Loss after mini-batch   741: 0.379\n",
      "Training Loss: 0.241 \t\t Validation Loss:49.810\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 6.923\n",
      "Loss after mini-batch    11: 0.047\n",
      "Loss after mini-batch    21: 7.559\n",
      "Loss after mini-batch    31: 0.466\n",
      "Loss after mini-batch    41: 0.615\n",
      "Loss after mini-batch    51: 21.717\n",
      "Loss after mini-batch    61: 0.624\n",
      "Loss after mini-batch    71: 24.579\n",
      "Loss after mini-batch    81: 5.279\n",
      "Loss after mini-batch    91: 0.776\n",
      "Loss after mini-batch   101: 0.903\n",
      "Loss after mini-batch   111: 2.152\n",
      "Loss after mini-batch   121: 3.193\n",
      "Loss after mini-batch   131: 0.542\n",
      "Loss after mini-batch   141: 0.132\n",
      "Loss after mini-batch   151: 2.056\n",
      "Loss after mini-batch   161: 0.620\n",
      "Loss after mini-batch   171: 0.340\n",
      "Loss after mini-batch   181: 4.416\n",
      "Loss after mini-batch   191: 1.279\n",
      "Loss after mini-batch   201: 0.174\n",
      "Loss after mini-batch   211: 3.294\n",
      "Loss after mini-batch   221: 0.470\n",
      "Loss after mini-batch   231: 3.930\n",
      "Loss after mini-batch   241: 0.257\n",
      "Loss after mini-batch   251: 0.487\n",
      "Loss after mini-batch   261: 0.440\n",
      "Loss after mini-batch   271: 23.905\n",
      "Loss after mini-batch   281: 0.185\n",
      "Loss after mini-batch   291: 2.568\n",
      "Loss after mini-batch   301: 0.323\n",
      "Loss after mini-batch   311: 45.798\n",
      "Loss after mini-batch   321: 0.717\n",
      "Loss after mini-batch   331: 0.373\n",
      "Loss after mini-batch   341: 0.276\n",
      "Loss after mini-batch   351: 0.540\n",
      "Loss after mini-batch   361: 0.477\n",
      "Loss after mini-batch   371: 0.005\n",
      "Loss after mini-batch   381: 56.981\n",
      "Loss after mini-batch   391: 0.774\n",
      "Loss after mini-batch   401: 22.711\n",
      "Loss after mini-batch   411: 0.785\n",
      "Loss after mini-batch   421: 5.236\n",
      "Loss after mini-batch   431: 16.118\n",
      "Loss after mini-batch   441: 0.865\n",
      "Loss after mini-batch   451: 0.954\n",
      "Loss after mini-batch   461: 0.716\n",
      "Loss after mini-batch   471: 5.663\n",
      "Loss after mini-batch   481: 21.705\n",
      "Loss after mini-batch   491: 35.025\n",
      "Loss after mini-batch   501: 18.662\n",
      "Loss after mini-batch   511: 0.034\n",
      "Loss after mini-batch   521: 0.885\n",
      "Loss after mini-batch   531: 0.760\n",
      "Loss after mini-batch   541: 15.687\n",
      "Loss after mini-batch   551: 5.059\n",
      "Loss after mini-batch   561: 0.794\n",
      "Loss after mini-batch   571: 0.804\n",
      "Loss after mini-batch   581: 0.358\n",
      "Loss after mini-batch   591: 0.460\n",
      "Loss after mini-batch   601: 2.193\n",
      "Loss after mini-batch   611: 0.260\n",
      "Loss after mini-batch   621: 0.107\n",
      "Loss after mini-batch   631: 0.525\n",
      "Loss after mini-batch   641: 0.659\n",
      "Loss after mini-batch   651: 0.559\n",
      "Loss after mini-batch   661: 0.221\n",
      "Loss after mini-batch   671: 0.499\n",
      "Loss after mini-batch   681: 0.531\n",
      "Loss after mini-batch   691: 0.605\n",
      "Loss after mini-batch   701: 0.739\n",
      "Loss after mini-batch   711: 0.608\n",
      "Loss after mini-batch   721: 0.558\n",
      "Loss after mini-batch   731: 7.522\n",
      "Loss after mini-batch   741: 0.505\n",
      "Training Loss: 0.762 \t\t Validation Loss:15.506\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.688\n",
      "Loss after mini-batch    11: 5.568\n",
      "Loss after mini-batch    21: 24.169\n",
      "Loss after mini-batch    31: 8.785\n",
      "Loss after mini-batch    41: 4.965\n",
      "Loss after mini-batch    51: 0.402\n",
      "Loss after mini-batch    61: 1.133\n",
      "Loss after mini-batch    71: 0.638\n",
      "Loss after mini-batch    81: 0.476\n",
      "Loss after mini-batch    91: 1.112\n",
      "Loss after mini-batch   101: 4.010\n",
      "Loss after mini-batch   111: 4.280\n",
      "Loss after mini-batch   121: 0.773\n",
      "Loss after mini-batch   131: 0.720\n",
      "Loss after mini-batch   141: 0.773\n",
      "Loss after mini-batch   151: 0.697\n",
      "Loss after mini-batch   161: 0.626\n",
      "Loss after mini-batch   171: 0.793\n",
      "Loss after mini-batch   181: 0.824\n",
      "Loss after mini-batch   191: 5.760\n",
      "Loss after mini-batch   201: 0.677\n",
      "Loss after mini-batch   211: 3.661\n",
      "Loss after mini-batch   221: 0.586\n",
      "Loss after mini-batch   231: 0.678\n",
      "Loss after mini-batch   241: 8.152\n",
      "Loss after mini-batch   251: 0.599\n",
      "Loss after mini-batch   261: 0.106\n",
      "Loss after mini-batch   271: 19.629\n",
      "Loss after mini-batch   281: 0.258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   291: 25.601\n",
      "Loss after mini-batch   301: 0.427\n",
      "Loss after mini-batch   311: 13.266\n",
      "Loss after mini-batch   321: 5.293\n",
      "Loss after mini-batch   331: 0.337\n",
      "Loss after mini-batch   341: 8.475\n",
      "Loss after mini-batch   351: 3.556\n",
      "Loss after mini-batch   361: 3.081\n",
      "Loss after mini-batch   371: 0.722\n",
      "Loss after mini-batch   381: 0.505\n",
      "Loss after mini-batch   391: 0.298\n",
      "Loss after mini-batch   401: 8.216\n",
      "Loss after mini-batch   411: 0.613\n",
      "Loss after mini-batch   421: 0.504\n",
      "Loss after mini-batch   431: 4.169\n",
      "Loss after mini-batch   441: 0.510\n",
      "Loss after mini-batch   451: 0.522\n",
      "Loss after mini-batch   461: 21.517\n",
      "Loss after mini-batch   471: 0.100\n",
      "Loss after mini-batch   481: 0.974\n",
      "Loss after mini-batch   491: 0.533\n",
      "Loss after mini-batch   501: 2.127\n",
      "Loss after mini-batch   511: 13.935\n",
      "Loss after mini-batch   521: 0.472\n",
      "Loss after mini-batch   531: 15.551\n",
      "Loss after mini-batch   541: 2.507\n",
      "Loss after mini-batch   551: 0.191\n",
      "Loss after mini-batch   561: 22.104\n",
      "Loss after mini-batch   571: 4.077\n",
      "Loss after mini-batch   581: 0.522\n",
      "Loss after mini-batch   591: 3.180\n",
      "Loss after mini-batch   601: 18.626\n",
      "Loss after mini-batch   611: 2.499\n",
      "Loss after mini-batch   621: 55.924\n",
      "Loss after mini-batch   631: 5.541\n",
      "Loss after mini-batch   641: 16.132\n",
      "Loss after mini-batch   651: 1.064\n",
      "Loss after mini-batch   661: 0.533\n",
      "Loss after mini-batch   671: 0.356\n",
      "Loss after mini-batch   681: 0.790\n",
      "Loss after mini-batch   691: 2.183\n",
      "Loss after mini-batch   701: 1.062\n",
      "Loss after mini-batch   711: 0.685\n",
      "Loss after mini-batch   721: 0.364\n",
      "Loss after mini-batch   731: 0.529\n",
      "Loss after mini-batch   741: 0.319\n",
      "Training Loss: 0.505 \t\t Validation Loss:0.541\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 3.358\n",
      "Loss after mini-batch    11: 0.333\n",
      "Loss after mini-batch    21: 0.393\n",
      "Loss after mini-batch    31: 3.116\n",
      "Loss after mini-batch    41: 1.239\n",
      "Loss after mini-batch    51: 7.042\n",
      "Loss after mini-batch    61: 0.398\n",
      "Loss after mini-batch    71: 1.752\n",
      "Loss after mini-batch    81: 0.480\n",
      "Loss after mini-batch    91: 8.839\n",
      "Loss after mini-batch   101: 5.356\n",
      "Loss after mini-batch   111: 5.045\n",
      "Loss after mini-batch   121: 4.250\n",
      "Loss after mini-batch   131: 0.779\n",
      "Loss after mini-batch   141: 0.052\n",
      "Loss after mini-batch   151: 0.040\n",
      "Loss after mini-batch   161: 1.791\n",
      "Loss after mini-batch   171: 0.283\n",
      "Loss after mini-batch   181: 2.478\n",
      "Loss after mini-batch   191: 13.897\n",
      "Loss after mini-batch   201: 0.214\n",
      "Loss after mini-batch   211: 0.364\n",
      "Loss after mini-batch   221: 2.443\n",
      "Loss after mini-batch   231: 0.640\n",
      "Loss after mini-batch   241: 0.230\n",
      "Loss after mini-batch   251: 2.191\n",
      "Loss after mini-batch   261: 0.539\n",
      "Loss after mini-batch   271: 0.430\n",
      "Loss after mini-batch   281: 32.922\n",
      "Loss after mini-batch   291: 5.204\n",
      "Loss after mini-batch   301: 5.084\n",
      "Loss after mini-batch   311: 0.877\n",
      "Loss after mini-batch   321: 0.705\n",
      "Loss after mini-batch   331: 1.132\n",
      "Loss after mini-batch   341: 0.722\n",
      "Loss after mini-batch   351: 0.521\n",
      "Loss after mini-batch   361: 2.521\n",
      "Loss after mini-batch   371: 0.909\n",
      "Loss after mini-batch   381: 8.928\n",
      "Loss after mini-batch   391: 0.697\n",
      "Loss after mini-batch   401: 0.718\n",
      "Loss after mini-batch   411: 0.251\n",
      "Loss after mini-batch   421: 0.466\n",
      "Loss after mini-batch   431: 0.482\n",
      "Loss after mini-batch   441: 56.209\n",
      "Loss after mini-batch   451: 8.891\n",
      "Loss after mini-batch   461: 4.046\n",
      "Loss after mini-batch   471: 0.462\n",
      "Loss after mini-batch   481: 0.421\n",
      "Loss after mini-batch   491: 0.168\n",
      "Loss after mini-batch   501: 5.819\n",
      "Loss after mini-batch   511: 0.364\n",
      "Loss after mini-batch   521: 2.394\n",
      "Loss after mini-batch   531: 0.192\n",
      "Loss after mini-batch   541: 0.052\n",
      "Loss after mini-batch   551: 4.729\n",
      "Loss after mini-batch   561: 0.193\n",
      "Loss after mini-batch   571: 19.463\n",
      "Loss after mini-batch   581: 0.260\n",
      "Loss after mini-batch   591: 0.376\n",
      "Loss after mini-batch   601: 7.442\n",
      "Loss after mini-batch   611: 0.128\n",
      "Loss after mini-batch   621: 0.681\n",
      "Loss after mini-batch   631: 5.350\n",
      "Loss after mini-batch   641: 0.628\n",
      "Loss after mini-batch   651: 0.232\n",
      "Loss after mini-batch   661: 37.578\n",
      "Loss after mini-batch   671: 0.915\n",
      "Loss after mini-batch   681: 1.084\n",
      "Loss after mini-batch   691: 0.955\n",
      "Loss after mini-batch   701: 1.010\n",
      "Loss after mini-batch   711: 0.822\n",
      "Loss after mini-batch   721: 95.355\n",
      "Loss after mini-batch   731: 14.461\n",
      "Loss after mini-batch   741: 2.244\n",
      "Training Loss: 0.630 \t\t Validation Loss:1.136\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 8.935\n",
      "Loss after mini-batch    11: 0.251\n",
      "Loss after mini-batch    21: 0.519\n",
      "Loss after mini-batch    31: 0.070\n",
      "Loss after mini-batch    41: 0.136\n",
      "Loss after mini-batch    51: 0.415\n",
      "Loss after mini-batch    61: 0.062\n",
      "Loss after mini-batch    71: 3.170\n",
      "Loss after mini-batch    81: 10.661\n",
      "Loss after mini-batch    91: 0.631\n",
      "Loss after mini-batch   101: 0.591\n",
      "Loss after mini-batch   111: 0.768\n",
      "Loss after mini-batch   121: 0.352\n",
      "Loss after mini-batch   131: 0.080\n",
      "Loss after mini-batch   141: 0.538\n",
      "Loss after mini-batch   151: 0.471\n",
      "Loss after mini-batch   161: 0.408\n",
      "Loss after mini-batch   171: 0.198\n",
      "Loss after mini-batch   181: 29.093\n",
      "Loss after mini-batch   191: 26.908\n",
      "Loss after mini-batch   201: 0.279\n",
      "Loss after mini-batch   211: 0.341\n",
      "Loss after mini-batch   221: 2.970\n",
      "Loss after mini-batch   231: 5.339\n",
      "Loss after mini-batch   241: 0.439\n",
      "Loss after mini-batch   251: 0.511\n",
      "Loss after mini-batch   261: 0.299\n",
      "Loss after mini-batch   271: 0.562\n",
      "Loss after mini-batch   281: 0.548\n",
      "Loss after mini-batch   291: 0.931\n",
      "Loss after mini-batch   301: 0.196\n",
      "Loss after mini-batch   311: 7.671\n",
      "Loss after mini-batch   321: 0.426\n",
      "Loss after mini-batch   331: 0.212\n",
      "Loss after mini-batch   341: 0.117\n",
      "Loss after mini-batch   351: 0.511\n",
      "Loss after mini-batch   361: 54.489\n",
      "Loss after mini-batch   371: 1.537\n",
      "Loss after mini-batch   381: 0.062\n",
      "Loss after mini-batch   391: 71.240\n",
      "Loss after mini-batch   401: 18.431\n",
      "Loss after mini-batch   411: 0.420\n",
      "Loss after mini-batch   421: 0.480\n",
      "Loss after mini-batch   431: 1.515\n",
      "Loss after mini-batch   441: 3.915\n",
      "Loss after mini-batch   451: 40.726\n",
      "Loss after mini-batch   461: 0.326\n",
      "Loss after mini-batch   471: 0.170\n",
      "Loss after mini-batch   481: 14.418\n",
      "Loss after mini-batch   491: 0.373\n",
      "Loss after mini-batch   501: 0.302\n",
      "Loss after mini-batch   511: 0.308\n",
      "Loss after mini-batch   521: 0.263\n",
      "Loss after mini-batch   531: 0.044\n",
      "Loss after mini-batch   541: 3.428\n",
      "Loss after mini-batch   551: 3.075\n",
      "Loss after mini-batch   561: 17.101\n",
      "Loss after mini-batch   571: 0.526\n",
      "Loss after mini-batch   581: 0.543\n",
      "Loss after mini-batch   591: 0.394\n",
      "Loss after mini-batch   601: 2.279\n",
      "Loss after mini-batch   611: 0.641\n",
      "Loss after mini-batch   621: 0.375\n",
      "Loss after mini-batch   631: 9.951\n",
      "Loss after mini-batch   641: 21.904\n",
      "Loss after mini-batch   651: 2.410\n",
      "Loss after mini-batch   661: 0.530\n",
      "Loss after mini-batch   671: 0.827\n",
      "Loss after mini-batch   681: 0.957\n",
      "Loss after mini-batch   691: 0.719\n",
      "Loss after mini-batch   701: 2.027\n",
      "Loss after mini-batch   711: 3.956\n",
      "Loss after mini-batch   721: 0.265\n",
      "Loss after mini-batch   731: 23.782\n",
      "Loss after mini-batch   741: 0.914\n",
      "Training Loss: 0.698 \t\t Validation Loss:27.760\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 2.826\n",
      "Loss after mini-batch    11: 14.165\n",
      "Loss after mini-batch    21: 0.664\n",
      "Loss after mini-batch    31: 0.594\n",
      "Loss after mini-batch    41: 0.592\n",
      "Loss after mini-batch    51: 0.561\n",
      "Loss after mini-batch    61: 55.530\n",
      "Loss after mini-batch    71: 0.398\n",
      "Loss after mini-batch    81: 0.506\n",
      "Loss after mini-batch    91: 13.814\n",
      "Loss after mini-batch   101: 0.448\n",
      "Loss after mini-batch   111: 0.896\n",
      "Loss after mini-batch   121: 19.506\n",
      "Loss after mini-batch   131: 0.077\n",
      "Loss after mini-batch   141: 1.388\n",
      "Loss after mini-batch   151: 0.170\n",
      "Loss after mini-batch   161: 4.195\n",
      "Loss after mini-batch   171: 0.484\n",
      "Loss after mini-batch   181: 0.118\n",
      "Loss after mini-batch   191: 0.783\n",
      "Loss after mini-batch   201: 0.300\n",
      "Loss after mini-batch   211: 9.031\n",
      "Loss after mini-batch   221: 2.740\n",
      "Loss after mini-batch   231: 0.806\n",
      "Loss after mini-batch   241: 6.809\n",
      "Loss after mini-batch   251: 0.643\n",
      "Loss after mini-batch   261: 0.789\n",
      "Loss after mini-batch   271: 0.729\n",
      "Loss after mini-batch   281: 0.639\n",
      "Loss after mini-batch   291: 26.723\n",
      "Loss after mini-batch   301: 0.607\n",
      "Loss after mini-batch   311: 2.862\n",
      "Loss after mini-batch   321: 3.935\n",
      "Loss after mini-batch   331: 0.163\n",
      "Loss after mini-batch   341: 0.328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   351: 0.402\n",
      "Loss after mini-batch   361: 3.149\n",
      "Loss after mini-batch   371: 0.855\n",
      "Loss after mini-batch   381: 0.073\n",
      "Loss after mini-batch   391: 0.372\n",
      "Loss after mini-batch   401: 0.384\n",
      "Loss after mini-batch   411: 0.048\n",
      "Loss after mini-batch   421: 9.707\n",
      "Loss after mini-batch   431: 0.142\n",
      "Loss after mini-batch   441: 4.005\n",
      "Loss after mini-batch   451: 0.162\n",
      "Loss after mini-batch   461: 1.438\n",
      "Loss after mini-batch   471: 0.608\n",
      "Loss after mini-batch   481: 0.568\n",
      "Loss after mini-batch   491: 18.204\n",
      "Loss after mini-batch   501: 0.326\n",
      "Loss after mini-batch   511: 0.585\n",
      "Loss after mini-batch   521: 11.319\n",
      "Loss after mini-batch   531: 0.579\n",
      "Loss after mini-batch   541: 7.083\n",
      "Loss after mini-batch   551: 8.683\n",
      "Loss after mini-batch   561: 0.467\n",
      "Loss after mini-batch   571: 0.362\n",
      "Loss after mini-batch   581: 0.366\n",
      "Loss after mini-batch   591: 0.589\n",
      "Loss after mini-batch   601: 0.547\n",
      "Loss after mini-batch   611: 4.732\n",
      "Loss after mini-batch   621: 0.415\n",
      "Loss after mini-batch   631: 0.629\n",
      "Loss after mini-batch   641: 0.025\n",
      "Loss after mini-batch   651: 27.523\n",
      "Loss after mini-batch   661: 1.374\n",
      "Loss after mini-batch   671: 15.706\n",
      "Loss after mini-batch   681: 0.173\n",
      "Loss after mini-batch   691: 0.275\n",
      "Loss after mini-batch   701: 15.945\n",
      "Loss after mini-batch   711: 0.300\n",
      "Loss after mini-batch   721: 0.952\n",
      "Loss after mini-batch   731: 0.188\n",
      "Loss after mini-batch   741: 1.482\n",
      "Training Loss: 2.660 \t\t Validation Loss:2.677\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 1.314\n",
      "Loss after mini-batch    11: 0.301\n",
      "Loss after mini-batch    21: 0.274\n",
      "Loss after mini-batch    31: 0.284\n",
      "Loss after mini-batch    41: 0.783\n",
      "Loss after mini-batch    51: 0.435\n",
      "Loss after mini-batch    61: 0.402\n",
      "Loss after mini-batch    71: 0.448\n",
      "Loss after mini-batch    81: 19.137\n",
      "Loss after mini-batch    91: 1.130\n",
      "Loss after mini-batch   101: 0.584\n",
      "Loss after mini-batch   111: 0.424\n",
      "Loss after mini-batch   121: 0.072\n",
      "Loss after mini-batch   131: 5.369\n",
      "Loss after mini-batch   141: 0.444\n",
      "Loss after mini-batch   151: 0.246\n",
      "Loss after mini-batch   161: 0.374\n",
      "Loss after mini-batch   171: 29.704\n",
      "Loss after mini-batch   181: 0.203\n",
      "Loss after mini-batch   191: 1.443\n",
      "Loss after mini-batch   201: 0.158\n",
      "Loss after mini-batch   211: 0.207\n",
      "Loss after mini-batch   221: 0.130\n",
      "Loss after mini-batch   231: 0.042\n",
      "Loss after mini-batch   241: 0.386\n",
      "Loss after mini-batch   251: 1.463\n",
      "Loss after mini-batch   261: 0.058\n",
      "Loss after mini-batch   271: 6.167\n",
      "Loss after mini-batch   281: 0.041\n",
      "Loss after mini-batch   291: 58.628\n",
      "Loss after mini-batch   301: 8.427\n",
      "Loss after mini-batch   311: 0.023\n",
      "Loss after mini-batch   321: 0.531\n",
      "Loss after mini-batch   331: 21.875\n",
      "Loss after mini-batch   341: 0.650\n",
      "Loss after mini-batch   351: 0.307\n",
      "Loss after mini-batch   361: 0.329\n",
      "Loss after mini-batch   371: 0.193\n",
      "Loss after mini-batch   381: 61.555\n",
      "Loss after mini-batch   391: 15.698\n",
      "Loss after mini-batch   401: 0.883\n",
      "Loss after mini-batch   411: 0.752\n",
      "Loss after mini-batch   421: 0.264\n",
      "Loss after mini-batch   431: 9.123\n",
      "Loss after mini-batch   441: 0.755\n",
      "Loss after mini-batch   451: 0.322\n",
      "Loss after mini-batch   461: 13.413\n",
      "Loss after mini-batch   471: 0.633\n",
      "Loss after mini-batch   481: 0.508\n",
      "Loss after mini-batch   491: 0.176\n",
      "Loss after mini-batch   501: 8.789\n",
      "Loss after mini-batch   511: 20.742\n",
      "Loss after mini-batch   521: 0.985\n",
      "Loss after mini-batch   531: 13.399\n",
      "Loss after mini-batch   541: 55.335\n",
      "Loss after mini-batch   551: 2.958\n",
      "Loss after mini-batch   561: 0.771\n",
      "Loss after mini-batch   571: 0.624\n",
      "Loss after mini-batch   581: 0.405\n",
      "Loss after mini-batch   591: 0.748\n",
      "Loss after mini-batch   601: 3.556\n",
      "Loss after mini-batch   611: 0.140\n",
      "Loss after mini-batch   621: 1.841\n",
      "Loss after mini-batch   631: 14.402\n",
      "Loss after mini-batch   641: 0.349\n",
      "Loss after mini-batch   651: 5.362\n",
      "Loss after mini-batch   661: 0.400\n",
      "Loss after mini-batch   671: 0.501\n",
      "Loss after mini-batch   681: 0.246\n",
      "Loss after mini-batch   691: 0.363\n",
      "Loss after mini-batch   701: 0.229\n",
      "Loss after mini-batch   711: 0.211\n",
      "Loss after mini-batch   721: 15.576\n",
      "Loss after mini-batch   731: 0.348\n",
      "Loss after mini-batch   741: 4.542\n",
      "Training Loss: 0.384 \t\t Validation Loss:0.681\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 0.280\n",
      "Loss after mini-batch    11: 0.244\n",
      "Loss after mini-batch    21: 0.181\n",
      "Loss after mini-batch    31: 12.504\n",
      "Loss after mini-batch    41: 0.262\n",
      "Loss after mini-batch    51: 0.548\n",
      "Loss after mini-batch    61: 3.129\n",
      "Loss after mini-batch    71: 0.022\n",
      "Loss after mini-batch    81: 6.621\n",
      "Loss after mini-batch    91: 0.234\n",
      "Loss after mini-batch   101: 0.096\n",
      "Loss after mini-batch   111: 0.360\n",
      "Loss after mini-batch   121: 0.013\n",
      "Loss after mini-batch   131: 0.827\n",
      "Loss after mini-batch   141: 2.928\n",
      "Loss after mini-batch   151: 3.237\n",
      "Loss after mini-batch   161: 0.559\n",
      "Loss after mini-batch   171: 0.327\n",
      "Loss after mini-batch   181: 0.261\n",
      "Loss after mini-batch   191: 16.085\n",
      "Loss after mini-batch   201: 0.039\n",
      "Loss after mini-batch   211: 1.355\n",
      "Loss after mini-batch   221: 0.255\n",
      "Loss after mini-batch   231: 19.877\n",
      "Loss after mini-batch   241: 0.213\n",
      "Loss after mini-batch   251: 14.755\n",
      "Loss after mini-batch   261: 5.178\n",
      "Loss after mini-batch   271: 0.586\n",
      "Loss after mini-batch   281: 0.412\n",
      "Loss after mini-batch   291: 0.465\n",
      "Loss after mini-batch   301: 0.595\n",
      "Loss after mini-batch   311: 0.574\n",
      "Loss after mini-batch   321: 12.918\n",
      "Loss after mini-batch   331: 0.461\n",
      "Loss after mini-batch   341: 0.546\n",
      "Loss after mini-batch   351: 0.437\n",
      "Loss after mini-batch   361: 0.417\n",
      "Loss after mini-batch   371: 0.302\n",
      "Loss after mini-batch   381: 0.335\n",
      "Loss after mini-batch   391: 0.233\n",
      "Loss after mini-batch   401: 0.018\n",
      "Loss after mini-batch   411: 36.109\n",
      "Loss after mini-batch   421: 56.878\n",
      "Loss after mini-batch   431: 0.391\n",
      "Loss after mini-batch   441: 0.176\n",
      "Loss after mini-batch   451: 0.156\n",
      "Loss after mini-batch   461: 0.438\n",
      "Loss after mini-batch   471: 0.342\n",
      "Loss after mini-batch   481: 0.517\n",
      "Loss after mini-batch   491: 0.518\n",
      "Loss after mini-batch   501: 13.720\n",
      "Loss after mini-batch   511: 0.341\n",
      "Loss after mini-batch   521: 34.407\n",
      "Loss after mini-batch   531: 18.553\n",
      "Loss after mini-batch   541: 0.113\n",
      "Loss after mini-batch   551: 0.054\n",
      "Loss after mini-batch   561: 2.205\n",
      "Loss after mini-batch   571: 0.539\n",
      "Loss after mini-batch   581: 8.502\n",
      "Loss after mini-batch   591: 1.058\n",
      "Loss after mini-batch   601: 0.164\n",
      "Loss after mini-batch   611: 0.460\n",
      "Loss after mini-batch   621: 0.257\n",
      "Loss after mini-batch   631: 0.553\n",
      "Loss after mini-batch   641: 12.625\n",
      "Loss after mini-batch   651: 0.561\n",
      "Loss after mini-batch   661: 5.261\n",
      "Loss after mini-batch   671: 1.013\n",
      "Loss after mini-batch   681: 0.018\n",
      "Loss after mini-batch   691: 20.360\n",
      "Loss after mini-batch   701: 0.267\n",
      "Loss after mini-batch   711: 1.255\n",
      "Loss after mini-batch   721: 0.727\n",
      "Loss after mini-batch   731: 0.383\n",
      "Loss after mini-batch   741: 0.674\n",
      "Training Loss: 0.513 \t\t Validation Loss:0.988\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 15.421\n",
      "Loss after mini-batch    11: 1.046\n",
      "Loss after mini-batch    21: 3.837\n",
      "Loss after mini-batch    31: 1.319\n",
      "Loss after mini-batch    41: 0.850\n",
      "Loss after mini-batch    51: 0.117\n",
      "Loss after mini-batch    61: 5.476\n",
      "Loss after mini-batch    71: 0.698\n",
      "Loss after mini-batch    81: 0.614\n",
      "Loss after mini-batch    91: 2.010\n",
      "Loss after mini-batch   101: 0.373\n",
      "Loss after mini-batch   111: 0.604\n",
      "Loss after mini-batch   121: 0.585\n",
      "Loss after mini-batch   131: 0.650\n",
      "Loss after mini-batch   141: 0.581\n",
      "Loss after mini-batch   151: 0.287\n",
      "Loss after mini-batch   161: 21.472\n",
      "Loss after mini-batch   171: 0.342\n",
      "Loss after mini-batch   181: 0.764\n",
      "Loss after mini-batch   191: 93.709\n",
      "Loss after mini-batch   201: 0.782\n",
      "Loss after mini-batch   211: 53.942\n",
      "Loss after mini-batch   221: 0.427\n",
      "Loss after mini-batch   231: 0.342\n",
      "Loss after mini-batch   241: 0.244\n",
      "Loss after mini-batch   251: 0.602\n",
      "Loss after mini-batch   261: 0.167\n",
      "Loss after mini-batch   271: 5.150\n",
      "Loss after mini-batch   281: 3.877\n",
      "Loss after mini-batch   291: 0.265\n",
      "Loss after mini-batch   301: 13.935\n",
      "Loss after mini-batch   311: 0.500\n",
      "Loss after mini-batch   321: 0.415\n",
      "Loss after mini-batch   331: 0.138\n",
      "Loss after mini-batch   341: 1.892\n",
      "Loss after mini-batch   351: 0.064\n",
      "Loss after mini-batch   361: 0.575\n",
      "Loss after mini-batch   371: 0.387\n",
      "Loss after mini-batch   381: 1.945\n",
      "Loss after mini-batch   391: 0.510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   401: 0.673\n",
      "Loss after mini-batch   411: 0.372\n",
      "Loss after mini-batch   421: 34.449\n",
      "Loss after mini-batch   431: 0.858\n",
      "Loss after mini-batch   441: 0.377\n",
      "Loss after mini-batch   451: 0.649\n",
      "Loss after mini-batch   461: 0.526\n",
      "Loss after mini-batch   471: 6.126\n",
      "Loss after mini-batch   481: 0.458\n",
      "Loss after mini-batch   491: 0.141\n",
      "Loss after mini-batch   501: 17.889\n",
      "Loss after mini-batch   511: 0.171\n",
      "Loss after mini-batch   521: 2.508\n",
      "Loss after mini-batch   531: 0.431\n",
      "Loss after mini-batch   541: 0.523\n",
      "Loss after mini-batch   551: 0.506\n",
      "Loss after mini-batch   561: 5.003\n",
      "Loss after mini-batch   571: 0.216\n",
      "Loss after mini-batch   581: 0.142\n",
      "Loss after mini-batch   591: 0.246\n",
      "Loss after mini-batch   601: 0.279\n",
      "Loss after mini-batch   611: 0.003\n",
      "Loss after mini-batch   621: 0.180\n",
      "Loss after mini-batch   631: 0.430\n",
      "Loss after mini-batch   641: 0.242\n",
      "Loss after mini-batch   651: 0.366\n",
      "Loss after mini-batch   661: 0.578\n",
      "Loss after mini-batch   671: 4.369\n",
      "Loss after mini-batch   681: 0.376\n",
      "Loss after mini-batch   691: 0.626\n",
      "Loss after mini-batch   701: 0.394\n",
      "Loss after mini-batch   711: 0.369\n",
      "Loss after mini-batch   721: 39.793\n",
      "Loss after mini-batch   731: 4.973\n",
      "Loss after mini-batch   741: 6.349\n",
      "Training Loss: 0.530 \t\t Validation Loss:24.268\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 0.551\n",
      "Loss after mini-batch    11: 0.356\n",
      "Loss after mini-batch    21: 5.071\n",
      "Loss after mini-batch    31: 6.898\n",
      "Loss after mini-batch    41: 0.558\n",
      "Loss after mini-batch    51: 0.187\n",
      "Loss after mini-batch    61: 38.072\n",
      "Loss after mini-batch    71: 0.518\n",
      "Loss after mini-batch    81: 0.479\n",
      "Loss after mini-batch    91: 0.651\n",
      "Loss after mini-batch   101: 0.561\n",
      "Loss after mini-batch   111: 0.317\n",
      "Loss after mini-batch   121: 16.115\n",
      "Loss after mini-batch   131: 3.365\n",
      "Loss after mini-batch   141: 0.596\n",
      "Loss after mini-batch   151: 4.277\n",
      "Loss after mini-batch   161: 43.137\n",
      "Loss after mini-batch   171: 0.602\n",
      "Loss after mini-batch   181: 0.397\n",
      "Loss after mini-batch   191: 1.640\n",
      "Loss after mini-batch   201: 6.479\n",
      "Loss after mini-batch   211: 2.575\n",
      "Loss after mini-batch   221: 0.390\n",
      "Loss after mini-batch   231: 0.550\n",
      "Loss after mini-batch   241: 0.712\n",
      "Loss after mini-batch   251: 0.586\n",
      "Loss after mini-batch   261: 0.258\n",
      "Loss after mini-batch   271: 0.440\n",
      "Loss after mini-batch   281: 0.188\n",
      "Loss after mini-batch   291: 4.330\n",
      "Loss after mini-batch   301: 17.323\n",
      "Loss after mini-batch   311: 16.093\n",
      "Loss after mini-batch   321: 0.481\n",
      "Loss after mini-batch   331: 0.509\n",
      "Loss after mini-batch   341: 0.582\n",
      "Loss after mini-batch   351: 3.812\n",
      "Loss after mini-batch   361: 0.535\n",
      "Loss after mini-batch   371: 0.684\n",
      "Loss after mini-batch   381: 0.348\n",
      "Loss after mini-batch   391: 0.551\n",
      "Loss after mini-batch   401: 2.471\n",
      "Loss after mini-batch   411: 13.271\n",
      "Loss after mini-batch   421: 0.232\n",
      "Loss after mini-batch   431: 0.001\n",
      "Loss after mini-batch   441: 0.287\n",
      "Loss after mini-batch   451: 0.179\n",
      "Loss after mini-batch   461: 0.330\n",
      "Loss after mini-batch   471: 0.511\n",
      "Loss after mini-batch   481: 0.250\n",
      "Loss after mini-batch   491: 0.159\n",
      "Loss after mini-batch   501: 1.376\n",
      "Loss after mini-batch   511: 0.455\n",
      "Loss after mini-batch   521: 0.707\n",
      "Loss after mini-batch   531: 0.498\n",
      "Loss after mini-batch   541: 0.218\n",
      "Loss after mini-batch   551: 0.519\n",
      "Loss after mini-batch   561: 53.693\n",
      "Loss after mini-batch   571: 1.197\n",
      "Loss after mini-batch   581: 0.399\n",
      "Loss after mini-batch   591: 0.160\n",
      "Loss after mini-batch   601: 1.598\n",
      "Loss after mini-batch   611: 0.518\n",
      "Loss after mini-batch   621: 0.550\n",
      "Loss after mini-batch   631: 0.674\n",
      "Loss after mini-batch   641: 0.669\n",
      "Loss after mini-batch   651: 1.064\n",
      "Loss after mini-batch   661: 0.407\n",
      "Loss after mini-batch   671: 0.383\n",
      "Loss after mini-batch   681: 0.294\n",
      "Loss after mini-batch   691: 0.085\n",
      "Loss after mini-batch   701: 0.466\n",
      "Loss after mini-batch   711: 0.037\n",
      "Loss after mini-batch   721: 20.282\n",
      "Loss after mini-batch   731: 0.275\n",
      "Loss after mini-batch   741: 0.010\n",
      "Training Loss: 15.842 \t\t Validation Loss:20.736\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 10.627\n",
      "Loss after mini-batch    11: 5.502\n",
      "Loss after mini-batch    21: 0.912\n",
      "Loss after mini-batch    31: 0.612\n",
      "Loss after mini-batch    41: 23.145\n",
      "Loss after mini-batch    51: 5.521\n",
      "Loss after mini-batch    61: 3.386\n",
      "Loss after mini-batch    71: 2.732\n",
      "Loss after mini-batch    81: 12.881\n",
      "Loss after mini-batch    91: 1.052\n",
      "Loss after mini-batch   101: 15.045\n",
      "Loss after mini-batch   111: 4.725\n",
      "Loss after mini-batch   121: 52.070\n",
      "Loss after mini-batch   131: 1.155\n",
      "Loss after mini-batch   141: 1.423\n",
      "Loss after mini-batch   151: 1.249\n",
      "Loss after mini-batch   161: 1.188\n",
      "Loss after mini-batch   171: 1.921\n",
      "Loss after mini-batch   181: 0.916\n",
      "Loss after mini-batch   191: 0.762\n",
      "Loss after mini-batch   201: 0.478\n",
      "Loss after mini-batch   211: 0.340\n",
      "Loss after mini-batch   221: 2.189\n",
      "Loss after mini-batch   231: 0.341\n",
      "Loss after mini-batch   241: 54.877\n",
      "Loss after mini-batch   251: 0.053\n",
      "Loss after mini-batch   261: 0.540\n",
      "Loss after mini-batch   271: 0.901\n",
      "Loss after mini-batch   281: 0.049\n",
      "Loss after mini-batch   291: 0.407\n",
      "Loss after mini-batch   301: 0.352\n",
      "Loss after mini-batch   311: 0.794\n",
      "Loss after mini-batch   321: 14.059\n",
      "Loss after mini-batch   331: 0.503\n",
      "Loss after mini-batch   341: 0.875\n",
      "Loss after mini-batch   351: 0.364\n",
      "Loss after mini-batch   361: 20.191\n",
      "Loss after mini-batch   371: 24.178\n",
      "Loss after mini-batch   381: 4.989\n",
      "Loss after mini-batch   391: 12.766\n",
      "Loss after mini-batch   401: 0.015\n",
      "Loss after mini-batch   411: 2.868\n",
      "Loss after mini-batch   421: 0.906\n",
      "Loss after mini-batch   431: 19.657\n",
      "Loss after mini-batch   441: 11.635\n",
      "Loss after mini-batch   451: 0.951\n",
      "Loss after mini-batch   461: 0.775\n",
      "Loss after mini-batch   471: 0.785\n",
      "Loss after mini-batch   481: 0.900\n",
      "Loss after mini-batch   491: 0.432\n",
      "Loss after mini-batch   501: 0.585\n",
      "Loss after mini-batch   511: 0.717\n",
      "Loss after mini-batch   521: 0.157\n",
      "Loss after mini-batch   531: 0.412\n",
      "Loss after mini-batch   541: 0.210\n",
      "Loss after mini-batch   551: 0.606\n",
      "Loss after mini-batch   561: 0.354\n",
      "Loss after mini-batch   571: 9.968\n",
      "Loss after mini-batch   581: 0.319\n",
      "Loss after mini-batch   591: 0.147\n",
      "Loss after mini-batch   601: 0.013\n",
      "Loss after mini-batch   611: 3.358\n",
      "Loss after mini-batch   621: 0.380\n",
      "Loss after mini-batch   631: 0.705\n",
      "Loss after mini-batch   641: 0.093\n",
      "Loss after mini-batch   651: 0.310\n",
      "Loss after mini-batch   661: 0.443\n",
      "Loss after mini-batch   671: 2.690\n",
      "Loss after mini-batch   681: 0.259\n",
      "Loss after mini-batch   691: 1.255\n",
      "Loss after mini-batch   701: 0.417\n",
      "Loss after mini-batch   711: 0.321\n",
      "Loss after mini-batch   721: 0.111\n",
      "Loss after mini-batch   731: 0.309\n",
      "Loss after mini-batch   741: 4.034\n",
      "Training Loss: 0.747 \t\t Validation Loss:53.430\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 8.032\n",
      "Loss after mini-batch    11: 0.255\n",
      "Loss after mini-batch    21: 0.172\n",
      "Loss after mini-batch    31: 2.131\n",
      "Loss after mini-batch    41: 5.144\n",
      "Loss after mini-batch    51: 25.953\n",
      "Loss after mini-batch    61: 4.891\n",
      "Loss after mini-batch    71: 0.123\n",
      "Loss after mini-batch    81: 0.845\n",
      "Loss after mini-batch    91: 7.813\n",
      "Loss after mini-batch   101: 4.586\n",
      "Loss after mini-batch   111: 0.592\n",
      "Loss after mini-batch   121: 0.543\n",
      "Loss after mini-batch   131: 32.128\n",
      "Loss after mini-batch   141: 0.201\n",
      "Loss after mini-batch   151: 8.999\n",
      "Loss after mini-batch   161: 3.384\n",
      "Loss after mini-batch   171: 0.671\n",
      "Loss after mini-batch   181: 0.219\n",
      "Loss after mini-batch   191: 0.303\n",
      "Loss after mini-batch   201: 0.706\n",
      "Loss after mini-batch   211: 0.042\n",
      "Loss after mini-batch   221: 0.246\n",
      "Loss after mini-batch   231: 0.064\n",
      "Loss after mini-batch   241: 4.287\n",
      "Loss after mini-batch   251: 0.157\n",
      "Loss after mini-batch   261: 2.207\n",
      "Loss after mini-batch   271: 15.812\n",
      "Loss after mini-batch   281: 7.593\n",
      "Loss after mini-batch   291: 0.629\n",
      "Loss after mini-batch   301: 1.568\n",
      "Loss after mini-batch   311: 13.772\n",
      "Loss after mini-batch   321: 2.444\n",
      "Loss after mini-batch   331: 5.433\n",
      "Loss after mini-batch   341: 8.162\n",
      "Loss after mini-batch   351: 0.427\n",
      "Loss after mini-batch   361: 0.716\n",
      "Loss after mini-batch   371: 51.332\n",
      "Loss after mini-batch   381: 0.418\n",
      "Loss after mini-batch   391: 2.877\n",
      "Loss after mini-batch   401: 1.922\n",
      "Loss after mini-batch   411: 3.388\n",
      "Loss after mini-batch   421: 0.080\n",
      "Loss after mini-batch   431: 0.392\n",
      "Loss after mini-batch   441: 5.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   451: 13.511\n",
      "Loss after mini-batch   461: 0.120\n",
      "Loss after mini-batch   471: 0.785\n",
      "Loss after mini-batch   481: 0.312\n",
      "Loss after mini-batch   491: 12.696\n",
      "Loss after mini-batch   501: 21.546\n",
      "Loss after mini-batch   511: 15.589\n",
      "Loss after mini-batch   521: 0.956\n",
      "Loss after mini-batch   531: 0.833\n",
      "Loss after mini-batch   541: 1.327\n",
      "Loss after mini-batch   551: 1.218\n",
      "Loss after mini-batch   561: 0.814\n",
      "Loss after mini-batch   571: 0.134\n",
      "Loss after mini-batch   581: 5.747\n",
      "Loss after mini-batch   591: 0.254\n",
      "Loss after mini-batch   601: 0.465\n",
      "Loss after mini-batch   611: 0.833\n",
      "Loss after mini-batch   621: 0.390\n",
      "Loss after mini-batch   631: 12.283\n",
      "Loss after mini-batch   641: 0.266\n",
      "Loss after mini-batch   651: 0.228\n",
      "Loss after mini-batch   661: 0.510\n",
      "Loss after mini-batch   671: 0.372\n",
      "Loss after mini-batch   681: 0.189\n",
      "Loss after mini-batch   691: 0.206\n",
      "Loss after mini-batch   701: 0.417\n",
      "Loss after mini-batch   711: 0.420\n",
      "Loss after mini-batch   721: 3.058\n",
      "Loss after mini-batch   731: 3.422\n",
      "Loss after mini-batch   741: 0.356\n",
      "Training Loss: 10.633 \t\t Validation Loss:18.590\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 7.569\n",
      "Loss after mini-batch    11: 0.926\n",
      "Loss after mini-batch    21: 3.385\n",
      "Loss after mini-batch    31: 0.286\n",
      "Loss after mini-batch    41: 1.728\n",
      "Loss after mini-batch    51: 0.226\n",
      "Loss after mini-batch    61: 0.162\n",
      "Loss after mini-batch    71: 0.000\n",
      "Loss after mini-batch    81: 0.344\n",
      "Loss after mini-batch    91: 0.900\n",
      "Loss after mini-batch   101: 0.925\n",
      "Loss after mini-batch   111: 0.401\n",
      "Loss after mini-batch   121: 0.244\n",
      "Loss after mini-batch   131: 0.229\n",
      "Loss after mini-batch   141: 0.052\n",
      "Loss after mini-batch   151: 0.224\n",
      "Loss after mini-batch   161: 0.532\n",
      "Loss after mini-batch   171: 1.338\n",
      "Loss after mini-batch   181: 0.659\n",
      "Loss after mini-batch   191: 0.060\n",
      "Loss after mini-batch   201: 0.610\n",
      "Loss after mini-batch   211: 0.167\n",
      "Loss after mini-batch   221: 0.029\n",
      "Loss after mini-batch   231: 1.264\n",
      "Loss after mini-batch   241: 12.367\n",
      "Loss after mini-batch   251: 2.319\n",
      "Loss after mini-batch   261: 12.088\n",
      "Loss after mini-batch   271: 3.203\n",
      "Loss after mini-batch   281: 14.678\n",
      "Loss after mini-batch   291: 0.479\n",
      "Loss after mini-batch   301: 1.053\n",
      "Loss after mini-batch   311: 0.623\n",
      "Loss after mini-batch   321: 3.992\n",
      "Loss after mini-batch   331: 0.620\n",
      "Loss after mini-batch   341: 0.823\n",
      "Loss after mini-batch   351: 0.361\n",
      "Loss after mini-batch   361: 0.455\n",
      "Loss after mini-batch   371: 0.787\n",
      "Loss after mini-batch   381: 0.676\n",
      "Loss after mini-batch   391: 0.717\n",
      "Loss after mini-batch   401: 5.218\n",
      "Loss after mini-batch   411: 0.448\n",
      "Loss after mini-batch   421: 0.030\n",
      "Loss after mini-batch   431: 1.737\n",
      "Loss after mini-batch   441: 11.003\n",
      "Loss after mini-batch   451: 0.226\n",
      "Loss after mini-batch   461: 1.171\n",
      "Loss after mini-batch   471: 2.739\n",
      "Loss after mini-batch   481: 14.972\n",
      "Loss after mini-batch   491: 34.626\n",
      "Loss after mini-batch   501: 5.904\n",
      "Loss after mini-batch   511: 0.961\n",
      "Loss after mini-batch   521: 0.186\n",
      "Loss after mini-batch   531: 15.185\n",
      "Loss after mini-batch   541: 7.665\n",
      "Loss after mini-batch   551: 4.206\n",
      "Loss after mini-batch   561: 0.907\n",
      "Loss after mini-batch   571: 1.133\n",
      "Loss after mini-batch   581: 0.533\n",
      "Loss after mini-batch   591: 0.823\n",
      "Loss after mini-batch   601: 0.006\n",
      "Loss after mini-batch   611: 0.042\n",
      "Loss after mini-batch   621: 5.007\n",
      "Loss after mini-batch   631: 0.097\n",
      "Loss after mini-batch   641: 0.111\n",
      "Loss after mini-batch   651: 16.495\n",
      "Loss after mini-batch   661: 0.259\n",
      "Loss after mini-batch   671: 1.806\n",
      "Loss after mini-batch   681: 2.263\n",
      "Loss after mini-batch   691: 0.403\n",
      "Loss after mini-batch   701: 0.489\n",
      "Loss after mini-batch   711: 0.083\n",
      "Loss after mini-batch   721: 20.385\n",
      "Loss after mini-batch   731: 0.575\n",
      "Loss after mini-batch   741: 0.521\n",
      "Training Loss: 5.433 \t\t Validation Loss:25.826\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 0.908\n",
      "Loss after mini-batch    11: 0.597\n",
      "Loss after mini-batch    21: 1.779\n",
      "Loss after mini-batch    31: 9.623\n",
      "Loss after mini-batch    41: 10.208\n",
      "Loss after mini-batch    51: 0.927\n",
      "Loss after mini-batch    61: 0.460\n",
      "Loss after mini-batch    71: 12.058\n",
      "Loss after mini-batch    81: 0.773\n",
      "Loss after mini-batch    91: 0.202\n",
      "Loss after mini-batch   101: 0.288\n",
      "Loss after mini-batch   111: 25.445\n",
      "Loss after mini-batch   121: 0.419\n",
      "Loss after mini-batch   131: 7.952\n",
      "Loss after mini-batch   141: 0.091\n",
      "Loss after mini-batch   151: 1.386\n",
      "Loss after mini-batch   161: 0.452\n",
      "Loss after mini-batch   171: 0.367\n",
      "Loss after mini-batch   181: 0.931\n",
      "Loss after mini-batch   191: 0.240\n",
      "Loss after mini-batch   201: 3.277\n",
      "Loss after mini-batch   211: 4.649\n",
      "Loss after mini-batch   221: 0.445\n",
      "Loss after mini-batch   231: 0.215\n",
      "Loss after mini-batch   241: 13.332\n",
      "Loss after mini-batch   251: 2.402\n",
      "Loss after mini-batch   261: 0.232\n",
      "Loss after mini-batch   271: 0.028\n",
      "Loss after mini-batch   281: 21.642\n",
      "Loss after mini-batch   291: 0.048\n",
      "Loss after mini-batch   301: 1.590\n",
      "Loss after mini-batch   311: 0.464\n",
      "Loss after mini-batch   321: 5.628\n",
      "Loss after mini-batch   331: 15.741\n",
      "Loss after mini-batch   341: 0.021\n",
      "Loss after mini-batch   351: 0.373\n",
      "Loss after mini-batch   361: 0.567\n",
      "Loss after mini-batch   371: 0.515\n",
      "Loss after mini-batch   381: 0.301\n",
      "Loss after mini-batch   391: 0.058\n",
      "Loss after mini-batch   401: 0.001\n",
      "Loss after mini-batch   411: 0.179\n",
      "Loss after mini-batch   421: 0.044\n",
      "Loss after mini-batch   431: 0.462\n",
      "Loss after mini-batch   441: 0.240\n",
      "Loss after mini-batch   451: 0.471\n",
      "Loss after mini-batch   461: 0.080\n",
      "Loss after mini-batch   471: 16.695\n",
      "Loss after mini-batch   481: 4.489\n",
      "Loss after mini-batch   491: 0.862\n",
      "Loss after mini-batch   501: 0.169\n",
      "Loss after mini-batch   511: 0.380\n",
      "Loss after mini-batch   521: 1.132\n",
      "Loss after mini-batch   531: 0.071\n",
      "Loss after mini-batch   541: 0.052\n",
      "Loss after mini-batch   551: 0.007\n",
      "Loss after mini-batch   561: 1.924\n",
      "Loss after mini-batch   571: 0.519\n",
      "Loss after mini-batch   581: 0.211\n",
      "Loss after mini-batch   591: 0.576\n",
      "Loss after mini-batch   601: 1.028\n",
      "Loss after mini-batch   611: 0.243\n",
      "Loss after mini-batch   621: 0.189\n",
      "Loss after mini-batch   631: 4.966\n",
      "Loss after mini-batch   641: 0.220\n",
      "Loss after mini-batch   651: 0.012\n",
      "Loss after mini-batch   661: 0.391\n",
      "Loss after mini-batch   671: 0.413\n",
      "Loss after mini-batch   681: 0.098\n",
      "Loss after mini-batch   691: 0.926\n",
      "Loss after mini-batch   701: 0.139\n",
      "Loss after mini-batch   711: 29.860\n",
      "Loss after mini-batch   721: 0.513\n",
      "Loss after mini-batch   731: 1.806\n",
      "Loss after mini-batch   741: 13.143\n",
      "Training Loss: 7.617 \t\t Validation Loss:7.859\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 0.660\n",
      "Loss after mini-batch    11: 0.566\n",
      "Loss after mini-batch    21: 18.471\n",
      "Loss after mini-batch    31: 0.338\n",
      "Loss after mini-batch    41: 0.374\n",
      "Loss after mini-batch    51: 12.590\n",
      "Loss after mini-batch    61: 0.897\n",
      "Loss after mini-batch    71: 0.504\n",
      "Loss after mini-batch    81: 0.367\n",
      "Loss after mini-batch    91: 2.506\n",
      "Loss after mini-batch   101: 0.372\n",
      "Loss after mini-batch   111: 0.095\n",
      "Loss after mini-batch   121: 1.650\n",
      "Loss after mini-batch   131: 0.193\n",
      "Loss after mini-batch   141: 15.927\n",
      "Loss after mini-batch   151: 22.726\n",
      "Loss after mini-batch   161: 0.609\n",
      "Loss after mini-batch   171: 2.329\n",
      "Loss after mini-batch   181: 0.529\n",
      "Loss after mini-batch   191: 0.368\n",
      "Loss after mini-batch   201: 0.148\n",
      "Loss after mini-batch   211: 0.310\n",
      "Loss after mini-batch   221: 2.382\n",
      "Loss after mini-batch   231: 13.634\n",
      "Loss after mini-batch   241: 1.145\n",
      "Loss after mini-batch   251: 0.603\n",
      "Loss after mini-batch   261: 54.182\n",
      "Loss after mini-batch   271: 0.225\n",
      "Loss after mini-batch   281: 0.720\n",
      "Loss after mini-batch   291: 0.463\n",
      "Loss after mini-batch   301: 2.380\n",
      "Loss after mini-batch   311: 0.903\n",
      "Loss after mini-batch   321: 0.709\n",
      "Loss after mini-batch   331: 0.300\n",
      "Loss after mini-batch   341: 0.220\n",
      "Loss after mini-batch   351: 0.028\n",
      "Loss after mini-batch   361: 0.470\n",
      "Loss after mini-batch   371: 11.467\n",
      "Loss after mini-batch   381: 31.911\n",
      "Loss after mini-batch   391: 0.018\n",
      "Loss after mini-batch   401: 0.744\n",
      "Loss after mini-batch   411: 1.029\n",
      "Loss after mini-batch   421: 59.440\n",
      "Loss after mini-batch   431: 41.339\n",
      "Loss after mini-batch   441: 0.195\n",
      "Loss after mini-batch   451: 0.054\n",
      "Loss after mini-batch   461: 0.106\n",
      "Loss after mini-batch   471: 0.172\n",
      "Loss after mini-batch   481: 0.167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   491: 0.953\n",
      "Loss after mini-batch   501: 0.323\n",
      "Loss after mini-batch   511: 51.648\n",
      "Loss after mini-batch   521: 30.486\n",
      "Loss after mini-batch   531: 2.602\n",
      "Loss after mini-batch   541: 0.436\n",
      "Loss after mini-batch   551: 1.678\n",
      "Loss after mini-batch   561: 2.414\n",
      "Loss after mini-batch   571: 0.063\n",
      "Loss after mini-batch   581: 0.761\n",
      "Loss after mini-batch   591: 4.756\n",
      "Loss after mini-batch   601: 0.453\n",
      "Loss after mini-batch   611: 5.731\n",
      "Loss after mini-batch   621: 1.051\n",
      "Loss after mini-batch   631: 0.689\n",
      "Loss after mini-batch   641: 0.206\n",
      "Loss after mini-batch   651: 0.111\n",
      "Loss after mini-batch   661: 0.377\n",
      "Loss after mini-batch   671: 0.306\n",
      "Loss after mini-batch   681: 21.490\n",
      "Loss after mini-batch   691: 0.056\n",
      "Loss after mini-batch   701: 11.049\n",
      "Loss after mini-batch   711: 0.731\n",
      "Loss after mini-batch   721: 0.076\n",
      "Loss after mini-batch   731: 90.710\n",
      "Loss after mini-batch   741: 3.057\n",
      "Training Loss: 4.438 \t\t Validation Loss:29.038\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 7.066\n",
      "Loss after mini-batch    11: 1.234\n",
      "Loss after mini-batch    21: 0.161\n",
      "Loss after mini-batch    31: 8.209\n",
      "Loss after mini-batch    41: 0.153\n",
      "Loss after mini-batch    51: 0.137\n",
      "Loss after mini-batch    61: 0.290\n",
      "Loss after mini-batch    71: 42.165\n",
      "Loss after mini-batch    81: 0.058\n",
      "Loss after mini-batch    91: 79.205\n",
      "Loss after mini-batch   101: 3.126\n",
      "Loss after mini-batch   111: 30.224\n",
      "Loss after mini-batch   121: 0.164\n",
      "Loss after mini-batch   131: 0.575\n",
      "Loss after mini-batch   141: 0.699\n",
      "Loss after mini-batch   151: 0.570\n",
      "Loss after mini-batch   161: 0.140\n",
      "Loss after mini-batch   171: 3.962\n",
      "Loss after mini-batch   181: 0.455\n",
      "Loss after mini-batch   191: 0.294\n",
      "Loss after mini-batch   201: 0.518\n",
      "Loss after mini-batch   211: 1.028\n",
      "Loss after mini-batch   221: 12.626\n",
      "Loss after mini-batch   231: 1.387\n",
      "Loss after mini-batch   241: 0.732\n",
      "Loss after mini-batch   251: 0.324\n",
      "Loss after mini-batch   261: 0.278\n",
      "Loss after mini-batch   271: 0.395\n",
      "Loss after mini-batch   281: 0.540\n",
      "Loss after mini-batch   291: 0.570\n",
      "Loss after mini-batch   301: 0.166\n",
      "Loss after mini-batch   311: 3.169\n",
      "Loss after mini-batch   321: 0.344\n",
      "Loss after mini-batch   331: 0.207\n",
      "Loss after mini-batch   341: 0.131\n",
      "Loss after mini-batch   351: 0.912\n",
      "Loss after mini-batch   361: 0.491\n",
      "Loss after mini-batch   371: 13.719\n",
      "Loss after mini-batch   381: 0.046\n",
      "Loss after mini-batch   391: 26.135\n",
      "Loss after mini-batch   401: 0.599\n",
      "Loss after mini-batch   411: 0.705\n",
      "Loss after mini-batch   421: 0.355\n",
      "Loss after mini-batch   431: 13.066\n",
      "Loss after mini-batch   441: 0.148\n",
      "Loss after mini-batch   451: 0.367\n",
      "Loss after mini-batch   461: 0.332\n",
      "Loss after mini-batch   471: 2.422\n",
      "Loss after mini-batch   481: 0.554\n",
      "Loss after mini-batch   491: 0.303\n",
      "Loss after mini-batch   501: 1.052\n",
      "Loss after mini-batch   511: 0.289\n",
      "Loss after mini-batch   521: 10.201\n",
      "Loss after mini-batch   531: 1.698\n",
      "Loss after mini-batch   541: 0.607\n",
      "Loss after mini-batch   551: 5.085\n",
      "Loss after mini-batch   561: 0.500\n",
      "Loss after mini-batch   571: 0.015\n",
      "Loss after mini-batch   581: 0.245\n",
      "Loss after mini-batch   591: 0.404\n",
      "Loss after mini-batch   601: 0.271\n",
      "Loss after mini-batch   611: 0.291\n",
      "Loss after mini-batch   621: 0.185\n",
      "Loss after mini-batch   631: 2.467\n",
      "Loss after mini-batch   641: 0.079\n",
      "Loss after mini-batch   651: 0.740\n",
      "Loss after mini-batch   661: 0.319\n",
      "Loss after mini-batch   671: 0.647\n",
      "Loss after mini-batch   681: 0.501\n",
      "Loss after mini-batch   691: 0.382\n",
      "Loss after mini-batch   701: 3.924\n",
      "Loss after mini-batch   711: 6.142\n",
      "Loss after mini-batch   721: 0.189\n",
      "Loss after mini-batch   731: 0.341\n",
      "Loss after mini-batch   741: 0.106\n",
      "Training Loss: 0.104 \t\t Validation Loss:0.126\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.011\n",
      "Loss after mini-batch    11: 13.857\n",
      "Loss after mini-batch    21: 0.255\n",
      "Loss after mini-batch    31: 0.137\n",
      "Loss after mini-batch    41: 4.916\n",
      "Loss after mini-batch    51: 0.342\n",
      "Loss after mini-batch    61: 0.404\n",
      "Loss after mini-batch    71: 0.067\n",
      "Loss after mini-batch    81: 0.077\n",
      "Loss after mini-batch    91: 0.043\n",
      "Loss after mini-batch   101: 0.102\n",
      "Loss after mini-batch   111: 0.380\n",
      "Loss after mini-batch   121: 0.782\n",
      "Loss after mini-batch   131: 11.565\n",
      "Loss after mini-batch   141: 0.237\n",
      "Loss after mini-batch   151: 0.115\n",
      "Loss after mini-batch   161: 0.196\n",
      "Loss after mini-batch   171: 0.051\n",
      "Loss after mini-batch   181: 3.671\n",
      "Loss after mini-batch   191: 0.024\n",
      "Loss after mini-batch   201: 0.443\n",
      "Loss after mini-batch   211: 0.401\n",
      "Loss after mini-batch   221: 0.201\n",
      "Loss after mini-batch   231: 13.357\n",
      "Loss after mini-batch   241: 0.024\n",
      "Loss after mini-batch   251: 0.884\n",
      "Loss after mini-batch   261: 0.050\n",
      "Loss after mini-batch   271: 0.314\n",
      "Loss after mini-batch   281: 1.913\n",
      "Loss after mini-batch   291: 0.055\n",
      "Loss after mini-batch   301: 55.875\n",
      "Loss after mini-batch   311: 0.485\n",
      "Loss after mini-batch   321: 1.351\n",
      "Loss after mini-batch   331: 11.820\n",
      "Loss after mini-batch   341: 0.511\n",
      "Loss after mini-batch   351: 0.777\n",
      "Loss after mini-batch   361: 0.105\n",
      "Loss after mini-batch   371: 0.314\n",
      "Loss after mini-batch   381: 1.413\n",
      "Loss after mini-batch   391: 0.010\n",
      "Loss after mini-batch   401: 3.021\n",
      "Loss after mini-batch   411: 0.274\n",
      "Loss after mini-batch   421: 1.662\n",
      "Loss after mini-batch   431: 0.408\n",
      "Loss after mini-batch   441: 0.783\n",
      "Loss after mini-batch   451: 0.316\n",
      "Loss after mini-batch   461: 0.388\n",
      "Loss after mini-batch   471: 33.012\n",
      "Loss after mini-batch   481: 59.544\n",
      "Loss after mini-batch   491: 0.310\n",
      "Loss after mini-batch   501: 2.077\n",
      "Loss after mini-batch   511: 21.953\n",
      "Loss after mini-batch   521: 0.699\n",
      "Loss after mini-batch   531: 0.585\n",
      "Loss after mini-batch   541: 0.636\n",
      "Loss after mini-batch   551: 2.890\n",
      "Loss after mini-batch   561: 18.547\n",
      "Loss after mini-batch   571: 0.927\n",
      "Loss after mini-batch   581: 1.458\n",
      "Loss after mini-batch   591: 9.926\n",
      "Loss after mini-batch   601: 0.175\n",
      "Loss after mini-batch   611: 0.221\n",
      "Loss after mini-batch   621: 4.813\n",
      "Loss after mini-batch   631: 1.465\n",
      "Loss after mini-batch   641: 0.394\n",
      "Loss after mini-batch   651: 0.050\n",
      "Loss after mini-batch   661: 0.063\n",
      "Loss after mini-batch   671: 0.569\n",
      "Loss after mini-batch   681: 0.280\n",
      "Loss after mini-batch   691: 0.083\n",
      "Loss after mini-batch   701: 0.222\n",
      "Loss after mini-batch   711: 2.196\n",
      "Loss after mini-batch   721: 11.129\n",
      "Loss after mini-batch   731: 2.077\n",
      "Loss after mini-batch   741: 0.950\n",
      "Training Loss: 0.070 \t\t Validation Loss:4.034\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 2.921\n",
      "Loss after mini-batch    11: 0.013\n",
      "Loss after mini-batch    21: 9.901\n",
      "Loss after mini-batch    31: 0.584\n",
      "Loss after mini-batch    41: 0.917\n",
      "Loss after mini-batch    51: 0.415\n",
      "Loss after mini-batch    61: 0.096\n",
      "Loss after mini-batch    71: 68.406\n",
      "Loss after mini-batch    81: 5.542\n",
      "Loss after mini-batch    91: 0.790\n",
      "Loss after mini-batch   101: 0.107\n",
      "Loss after mini-batch   111: 26.585\n",
      "Loss after mini-batch   121: 0.138\n",
      "Loss after mini-batch   131: 7.564\n",
      "Loss after mini-batch   141: 13.667\n",
      "Loss after mini-batch   151: 0.042\n",
      "Loss after mini-batch   161: 0.218\n",
      "Loss after mini-batch   171: 0.588\n",
      "Loss after mini-batch   181: 5.911\n",
      "Loss after mini-batch   191: 0.361\n",
      "Loss after mini-batch   201: 2.637\n",
      "Loss after mini-batch   211: 22.635\n",
      "Loss after mini-batch   221: 16.896\n",
      "Loss after mini-batch   231: 0.504\n",
      "Loss after mini-batch   241: 1.367\n",
      "Loss after mini-batch   251: 1.256\n",
      "Loss after mini-batch   261: 0.717\n",
      "Loss after mini-batch   271: 11.542\n",
      "Loss after mini-batch   281: 0.026\n",
      "Loss after mini-batch   291: 0.303\n",
      "Loss after mini-batch   301: 0.209\n",
      "Loss after mini-batch   311: 57.186\n",
      "Loss after mini-batch   321: 0.077\n",
      "Loss after mini-batch   331: 0.009\n",
      "Loss after mini-batch   341: 0.080\n",
      "Loss after mini-batch   351: 0.049\n",
      "Loss after mini-batch   361: 0.018\n",
      "Loss after mini-batch   371: 0.410\n",
      "Loss after mini-batch   381: 0.812\n",
      "Loss after mini-batch   391: 0.276\n",
      "Loss after mini-batch   401: 2.461\n",
      "Loss after mini-batch   411: 13.768\n",
      "Loss after mini-batch   421: 5.696\n",
      "Loss after mini-batch   431: 31.884\n",
      "Loss after mini-batch   441: 1.110\n",
      "Loss after mini-batch   451: 0.535\n",
      "Loss after mini-batch   461: 0.452\n",
      "Loss after mini-batch   471: 0.206\n",
      "Loss after mini-batch   481: 2.914\n",
      "Loss after mini-batch   491: 5.402\n",
      "Loss after mini-batch   501: 0.807\n",
      "Loss after mini-batch   511: 20.915\n",
      "Loss after mini-batch   521: 1.072\n",
      "Loss after mini-batch   531: 22.501\n",
      "Loss after mini-batch   541: 1.779\n",
      "Loss after mini-batch   551: 0.791\n",
      "Loss after mini-batch   561: 0.221\n",
      "Loss after mini-batch   571: 0.201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   581: 0.072\n",
      "Loss after mini-batch   591: 0.205\n",
      "Loss after mini-batch   601: 0.518\n",
      "Loss after mini-batch   611: 56.613\n",
      "Loss after mini-batch   621: 0.480\n",
      "Loss after mini-batch   631: 0.015\n",
      "Loss after mini-batch   641: 40.213\n",
      "Loss after mini-batch   651: 0.691\n",
      "Loss after mini-batch   661: 0.445\n",
      "Loss after mini-batch   671: 0.066\n",
      "Loss after mini-batch   681: 0.359\n",
      "Loss after mini-batch   691: 0.060\n",
      "Loss after mini-batch   701: 0.050\n",
      "Loss after mini-batch   711: 0.238\n",
      "Loss after mini-batch   721: 0.091\n",
      "Loss after mini-batch   731: 12.162\n",
      "Loss after mini-batch   741: 0.040\n",
      "Training Loss: 32.088 \t\t Validation Loss:32.464\n",
      "Starting epoch 29\n",
      "Loss after mini-batch     1: 1.522\n",
      "Loss after mini-batch    11: 1.370\n",
      "Loss after mini-batch    21: 68.690\n",
      "Loss after mini-batch    31: 2.327\n",
      "Loss after mini-batch    41: 0.484\n",
      "Loss after mini-batch    51: 0.850\n",
      "Loss after mini-batch    61: 0.874\n",
      "Loss after mini-batch    71: 1.880\n",
      "Loss after mini-batch    81: 10.899\n",
      "Loss after mini-batch    91: 0.021\n",
      "Loss after mini-batch   101: 2.220\n",
      "Loss after mini-batch   111: 0.101\n",
      "Loss after mini-batch   121: 7.748\n",
      "Loss after mini-batch   131: 0.245\n",
      "Loss after mini-batch   141: 1.103\n",
      "Loss after mini-batch   151: 0.038\n",
      "Loss after mini-batch   161: 0.744\n",
      "Loss after mini-batch   171: 1.160\n",
      "Loss after mini-batch   181: 0.857\n",
      "Loss after mini-batch   191: 0.081\n",
      "Loss after mini-batch   201: 0.603\n",
      "Loss after mini-batch   211: 1.214\n",
      "Loss after mini-batch   221: 0.040\n",
      "Loss after mini-batch   231: 0.127\n",
      "Loss after mini-batch   241: 0.318\n",
      "Loss after mini-batch   251: 0.602\n",
      "Loss after mini-batch   261: 0.184\n",
      "Loss after mini-batch   271: 0.263\n",
      "Loss after mini-batch   281: 12.648\n",
      "Loss after mini-batch   291: 8.489\n",
      "Loss after mini-batch   301: 0.262\n",
      "Loss after mini-batch   311: 26.227\n",
      "Loss after mini-batch   321: 0.135\n",
      "Loss after mini-batch   331: 0.043\n",
      "Loss after mini-batch   341: 53.356\n",
      "Loss after mini-batch   351: 10.973\n",
      "Loss after mini-batch   361: 0.674\n",
      "Loss after mini-batch   371: 56.587\n",
      "Loss after mini-batch   381: 28.281\n",
      "Loss after mini-batch   391: 0.444\n",
      "Loss after mini-batch   401: 0.115\n",
      "Loss after mini-batch   411: 4.662\n",
      "Loss after mini-batch   421: 0.412\n",
      "Loss after mini-batch   431: 0.017\n",
      "Loss after mini-batch   441: 10.815\n",
      "Loss after mini-batch   451: 2.665\n",
      "Loss after mini-batch   461: 0.365\n",
      "Loss after mini-batch   471: 0.163\n",
      "Loss after mini-batch   481: 1.075\n",
      "Loss after mini-batch   491: 0.514\n",
      "Loss after mini-batch   501: 0.032\n",
      "Loss after mini-batch   511: 1.618\n",
      "Loss after mini-batch   521: 0.113\n",
      "Loss after mini-batch   531: 0.299\n",
      "Loss after mini-batch   541: 5.061\n",
      "Loss after mini-batch   551: 0.163\n",
      "Loss after mini-batch   561: 0.075\n",
      "Loss after mini-batch   571: 3.993\n",
      "Loss after mini-batch   581: 53.719\n",
      "Loss after mini-batch   591: 13.969\n",
      "Loss after mini-batch   601: 0.070\n",
      "Loss after mini-batch   611: 13.607\n",
      "Loss after mini-batch   621: 0.165\n",
      "Loss after mini-batch   631: 0.078\n",
      "Loss after mini-batch   641: 0.242\n",
      "Loss after mini-batch   651: 1.541\n",
      "Loss after mini-batch   661: 2.123\n",
      "Loss after mini-batch   671: 0.079\n",
      "Loss after mini-batch   681: 0.044\n",
      "Loss after mini-batch   691: 0.025\n",
      "Loss after mini-batch   701: 13.188\n",
      "Loss after mini-batch   711: 15.451\n",
      "Loss after mini-batch   721: 18.020\n",
      "Loss after mini-batch   731: 0.684\n",
      "Loss after mini-batch   741: 0.600\n",
      "Training Loss: 0.068 \t\t Validation Loss:0.889\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 0.107\n",
      "Loss after mini-batch    11: 49.046\n",
      "Loss after mini-batch    21: 0.593\n",
      "Loss after mini-batch    31: 8.320\n",
      "Loss after mini-batch    41: 15.529\n",
      "Loss after mini-batch    51: 1.073\n",
      "Loss after mini-batch    61: 0.063\n",
      "Loss after mini-batch    71: 0.044\n",
      "Loss after mini-batch    81: 0.189\n",
      "Loss after mini-batch    91: 53.503\n",
      "Loss after mini-batch   101: 0.200\n",
      "Loss after mini-batch   111: 4.564\n",
      "Loss after mini-batch   121: 0.021\n",
      "Loss after mini-batch   131: 0.373\n",
      "Loss after mini-batch   141: 3.659\n",
      "Loss after mini-batch   151: 0.394\n",
      "Loss after mini-batch   161: 5.010\n",
      "Loss after mini-batch   171: 1.571\n",
      "Loss after mini-batch   181: 15.465\n",
      "Loss after mini-batch   191: 0.165\n",
      "Loss after mini-batch   201: 0.655\n",
      "Loss after mini-batch   211: 0.088\n",
      "Loss after mini-batch   221: 1.401\n",
      "Loss after mini-batch   231: 6.275\n",
      "Loss after mini-batch   241: 0.809\n",
      "Loss after mini-batch   251: 1.146\n",
      "Loss after mini-batch   261: 0.672\n",
      "Loss after mini-batch   271: 11.353\n",
      "Loss after mini-batch   281: 0.069\n",
      "Loss after mini-batch   291: 0.992\n",
      "Loss after mini-batch   301: 0.179\n",
      "Loss after mini-batch   311: 13.192\n",
      "Loss after mini-batch   321: 0.214\n",
      "Loss after mini-batch   331: 0.475\n",
      "Loss after mini-batch   341: 1.450\n",
      "Loss after mini-batch   351: 0.429\n",
      "Loss after mini-batch   361: 0.821\n",
      "Loss after mini-batch   371: 0.263\n",
      "Loss after mini-batch   381: 2.114\n",
      "Loss after mini-batch   391: 0.043\n",
      "Loss after mini-batch   401: 0.643\n",
      "Loss after mini-batch   411: 0.298\n",
      "Loss after mini-batch   421: 0.053\n",
      "Loss after mini-batch   431: 1.241\n",
      "Loss after mini-batch   441: 0.144\n",
      "Loss after mini-batch   451: 0.085\n",
      "Loss after mini-batch   461: 0.543\n",
      "Loss after mini-batch   471: 0.070\n",
      "Loss after mini-batch   481: 0.357\n",
      "Loss after mini-batch   491: 91.398\n",
      "Loss after mini-batch   501: 0.344\n",
      "Loss after mini-batch   511: 0.222\n",
      "Loss after mini-batch   521: 32.117\n",
      "Loss after mini-batch   531: 0.090\n",
      "Loss after mini-batch   541: 0.492\n",
      "Loss after mini-batch   551: 2.871\n",
      "Loss after mini-batch   561: 0.457\n",
      "Loss after mini-batch   571: 0.399\n",
      "Loss after mini-batch   581: 0.203\n",
      "Loss after mini-batch   591: 0.202\n",
      "Loss after mini-batch   601: 1.890\n",
      "Loss after mini-batch   611: 1.513\n",
      "Loss after mini-batch   621: 0.505\n",
      "Loss after mini-batch   631: 0.259\n",
      "Loss after mini-batch   641: 0.173\n",
      "Loss after mini-batch   651: 18.004\n",
      "Loss after mini-batch   661: 0.072\n",
      "Loss after mini-batch   671: 1.199\n",
      "Loss after mini-batch   681: 13.848\n",
      "Loss after mini-batch   691: 1.531\n",
      "Loss after mini-batch   701: 0.254\n",
      "Loss after mini-batch   711: 4.091\n",
      "Loss after mini-batch   721: 30.637\n",
      "Loss after mini-batch   731: 0.137\n",
      "Loss after mini-batch   741: 1.258\n",
      "Training Loss: 0.813 \t\t Validation Loss:0.885\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 0.843\n",
      "Loss after mini-batch    11: 0.248\n",
      "Loss after mini-batch    21: 47.965\n",
      "Loss after mini-batch    31: 1.983\n",
      "Loss after mini-batch    41: 1.557\n",
      "Loss after mini-batch    51: 4.754\n",
      "Loss after mini-batch    61: 1.062\n",
      "Loss after mini-batch    71: 1.904\n",
      "Loss after mini-batch    81: 0.088\n",
      "Loss after mini-batch    91: 0.107\n",
      "Loss after mini-batch   101: 4.123\n",
      "Loss after mini-batch   111: 0.037\n",
      "Loss after mini-batch   121: 0.154\n",
      "Loss after mini-batch   131: 1.164\n",
      "Loss after mini-batch   141: 56.688\n",
      "Loss after mini-batch   151: 0.349\n",
      "Loss after mini-batch   161: 1.603\n",
      "Loss after mini-batch   171: 0.234\n",
      "Loss after mini-batch   181: 1.814\n",
      "Loss after mini-batch   191: 17.692\n",
      "Loss after mini-batch   201: 0.313\n",
      "Loss after mini-batch   211: 2.121\n",
      "Loss after mini-batch   221: 30.999\n",
      "Loss after mini-batch   231: 0.175\n",
      "Loss after mini-batch   241: 1.040\n",
      "Loss after mini-batch   251: 0.221\n",
      "Loss after mini-batch   261: 53.365\n",
      "Loss after mini-batch   271: 0.092\n",
      "Loss after mini-batch   281: 7.139\n",
      "Loss after mini-batch   291: 1.608\n",
      "Loss after mini-batch   301: 3.766\n",
      "Loss after mini-batch   311: 0.623\n",
      "Loss after mini-batch   321: 0.502\n",
      "Loss after mini-batch   331: 12.966\n",
      "Loss after mini-batch   341: 0.735\n",
      "Loss after mini-batch   351: 2.433\n",
      "Loss after mini-batch   361: 0.232\n",
      "Loss after mini-batch   371: 0.634\n",
      "Loss after mini-batch   381: 0.676\n",
      "Loss after mini-batch   391: 1.194\n",
      "Loss after mini-batch   401: 0.603\n",
      "Loss after mini-batch   411: 0.921\n",
      "Loss after mini-batch   421: 13.445\n",
      "Loss after mini-batch   431: 0.120\n",
      "Loss after mini-batch   441: 0.326\n",
      "Loss after mini-batch   451: 1.648\n",
      "Loss after mini-batch   461: 0.043\n",
      "Loss after mini-batch   471: 75.863\n",
      "Loss after mini-batch   481: 0.460\n",
      "Loss after mini-batch   491: 0.943\n",
      "Loss after mini-batch   501: 0.608\n",
      "Loss after mini-batch   511: 0.647\n",
      "Loss after mini-batch   521: 3.366\n",
      "Loss after mini-batch   531: 8.487\n",
      "Loss after mini-batch   541: 0.283\n",
      "Loss after mini-batch   551: 0.354\n",
      "Loss after mini-batch   561: 1.158\n",
      "Loss after mini-batch   571: 1.525\n",
      "Loss after mini-batch   581: 10.693\n",
      "Loss after mini-batch   591: 11.658\n",
      "Loss after mini-batch   601: 4.941\n",
      "Loss after mini-batch   611: 13.253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   621: 0.464\n",
      "Loss after mini-batch   631: 1.645\n",
      "Loss after mini-batch   641: 12.121\n",
      "Loss after mini-batch   651: 0.114\n",
      "Loss after mini-batch   661: 1.651\n",
      "Loss after mini-batch   671: 7.357\n",
      "Loss after mini-batch   681: 57.169\n",
      "Loss after mini-batch   691: 0.115\n",
      "Loss after mini-batch   701: 9.044\n",
      "Loss after mini-batch   711: 4.002\n",
      "Loss after mini-batch   721: 0.364\n",
      "Loss after mini-batch   731: 0.055\n",
      "Loss after mini-batch   741: 0.438\n",
      "Training Loss: 0.623 \t\t Validation Loss:0.925\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 0.096\n",
      "Loss after mini-batch    11: 0.180\n",
      "Loss after mini-batch    21: 2.471\n",
      "Loss after mini-batch    31: 0.436\n",
      "Loss after mini-batch    41: 47.985\n",
      "Loss after mini-batch    51: 7.610\n",
      "Loss after mini-batch    61: 0.581\n",
      "Loss after mini-batch    71: 0.005\n",
      "Loss after mini-batch    81: 0.392\n",
      "Loss after mini-batch    91: 10.462\n",
      "Loss after mini-batch   101: 1.561\n",
      "Loss after mini-batch   111: 2.629\n",
      "Loss after mini-batch   121: 0.117\n",
      "Loss after mini-batch   131: 0.599\n",
      "Loss after mini-batch   141: 11.645\n",
      "Loss after mini-batch   151: 0.354\n",
      "Loss after mini-batch   161: 0.007\n",
      "Loss after mini-batch   171: 0.114\n",
      "Loss after mini-batch   181: 8.574\n",
      "Loss after mini-batch   191: 2.195\n",
      "Loss after mini-batch   201: 0.208\n",
      "Loss after mini-batch   211: 0.135\n",
      "Loss after mini-batch   221: 42.083\n",
      "Loss after mini-batch   231: 0.511\n",
      "Loss after mini-batch   241: 0.342\n",
      "Loss after mini-batch   251: 0.619\n",
      "Loss after mini-batch   261: 1.067\n",
      "Loss after mini-batch   271: 0.714\n",
      "Loss after mini-batch   281: 13.404\n",
      "Loss after mini-batch   291: 0.072\n",
      "Loss after mini-batch   301: 0.048\n",
      "Loss after mini-batch   311: 0.503\n",
      "Loss after mini-batch   321: 1.324\n",
      "Loss after mini-batch   331: 0.128\n",
      "Loss after mini-batch   341: 0.451\n",
      "Loss after mini-batch   351: 4.630\n",
      "Loss after mini-batch   361: 1.139\n",
      "Loss after mini-batch   371: 30.566\n",
      "Loss after mini-batch   381: 0.679\n",
      "Loss after mini-batch   391: 0.132\n",
      "Loss after mini-batch   401: 0.078\n",
      "Loss after mini-batch   411: 1.558\n",
      "Loss after mini-batch   421: 0.030\n",
      "Loss after mini-batch   431: 0.956\n",
      "Loss after mini-batch   441: 22.359\n",
      "Loss after mini-batch   451: 37.079\n",
      "Loss after mini-batch   461: 0.151\n",
      "Loss after mini-batch   471: 0.218\n",
      "Loss after mini-batch   481: 0.058\n",
      "Loss after mini-batch   491: 1.223\n",
      "Loss after mini-batch   501: 0.190\n",
      "Loss after mini-batch   511: 1.308\n",
      "Loss after mini-batch   521: 51.993\n",
      "Loss after mini-batch   531: 0.820\n",
      "Loss after mini-batch   541: 1.616\n",
      "Loss after mini-batch   551: 9.417\n",
      "Loss after mini-batch   561: 0.224\n",
      "Loss after mini-batch   571: 0.673\n",
      "Loss after mini-batch   581: 0.201\n",
      "Loss after mini-batch   591: 1.076\n",
      "Loss after mini-batch   601: 1.246\n",
      "Loss after mini-batch   611: 0.069\n",
      "Loss after mini-batch   621: 0.246\n",
      "Loss after mini-batch   631: 0.102\n",
      "Loss after mini-batch   641: 4.880\n",
      "Loss after mini-batch   651: 3.976\n",
      "Loss after mini-batch   661: 4.243\n",
      "Loss after mini-batch   671: 0.691\n",
      "Loss after mini-batch   681: 0.217\n",
      "Loss after mini-batch   691: 0.118\n",
      "Loss after mini-batch   701: 0.517\n",
      "Loss after mini-batch   711: 2.391\n",
      "Loss after mini-batch   721: 1.379\n",
      "Loss after mini-batch   731: 18.685\n",
      "Loss after mini-batch   741: 0.782\n",
      "Training Loss: 3.544 \t\t Validation Loss:3.577\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 1.875\n",
      "Loss after mini-batch    11: 0.012\n",
      "Loss after mini-batch    21: 0.058\n",
      "Loss after mini-batch    31: 0.391\n",
      "Loss after mini-batch    41: 0.006\n",
      "Loss after mini-batch    51: 1.548\n",
      "Loss after mini-batch    61: 0.125\n",
      "Loss after mini-batch    71: 0.452\n",
      "Loss after mini-batch    81: 0.246\n",
      "Loss after mini-batch    91: 0.857\n",
      "Loss after mini-batch   101: 0.174\n",
      "Loss after mini-batch   111: 0.104\n",
      "Loss after mini-batch   121: 1.412\n",
      "Loss after mini-batch   131: 7.986\n",
      "Loss after mini-batch   141: 0.246\n",
      "Loss after mini-batch   151: 0.107\n",
      "Loss after mini-batch   161: 0.012\n",
      "Loss after mini-batch   171: 0.159\n",
      "Loss after mini-batch   181: 0.656\n",
      "Loss after mini-batch   191: 0.081\n",
      "Loss after mini-batch   201: 0.872\n",
      "Loss after mini-batch   211: 13.731\n",
      "Loss after mini-batch   221: 0.404\n",
      "Loss after mini-batch   231: 0.228\n",
      "Loss after mini-batch   241: 0.338\n",
      "Loss after mini-batch   251: 1.727\n",
      "Loss after mini-batch   261: 0.153\n",
      "Loss after mini-batch   271: 9.073\n",
      "Loss after mini-batch   281: 0.066\n",
      "Loss after mini-batch   291: 0.135\n",
      "Loss after mini-batch   301: 0.698\n",
      "Loss after mini-batch   311: 1.675\n",
      "Loss after mini-batch   321: 3.032\n",
      "Loss after mini-batch   331: 0.354\n",
      "Loss after mini-batch   341: 0.473\n",
      "Loss after mini-batch   351: 0.645\n",
      "Loss after mini-batch   361: 0.088\n",
      "Loss after mini-batch   371: 1.554\n",
      "Loss after mini-batch   381: 0.130\n",
      "Loss after mini-batch   391: 12.636\n",
      "Loss after mini-batch   401: 0.183\n",
      "Loss after mini-batch   411: 11.042\n",
      "Loss after mini-batch   421: 0.023\n",
      "Loss after mini-batch   431: 12.872\n",
      "Loss after mini-batch   441: 0.180\n",
      "Loss after mini-batch   451: 1.848\n",
      "Loss after mini-batch   461: 0.740\n",
      "Loss after mini-batch   471: 0.041\n",
      "Loss after mini-batch   481: 0.906\n",
      "Loss after mini-batch   491: 0.159\n",
      "Loss after mini-batch   501: 0.115\n",
      "Loss after mini-batch   511: 0.334\n",
      "Loss after mini-batch   521: 10.416\n",
      "Loss after mini-batch   531: 5.289\n",
      "Loss after mini-batch   541: 0.121\n",
      "Loss after mini-batch   551: 0.288\n",
      "Loss after mini-batch   561: 1.283\n",
      "Loss after mini-batch   571: 12.649\n",
      "Loss after mini-batch   581: 0.560\n",
      "Loss after mini-batch   591: 0.258\n",
      "Loss after mini-batch   601: 0.269\n",
      "Loss after mini-batch   611: 0.182\n",
      "Loss after mini-batch   621: 0.282\n",
      "Loss after mini-batch   631: 2.655\n",
      "Loss after mini-batch   641: 0.093\n",
      "Loss after mini-batch   651: 19.249\n",
      "Loss after mini-batch   661: 0.734\n",
      "Loss after mini-batch   671: 2.588\n",
      "Loss after mini-batch   681: 3.190\n",
      "Loss after mini-batch   691: 28.578\n",
      "Loss after mini-batch   701: 0.176\n",
      "Loss after mini-batch   711: 8.693\n",
      "Loss after mini-batch   721: 30.135\n",
      "Loss after mini-batch   731: 2.608\n",
      "Loss after mini-batch   741: 5.715\n",
      "Training Loss: 0.224 \t\t Validation Loss:3.989\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 0.007\n",
      "Loss after mini-batch    11: 12.850\n",
      "Loss after mini-batch    21: 2.077\n",
      "Loss after mini-batch    31: 0.029\n",
      "Loss after mini-batch    41: 1.003\n",
      "Loss after mini-batch    51: 0.015\n",
      "Loss after mini-batch    61: 12.324\n",
      "Loss after mini-batch    71: 0.105\n",
      "Loss after mini-batch    81: 0.535\n",
      "Loss after mini-batch    91: 0.435\n",
      "Loss after mini-batch   101: 0.913\n",
      "Loss after mini-batch   111: 2.142\n",
      "Loss after mini-batch   121: 0.492\n",
      "Loss after mini-batch   131: 0.327\n",
      "Loss after mini-batch   141: 0.324\n",
      "Loss after mini-batch   151: 0.231\n",
      "Loss after mini-batch   161: 0.286\n",
      "Loss after mini-batch   171: 1.301\n",
      "Loss after mini-batch   181: 0.033\n",
      "Loss after mini-batch   191: 25.432\n",
      "Loss after mini-batch   201: 17.025\n",
      "Loss after mini-batch   211: 1.730\n",
      "Loss after mini-batch   221: 0.161\n",
      "Loss after mini-batch   231: 0.096\n",
      "Loss after mini-batch   241: 5.622\n",
      "Loss after mini-batch   251: 0.397\n",
      "Loss after mini-batch   261: 0.108\n",
      "Loss after mini-batch   271: 15.976\n",
      "Loss after mini-batch   281: 0.055\n",
      "Loss after mini-batch   291: 0.412\n",
      "Loss after mini-batch   301: 0.123\n",
      "Loss after mini-batch   311: 13.632\n",
      "Loss after mini-batch   321: 5.807\n",
      "Loss after mini-batch   331: 1.870\n",
      "Loss after mini-batch   341: 0.200\n",
      "Loss after mini-batch   351: 1.015\n",
      "Loss after mini-batch   361: 0.794\n",
      "Loss after mini-batch   371: 12.355\n",
      "Loss after mini-batch   381: 20.166\n",
      "Loss after mini-batch   391: 0.962\n",
      "Loss after mini-batch   401: 21.131\n",
      "Loss after mini-batch   411: 18.778\n",
      "Loss after mini-batch   421: 0.052\n",
      "Loss after mini-batch   431: 0.061\n",
      "Loss after mini-batch   441: 0.323\n",
      "Loss after mini-batch   451: 0.109\n",
      "Loss after mini-batch   461: 0.314\n",
      "Loss after mini-batch   471: 3.696\n",
      "Loss after mini-batch   481: 1.099\n",
      "Loss after mini-batch   491: 0.020\n",
      "Loss after mini-batch   501: 54.884\n",
      "Loss after mini-batch   511: 0.179\n",
      "Loss after mini-batch   521: 1.480\n",
      "Loss after mini-batch   531: 8.119\n",
      "Loss after mini-batch   541: 0.266\n",
      "Loss after mini-batch   551: 16.752\n",
      "Loss after mini-batch   561: 90.306\n",
      "Loss after mini-batch   571: 0.043\n",
      "Loss after mini-batch   581: 2.860\n",
      "Loss after mini-batch   591: 23.924\n",
      "Loss after mini-batch   601: 3.471\n",
      "Loss after mini-batch   611: 0.067\n",
      "Loss after mini-batch   621: 0.117\n",
      "Loss after mini-batch   631: 0.497\n",
      "Loss after mini-batch   641: 18.980\n",
      "Loss after mini-batch   651: 0.194\n",
      "Loss after mini-batch   661: 0.133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   671: 1.043\n",
      "Loss after mini-batch   681: 1.273\n",
      "Loss after mini-batch   691: 0.846\n",
      "Loss after mini-batch   701: 28.340\n",
      "Loss after mini-batch   711: 0.531\n",
      "Loss after mini-batch   721: 0.172\n",
      "Loss after mini-batch   731: 0.592\n",
      "Loss after mini-batch   741: 13.669\n",
      "Training Loss: 0.012 \t\t Validation Loss:2.391\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 10.579\n",
      "Loss after mini-batch    11: 0.137\n",
      "Loss after mini-batch    21: 0.156\n",
      "Loss after mini-batch    31: 0.304\n",
      "Loss after mini-batch    41: 0.516\n",
      "Loss after mini-batch    51: 0.093\n",
      "Loss after mini-batch    61: 46.283\n",
      "Loss after mini-batch    71: 0.007\n",
      "Loss after mini-batch    81: 0.026\n",
      "Loss after mini-batch    91: 0.242\n",
      "Loss after mini-batch   101: 18.699\n",
      "Loss after mini-batch   111: 0.819\n",
      "Loss after mini-batch   121: 0.002\n",
      "Loss after mini-batch   131: 0.074\n",
      "Loss after mini-batch   141: 0.164\n",
      "Loss after mini-batch   151: 0.622\n",
      "Loss after mini-batch   161: 0.505\n",
      "Loss after mini-batch   171: 0.941\n",
      "Loss after mini-batch   181: 11.779\n",
      "Loss after mini-batch   191: 0.244\n",
      "Loss after mini-batch   201: 22.841\n",
      "Loss after mini-batch   211: 30.128\n",
      "Loss after mini-batch   221: 1.875\n",
      "Loss after mini-batch   231: 0.617\n",
      "Loss after mini-batch   241: 0.087\n",
      "Loss after mini-batch   251: 1.441\n",
      "Loss after mini-batch   261: 2.343\n",
      "Loss after mini-batch   271: 35.059\n",
      "Loss after mini-batch   281: 1.612\n",
      "Loss after mini-batch   291: 0.848\n",
      "Loss after mini-batch   301: 0.344\n",
      "Loss after mini-batch   311: 0.254\n",
      "Loss after mini-batch   321: 0.184\n",
      "Loss after mini-batch   331: 2.993\n",
      "Loss after mini-batch   341: 0.155\n",
      "Loss after mini-batch   351: 5.758\n",
      "Loss after mini-batch   361: 0.298\n",
      "Loss after mini-batch   371: 3.471\n",
      "Loss after mini-batch   381: 0.118\n",
      "Loss after mini-batch   391: 0.135\n",
      "Loss after mini-batch   401: 0.268\n",
      "Loss after mini-batch   411: 0.326\n",
      "Loss after mini-batch   421: 0.410\n",
      "Loss after mini-batch   431: 8.080\n",
      "Loss after mini-batch   441: 0.161\n",
      "Loss after mini-batch   451: 0.820\n",
      "Loss after mini-batch   461: 1.351\n",
      "Loss after mini-batch   471: 0.196\n",
      "Loss after mini-batch   481: 2.475\n",
      "Loss after mini-batch   491: 0.015\n",
      "Loss after mini-batch   501: 1.891\n",
      "Loss after mini-batch   511: 0.005\n",
      "Loss after mini-batch   521: 1.562\n",
      "Loss after mini-batch   531: 0.221\n",
      "Loss after mini-batch   541: 0.079\n",
      "Loss after mini-batch   551: 0.122\n",
      "Loss after mini-batch   561: 0.645\n",
      "Loss after mini-batch   571: 1.339\n",
      "Loss after mini-batch   581: 0.002\n",
      "Loss after mini-batch   591: 0.163\n",
      "Loss after mini-batch   601: 0.731\n",
      "Loss after mini-batch   611: 0.104\n",
      "Loss after mini-batch   621: 0.033\n",
      "Loss after mini-batch   631: 0.177\n",
      "Loss after mini-batch   641: 0.037\n",
      "Loss after mini-batch   651: 1.098\n",
      "Loss after mini-batch   661: 0.217\n",
      "Loss after mini-batch   671: 1.016\n",
      "Loss after mini-batch   681: 5.508\n",
      "Loss after mini-batch   691: 0.455\n",
      "Loss after mini-batch   701: 0.032\n",
      "Loss after mini-batch   711: 0.213\n",
      "Loss after mini-batch   721: 0.136\n",
      "Loss after mini-batch   731: 0.672\n",
      "Loss after mini-batch   741: 3.907\n",
      "Training Loss: 0.228 \t\t Validation Loss:0.273\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 0.246\n",
      "Loss after mini-batch    11: 1.169\n",
      "Loss after mini-batch    21: 5.189\n",
      "Loss after mini-batch    31: 0.272\n",
      "Loss after mini-batch    41: 0.637\n",
      "Loss after mini-batch    51: 1.629\n",
      "Loss after mini-batch    61: 0.441\n",
      "Loss after mini-batch    71: 0.800\n",
      "Loss after mini-batch    81: 4.478\n",
      "Loss after mini-batch    91: 0.021\n",
      "Loss after mini-batch   101: 0.196\n",
      "Loss after mini-batch   111: 0.841\n",
      "Loss after mini-batch   121: 20.560\n",
      "Loss after mini-batch   131: 1.814\n",
      "Loss after mini-batch   141: 12.971\n",
      "Loss after mini-batch   151: 0.344\n",
      "Loss after mini-batch   161: 0.544\n",
      "Loss after mini-batch   171: 0.045\n",
      "Loss after mini-batch   181: 1.137\n",
      "Loss after mini-batch   191: 3.998\n",
      "Loss after mini-batch   201: 0.645\n",
      "Loss after mini-batch   211: 0.409\n",
      "Loss after mini-batch   221: 0.764\n",
      "Loss after mini-batch   231: 6.394\n",
      "Loss after mini-batch   241: 27.884\n",
      "Loss after mini-batch   251: 1.862\n",
      "Loss after mini-batch   261: 0.031\n",
      "Loss after mini-batch   271: 0.030\n",
      "Loss after mini-batch   281: 0.161\n",
      "Loss after mini-batch   291: 0.236\n",
      "Loss after mini-batch   301: 0.134\n",
      "Loss after mini-batch   311: 9.731\n",
      "Loss after mini-batch   321: 0.006\n",
      "Loss after mini-batch   331: 1.093\n",
      "Loss after mini-batch   341: 0.108\n",
      "Loss after mini-batch   351: 0.049\n",
      "Loss after mini-batch   361: 0.575\n",
      "Loss after mini-batch   371: 0.247\n",
      "Loss after mini-batch   381: 0.028\n",
      "Loss after mini-batch   391: 1.110\n",
      "Loss after mini-batch   401: 18.737\n",
      "Loss after mini-batch   411: 0.150\n",
      "Loss after mini-batch   421: 2.423\n",
      "Loss after mini-batch   431: 1.412\n",
      "Loss after mini-batch   441: 0.017\n",
      "Loss after mini-batch   451: 16.525\n",
      "Loss after mini-batch   461: 0.020\n",
      "Loss after mini-batch   471: 0.831\n",
      "Loss after mini-batch   481: 20.032\n",
      "Loss after mini-batch   491: 0.341\n",
      "Loss after mini-batch   501: 13.123\n",
      "Loss after mini-batch   511: 0.408\n",
      "Loss after mini-batch   521: 15.958\n",
      "Loss after mini-batch   531: 0.021\n",
      "Loss after mini-batch   541: 0.013\n",
      "Loss after mini-batch   551: 13.599\n",
      "Loss after mini-batch   561: 0.000\n",
      "Loss after mini-batch   571: 0.654\n",
      "Loss after mini-batch   581: 0.096\n",
      "Loss after mini-batch   591: 4.078\n",
      "Loss after mini-batch   601: 0.054\n",
      "Loss after mini-batch   611: 44.947\n",
      "Loss after mini-batch   621: 19.017\n",
      "Loss after mini-batch   631: 0.012\n",
      "Loss after mini-batch   641: 16.544\n",
      "Loss after mini-batch   651: 22.805\n",
      "Loss after mini-batch   661: 52.481\n",
      "Loss after mini-batch   671: 2.592\n",
      "Loss after mini-batch   681: 0.062\n",
      "Loss after mini-batch   691: 1.093\n",
      "Loss after mini-batch   701: 0.469\n",
      "Loss after mini-batch   711: 3.422\n",
      "Loss after mini-batch   721: 0.284\n",
      "Loss after mini-batch   731: 0.318\n",
      "Loss after mini-batch   741: 2.375\n",
      "Training Loss: 0.770 \t\t Validation Loss:6.213\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 12.470\n",
      "Loss after mini-batch    11: 52.500\n",
      "Loss after mini-batch    21: 17.830\n",
      "Loss after mini-batch    31: 0.054\n",
      "Loss after mini-batch    41: 1.329\n",
      "Loss after mini-batch    51: 2.410\n",
      "Loss after mini-batch    61: 0.794\n",
      "Loss after mini-batch    71: 0.268\n",
      "Loss after mini-batch    81: 5.211\n",
      "Loss after mini-batch    91: 5.802\n",
      "Loss after mini-batch   101: 1.558\n",
      "Loss after mini-batch   111: 26.755\n",
      "Loss after mini-batch   121: 0.015\n",
      "Loss after mini-batch   131: 2.855\n",
      "Loss after mini-batch   141: 0.312\n",
      "Loss after mini-batch   151: 0.085\n",
      "Loss after mini-batch   161: 13.208\n",
      "Loss after mini-batch   171: 2.467\n",
      "Loss after mini-batch   181: 20.256\n",
      "Loss after mini-batch   191: 0.148\n",
      "Loss after mini-batch   201: 33.565\n",
      "Loss after mini-batch   211: 0.341\n",
      "Loss after mini-batch   221: 1.106\n",
      "Loss after mini-batch   231: 0.114\n",
      "Loss after mini-batch   241: 0.215\n",
      "Loss after mini-batch   251: 7.812\n",
      "Loss after mini-batch   261: 0.268\n",
      "Loss after mini-batch   271: 3.885\n",
      "Loss after mini-batch   281: 5.062\n",
      "Loss after mini-batch   291: 1.036\n",
      "Loss after mini-batch   301: 0.130\n",
      "Loss after mini-batch   311: 0.149\n",
      "Loss after mini-batch   321: 0.483\n",
      "Loss after mini-batch   331: 0.379\n",
      "Loss after mini-batch   341: 0.184\n",
      "Loss after mini-batch   351: 36.228\n",
      "Loss after mini-batch   361: 27.145\n",
      "Loss after mini-batch   371: 2.977\n",
      "Loss after mini-batch   381: 1.185\n",
      "Loss after mini-batch   391: 7.776\n",
      "Loss after mini-batch   401: 0.017\n",
      "Loss after mini-batch   411: 0.309\n",
      "Loss after mini-batch   421: 0.945\n",
      "Loss after mini-batch   431: 0.122\n",
      "Loss after mini-batch   441: 15.153\n",
      "Loss after mini-batch   451: 0.055\n",
      "Loss after mini-batch   461: 0.501\n",
      "Loss after mini-batch   471: 0.581\n",
      "Loss after mini-batch   481: 0.525\n",
      "Loss after mini-batch   491: 18.563\n",
      "Loss after mini-batch   501: 2.682\n",
      "Loss after mini-batch   511: 0.168\n",
      "Loss after mini-batch   521: 0.442\n",
      "Loss after mini-batch   531: 1.670\n",
      "Loss after mini-batch   541: 0.270\n",
      "Loss after mini-batch   551: 0.030\n",
      "Loss after mini-batch   561: 0.047\n",
      "Loss after mini-batch   571: 9.189\n",
      "Loss after mini-batch   581: 0.018\n",
      "Loss after mini-batch   591: 0.094\n",
      "Loss after mini-batch   601: 4.088\n",
      "Loss after mini-batch   611: 0.685\n",
      "Loss after mini-batch   621: 0.126\n",
      "Loss after mini-batch   631: 4.106\n",
      "Loss after mini-batch   641: 0.124\n",
      "Loss after mini-batch   651: 2.133\n",
      "Loss after mini-batch   661: 0.289\n",
      "Loss after mini-batch   671: 44.817\n",
      "Loss after mini-batch   681: 0.110\n",
      "Loss after mini-batch   691: 0.063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   701: 0.032\n",
      "Loss after mini-batch   711: 0.648\n",
      "Loss after mini-batch   721: 12.656\n",
      "Loss after mini-batch   731: 0.285\n",
      "Loss after mini-batch   741: 1.285\n",
      "Training Loss: 0.006 \t\t Validation Loss:4.260\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 0.030\n",
      "Loss after mini-batch    11: 0.548\n",
      "Loss after mini-batch    21: 0.036\n",
      "Loss after mini-batch    31: 0.400\n",
      "Loss after mini-batch    41: 0.129\n",
      "Loss after mini-batch    51: 0.372\n",
      "Loss after mini-batch    61: 1.811\n",
      "Loss after mini-batch    71: 5.607\n",
      "Loss after mini-batch    81: 0.561\n",
      "Loss after mini-batch    91: 0.024\n",
      "Loss after mini-batch   101: 0.379\n",
      "Loss after mini-batch   111: 0.208\n",
      "Loss after mini-batch   121: 36.485\n",
      "Loss after mini-batch   131: 0.026\n",
      "Loss after mini-batch   141: 0.136\n",
      "Loss after mini-batch   151: 2.365\n",
      "Loss after mini-batch   161: 0.287\n",
      "Loss after mini-batch   171: 0.101\n",
      "Loss after mini-batch   181: 5.151\n",
      "Loss after mini-batch   191: 2.959\n",
      "Loss after mini-batch   201: 0.185\n",
      "Loss after mini-batch   211: 1.172\n",
      "Loss after mini-batch   221: 0.616\n",
      "Loss after mini-batch   231: 0.053\n",
      "Loss after mini-batch   241: 0.486\n",
      "Loss after mini-batch   251: 0.911\n",
      "Loss after mini-batch   261: 0.196\n",
      "Loss after mini-batch   271: 0.309\n",
      "Loss after mini-batch   281: 0.054\n",
      "Loss after mini-batch   291: 0.093\n",
      "Loss after mini-batch   301: 0.370\n",
      "Loss after mini-batch   311: 0.057\n",
      "Loss after mini-batch   321: 0.039\n",
      "Loss after mini-batch   331: 0.087\n",
      "Loss after mini-batch   341: 2.909\n",
      "Loss after mini-batch   351: 1.653\n",
      "Loss after mini-batch   361: 1.224\n",
      "Loss after mini-batch   371: 0.567\n",
      "Loss after mini-batch   381: 0.185\n",
      "Loss after mini-batch   391: 0.023\n",
      "Loss after mini-batch   401: 1.513\n",
      "Loss after mini-batch   411: 7.117\n",
      "Loss after mini-batch   421: 0.267\n",
      "Loss after mini-batch   431: 0.260\n",
      "Loss after mini-batch   441: 0.015\n",
      "Loss after mini-batch   451: 26.969\n",
      "Loss after mini-batch   461: 0.020\n",
      "Loss after mini-batch   471: 12.402\n",
      "Loss after mini-batch   481: 8.807\n",
      "Loss after mini-batch   491: 1.236\n",
      "Loss after mini-batch   501: 9.989\n",
      "Loss after mini-batch   511: 0.123\n",
      "Loss after mini-batch   521: 0.031\n",
      "Loss after mini-batch   531: 0.066\n",
      "Loss after mini-batch   541: 10.913\n",
      "Loss after mini-batch   551: 0.194\n",
      "Loss after mini-batch   561: 0.209\n",
      "Loss after mini-batch   571: 0.010\n",
      "Loss after mini-batch   581: 0.065\n",
      "Loss after mini-batch   591: 0.378\n",
      "Loss after mini-batch   601: 0.241\n",
      "Loss after mini-batch   611: 0.173\n",
      "Loss after mini-batch   621: 1.941\n",
      "Loss after mini-batch   631: 0.357\n",
      "Loss after mini-batch   641: 1.484\n",
      "Loss after mini-batch   651: 0.068\n",
      "Loss after mini-batch   661: 0.027\n",
      "Loss after mini-batch   671: 0.013\n",
      "Loss after mini-batch   681: 0.192\n",
      "Loss after mini-batch   691: 3.467\n",
      "Loss after mini-batch   701: 2.720\n",
      "Loss after mini-batch   711: 1.778\n",
      "Loss after mini-batch   721: 2.548\n",
      "Loss after mini-batch   731: 8.467\n",
      "Loss after mini-batch   741: 0.254\n",
      "Training Loss: 0.406 \t\t Validation Loss:0.467\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 1.091\n",
      "Loss after mini-batch    11: 0.959\n",
      "Loss after mini-batch    21: 0.354\n",
      "Loss after mini-batch    31: 0.566\n",
      "Loss after mini-batch    41: 21.414\n",
      "Loss after mini-batch    51: 0.172\n",
      "Loss after mini-batch    61: 0.162\n",
      "Loss after mini-batch    71: 1.363\n",
      "Loss after mini-batch    81: 1.819\n",
      "Loss after mini-batch    91: 0.052\n",
      "Loss after mini-batch   101: 2.912\n",
      "Loss after mini-batch   111: 0.111\n",
      "Loss after mini-batch   121: 2.734\n",
      "Loss after mini-batch   131: 0.278\n",
      "Loss after mini-batch   141: 0.607\n",
      "Loss after mini-batch   151: 0.233\n",
      "Loss after mini-batch   161: 0.631\n",
      "Loss after mini-batch   171: 0.328\n",
      "Loss after mini-batch   181: 0.776\n",
      "Loss after mini-batch   191: 0.014\n",
      "Loss after mini-batch   201: 0.109\n",
      "Loss after mini-batch   211: 0.298\n",
      "Loss after mini-batch   221: 10.590\n",
      "Loss after mini-batch   231: 0.652\n",
      "Loss after mini-batch   241: 43.133\n",
      "Loss after mini-batch   251: 1.711\n",
      "Loss after mini-batch   261: 0.498\n",
      "Loss after mini-batch   271: 5.910\n",
      "Loss after mini-batch   281: 1.345\n",
      "Loss after mini-batch   291: 0.267\n",
      "Loss after mini-batch   301: 2.021\n",
      "Loss after mini-batch   311: 0.104\n",
      "Loss after mini-batch   321: 0.077\n",
      "Loss after mini-batch   331: 8.403\n",
      "Loss after mini-batch   341: 0.805\n",
      "Loss after mini-batch   351: 0.391\n",
      "Loss after mini-batch   361: 10.154\n",
      "Loss after mini-batch   371: 1.253\n",
      "Loss after mini-batch   381: 7.430\n",
      "Loss after mini-batch   391: 4.924\n",
      "Loss after mini-batch   401: 43.455\n",
      "Loss after mini-batch   411: 0.306\n",
      "Loss after mini-batch   421: 11.638\n",
      "Loss after mini-batch   431: 0.046\n",
      "Loss after mini-batch   441: 0.081\n",
      "Loss after mini-batch   451: 1.559\n",
      "Loss after mini-batch   461: 22.073\n",
      "Loss after mini-batch   471: 1.089\n",
      "Loss after mini-batch   481: 0.639\n",
      "Loss after mini-batch   491: 1.673\n",
      "Loss after mini-batch   501: 0.519\n",
      "Loss after mini-batch   511: 0.113\n",
      "Loss after mini-batch   521: 5.184\n",
      "Loss after mini-batch   531: 0.716\n",
      "Loss after mini-batch   541: 0.020\n",
      "Loss after mini-batch   551: 3.358\n",
      "Loss after mini-batch   561: 3.659\n",
      "Loss after mini-batch   571: 2.760\n",
      "Loss after mini-batch   581: 0.484\n",
      "Loss after mini-batch   591: 0.072\n",
      "Loss after mini-batch   601: 0.939\n",
      "Loss after mini-batch   611: 0.137\n",
      "Loss after mini-batch   621: 12.425\n",
      "Loss after mini-batch   631: 0.465\n",
      "Loss after mini-batch   641: 17.391\n",
      "Loss after mini-batch   651: 0.045\n",
      "Loss after mini-batch   661: 0.457\n",
      "Loss after mini-batch   671: 0.091\n",
      "Loss after mini-batch   681: 0.832\n",
      "Loss after mini-batch   691: 2.002\n",
      "Loss after mini-batch   701: 62.962\n",
      "Loss after mini-batch   711: 0.006\n",
      "Loss after mini-batch   721: 0.148\n",
      "Loss after mini-batch   731: 29.393\n",
      "Loss after mini-batch   741: 0.167\n",
      "Training Loss: 26.643 \t\t Validation Loss:47.384\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 12.749\n",
      "Loss after mini-batch    11: 5.805\n",
      "Loss after mini-batch    21: 0.178\n",
      "Loss after mini-batch    31: 0.286\n",
      "Loss after mini-batch    41: 0.289\n",
      "Loss after mini-batch    51: 0.031\n",
      "Loss after mini-batch    61: 0.596\n",
      "Loss after mini-batch    71: 35.774\n",
      "Loss after mini-batch    81: 0.256\n",
      "Loss after mini-batch    91: 11.643\n",
      "Loss after mini-batch   101: 1.354\n",
      "Loss after mini-batch   111: 0.857\n",
      "Loss after mini-batch   121: 3.502\n",
      "Loss after mini-batch   131: 0.626\n",
      "Loss after mini-batch   141: 0.611\n",
      "Loss after mini-batch   151: 0.088\n",
      "Loss after mini-batch   161: 0.269\n",
      "Loss after mini-batch   171: 0.065\n",
      "Loss after mini-batch   181: 1.209\n",
      "Loss after mini-batch   191: 0.218\n",
      "Loss after mini-batch   201: 3.353\n",
      "Loss after mini-batch   211: 1.810\n",
      "Loss after mini-batch   221: 19.734\n",
      "Loss after mini-batch   231: 0.205\n",
      "Loss after mini-batch   241: 0.209\n",
      "Loss after mini-batch   251: 0.076\n",
      "Loss after mini-batch   261: 32.990\n",
      "Loss after mini-batch   271: 3.069\n",
      "Loss after mini-batch   281: 0.117\n",
      "Loss after mini-batch   291: 12.098\n",
      "Loss after mini-batch   301: 18.483\n",
      "Loss after mini-batch   311: 0.216\n",
      "Loss after mini-batch   321: 0.177\n",
      "Loss after mini-batch   331: 5.913\n",
      "Loss after mini-batch   341: 2.871\n",
      "Loss after mini-batch   351: 0.303\n",
      "Loss after mini-batch   361: 1.916\n",
      "Loss after mini-batch   371: 1.080\n",
      "Loss after mini-batch   381: 0.215\n",
      "Loss after mini-batch   391: 0.037\n",
      "Loss after mini-batch   401: 0.181\n",
      "Loss after mini-batch   411: 0.416\n",
      "Loss after mini-batch   421: 4.400\n",
      "Loss after mini-batch   431: 0.171\n",
      "Loss after mini-batch   441: 17.800\n",
      "Loss after mini-batch   451: 0.336\n",
      "Loss after mini-batch   461: 0.103\n",
      "Loss after mini-batch   471: 2.796\n",
      "Loss after mini-batch   481: 2.895\n",
      "Loss after mini-batch   491: 0.536\n",
      "Loss after mini-batch   501: 0.047\n",
      "Loss after mini-batch   511: 0.442\n",
      "Loss after mini-batch   521: 0.824\n",
      "Loss after mini-batch   531: 0.123\n",
      "Loss after mini-batch   541: 0.942\n",
      "Loss after mini-batch   551: 0.199\n",
      "Loss after mini-batch   561: 0.430\n",
      "Loss after mini-batch   571: 0.266\n",
      "Loss after mini-batch   581: 7.357\n",
      "Loss after mini-batch   591: 51.004\n",
      "Loss after mini-batch   601: 0.069\n",
      "Loss after mini-batch   611: 0.141\n",
      "Loss after mini-batch   621: 12.397\n",
      "Loss after mini-batch   631: 2.866\n",
      "Loss after mini-batch   641: 31.309\n",
      "Loss after mini-batch   651: 0.258\n",
      "Loss after mini-batch   661: 1.623\n",
      "Loss after mini-batch   671: 46.900\n",
      "Loss after mini-batch   681: 2.207\n",
      "Loss after mini-batch   691: 1.839\n",
      "Loss after mini-batch   701: 53.169\n",
      "Loss after mini-batch   711: 0.090\n",
      "Loss after mini-batch   721: 0.011\n",
      "Loss after mini-batch   731: 2.760\n",
      "Loss after mini-batch   741: 0.342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.148 \t\t Validation Loss:0.217\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 0.779\n",
      "Loss after mini-batch    11: 36.346\n",
      "Loss after mini-batch    21: 0.054\n",
      "Loss after mini-batch    31: 1.129\n",
      "Loss after mini-batch    41: 0.078\n",
      "Loss after mini-batch    51: 0.955\n",
      "Loss after mini-batch    61: 0.265\n",
      "Loss after mini-batch    71: 1.007\n",
      "Loss after mini-batch    81: 32.748\n",
      "Loss after mini-batch    91: 2.293\n",
      "Loss after mini-batch   101: 0.203\n",
      "Loss after mini-batch   111: 1.574\n",
      "Loss after mini-batch   121: 0.062\n",
      "Loss after mini-batch   131: 0.117\n",
      "Loss after mini-batch   141: 1.118\n",
      "Loss after mini-batch   151: 2.146\n",
      "Loss after mini-batch   161: 0.030\n",
      "Loss after mini-batch   171: 0.159\n",
      "Loss after mini-batch   181: 3.109\n",
      "Loss after mini-batch   191: 0.044\n",
      "Loss after mini-batch   201: 6.813\n",
      "Loss after mini-batch   211: 25.889\n",
      "Loss after mini-batch   221: 23.024\n",
      "Loss after mini-batch   231: 42.166\n",
      "Loss after mini-batch   241: 2.791\n",
      "Loss after mini-batch   251: 0.363\n",
      "Loss after mini-batch   261: 12.690\n",
      "Loss after mini-batch   271: 0.110\n",
      "Loss after mini-batch   281: 3.410\n",
      "Loss after mini-batch   291: 0.825\n",
      "Loss after mini-batch   301: 14.958\n",
      "Loss after mini-batch   311: 0.493\n",
      "Loss after mini-batch   321: 27.361\n",
      "Loss after mini-batch   331: 2.034\n",
      "Loss after mini-batch   341: 0.216\n",
      "Loss after mini-batch   351: 0.637\n",
      "Loss after mini-batch   361: 2.187\n",
      "Loss after mini-batch   371: 0.596\n",
      "Loss after mini-batch   381: 0.255\n",
      "Loss after mini-batch   391: 1.795\n",
      "Loss after mini-batch   401: 5.653\n",
      "Loss after mini-batch   411: 7.427\n",
      "Loss after mini-batch   421: 0.157\n",
      "Loss after mini-batch   431: 11.080\n",
      "Loss after mini-batch   441: 3.404\n",
      "Loss after mini-batch   451: 0.638\n",
      "Loss after mini-batch   461: 0.060\n",
      "Loss after mini-batch   471: 0.382\n",
      "Loss after mini-batch   481: 1.239\n",
      "Loss after mini-batch   491: 0.755\n",
      "Loss after mini-batch   501: 0.099\n",
      "Loss after mini-batch   511: 0.054\n",
      "Loss after mini-batch   521: 0.117\n",
      "Loss after mini-batch   531: 0.164\n",
      "Loss after mini-batch   541: 2.574\n",
      "Loss after mini-batch   551: 0.292\n",
      "Loss after mini-batch   561: 28.451\n",
      "Loss after mini-batch   571: 0.137\n",
      "Loss after mini-batch   581: 0.770\n",
      "Loss after mini-batch   591: 0.410\n",
      "Loss after mini-batch   601: 1.823\n",
      "Loss after mini-batch   611: 21.431\n",
      "Loss after mini-batch   621: 0.079\n",
      "Loss after mini-batch   631: 26.203\n",
      "Loss after mini-batch   641: 0.975\n",
      "Loss after mini-batch   651: 66.394\n",
      "Loss after mini-batch   661: 0.078\n",
      "Loss after mini-batch   671: 0.221\n",
      "Loss after mini-batch   681: 1.071\n",
      "Loss after mini-batch   691: 0.126\n",
      "Loss after mini-batch   701: 0.224\n",
      "Loss after mini-batch   711: 3.965\n",
      "Loss after mini-batch   721: 0.296\n",
      "Loss after mini-batch   731: 0.035\n",
      "Loss after mini-batch   741: 0.396\n",
      "Training Loss: 3.943 \t\t Validation Loss:8.409\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 0.178\n",
      "Loss after mini-batch    11: 0.009\n",
      "Loss after mini-batch    21: 0.338\n",
      "Loss after mini-batch    31: 0.105\n",
      "Loss after mini-batch    41: 1.160\n",
      "Loss after mini-batch    51: 0.236\n",
      "Loss after mini-batch    61: 8.000\n",
      "Loss after mini-batch    71: 2.126\n",
      "Loss after mini-batch    81: 3.770\n",
      "Loss after mini-batch    91: 0.048\n",
      "Loss after mini-batch   101: 2.390\n",
      "Loss after mini-batch   111: 1.694\n",
      "Loss after mini-batch   121: 2.341\n",
      "Loss after mini-batch   131: 0.074\n",
      "Loss after mini-batch   141: 20.826\n",
      "Loss after mini-batch   151: 4.609\n",
      "Loss after mini-batch   161: 0.731\n",
      "Loss after mini-batch   171: 0.093\n",
      "Loss after mini-batch   181: 54.605\n",
      "Loss after mini-batch   191: 7.330\n",
      "Loss after mini-batch   201: 0.459\n",
      "Loss after mini-batch   211: 0.470\n",
      "Loss after mini-batch   221: 3.660\n",
      "Loss after mini-batch   231: 0.177\n",
      "Loss after mini-batch   241: 0.038\n",
      "Loss after mini-batch   251: 3.100\n",
      "Loss after mini-batch   261: 25.178\n",
      "Loss after mini-batch   271: 8.202\n",
      "Loss after mini-batch   281: 0.883\n",
      "Loss after mini-batch   291: 0.056\n",
      "Loss after mini-batch   301: 0.216\n",
      "Loss after mini-batch   311: 0.372\n",
      "Loss after mini-batch   321: 1.358\n",
      "Loss after mini-batch   331: 1.300\n",
      "Loss after mini-batch   341: 0.127\n",
      "Loss after mini-batch   351: 24.146\n",
      "Loss after mini-batch   361: 1.850\n",
      "Loss after mini-batch   371: 3.964\n",
      "Loss after mini-batch   381: 0.127\n",
      "Loss after mini-batch   391: 0.016\n",
      "Loss after mini-batch   401: 0.045\n",
      "Loss after mini-batch   411: 24.533\n",
      "Loss after mini-batch   421: 0.083\n",
      "Loss after mini-batch   431: 1.841\n",
      "Loss after mini-batch   441: 1.487\n",
      "Loss after mini-batch   451: 0.059\n",
      "Loss after mini-batch   461: 0.565\n",
      "Loss after mini-batch   471: 0.071\n",
      "Loss after mini-batch   481: 11.578\n",
      "Loss after mini-batch   491: 1.215\n",
      "Loss after mini-batch   501: 0.097\n",
      "Loss after mini-batch   511: 0.078\n",
      "Loss after mini-batch   521: 6.907\n",
      "Loss after mini-batch   531: 0.246\n",
      "Loss after mini-batch   541: 10.165\n",
      "Loss after mini-batch   551: 8.678\n",
      "Loss after mini-batch   561: 0.449\n",
      "Loss after mini-batch   571: 0.208\n",
      "Loss after mini-batch   581: 17.405\n",
      "Loss after mini-batch   591: 8.674\n",
      "Loss after mini-batch   601: 0.365\n",
      "Loss after mini-batch   611: 8.722\n",
      "Loss after mini-batch   621: 11.338\n",
      "Loss after mini-batch   631: 0.440\n",
      "Loss after mini-batch   641: 0.038\n",
      "Loss after mini-batch   651: 5.771\n",
      "Loss after mini-batch   661: 3.208\n",
      "Loss after mini-batch   671: 0.596\n",
      "Loss after mini-batch   681: 0.209\n",
      "Loss after mini-batch   691: 0.103\n",
      "Loss after mini-batch   701: 35.466\n",
      "Loss after mini-batch   711: 0.194\n",
      "Loss after mini-batch   721: 5.380\n",
      "Loss after mini-batch   731: 1.419\n",
      "Loss after mini-batch   741: 0.062\n",
      "Training Loss: 3.057 \t\t Validation Loss:4.004\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 10.011\n",
      "Loss after mini-batch    11: 1.417\n",
      "Loss after mini-batch    21: 1.609\n",
      "Loss after mini-batch    31: 40.540\n",
      "Loss after mini-batch    41: 0.092\n",
      "Loss after mini-batch    51: 1.576\n",
      "Loss after mini-batch    61: 0.068\n",
      "Loss after mini-batch    71: 0.713\n",
      "Loss after mini-batch    81: 0.608\n",
      "Loss after mini-batch    91: 0.058\n",
      "Loss after mini-batch   101: 0.321\n",
      "Loss after mini-batch   111: 0.078\n",
      "Loss after mini-batch   121: 0.116\n",
      "Loss after mini-batch   131: 1.785\n",
      "Loss after mini-batch   141: 31.812\n",
      "Loss after mini-batch   151: 8.173\n",
      "Loss after mini-batch   161: 12.112\n",
      "Loss after mini-batch   171: 23.766\n",
      "Loss after mini-batch   181: 0.172\n",
      "Loss after mini-batch   191: 11.251\n",
      "Loss after mini-batch   201: 0.263\n",
      "Loss after mini-batch   211: 0.129\n",
      "Loss after mini-batch   221: 0.208\n",
      "Loss after mini-batch   231: 4.422\n",
      "Loss after mini-batch   241: 9.350\n",
      "Loss after mini-batch   251: 0.066\n",
      "Loss after mini-batch   261: 2.893\n",
      "Loss after mini-batch   271: 0.053\n",
      "Loss after mini-batch   281: 2.374\n",
      "Loss after mini-batch   291: 3.581\n",
      "Loss after mini-batch   301: 0.310\n",
      "Loss after mini-batch   311: 0.011\n",
      "Loss after mini-batch   321: 0.110\n",
      "Loss after mini-batch   331: 9.948\n",
      "Loss after mini-batch   341: 1.302\n",
      "Loss after mini-batch   351: 0.248\n",
      "Loss after mini-batch   361: 0.217\n",
      "Loss after mini-batch   371: 23.763\n",
      "Loss after mini-batch   381: 1.208\n",
      "Loss after mini-batch   391: 7.268\n",
      "Loss after mini-batch   401: 0.189\n",
      "Loss after mini-batch   411: 0.009\n",
      "Loss after mini-batch   421: 0.136\n",
      "Loss after mini-batch   431: 7.786\n",
      "Loss after mini-batch   441: 0.013\n",
      "Loss after mini-batch   451: 0.294\n",
      "Loss after mini-batch   461: 0.071\n",
      "Loss after mini-batch   471: 0.448\n",
      "Loss after mini-batch   481: 13.699\n",
      "Loss after mini-batch   491: 0.072\n",
      "Loss after mini-batch   501: 3.307\n",
      "Loss after mini-batch   511: 14.904\n",
      "Loss after mini-batch   521: 0.438\n",
      "Loss after mini-batch   531: 0.036\n",
      "Loss after mini-batch   541: 11.595\n",
      "Loss after mini-batch   551: 0.231\n",
      "Loss after mini-batch   561: 0.321\n",
      "Loss after mini-batch   571: 1.550\n",
      "Loss after mini-batch   581: 0.087\n",
      "Loss after mini-batch   591: 4.909\n",
      "Loss after mini-batch   601: 0.025\n",
      "Loss after mini-batch   611: 47.775\n",
      "Loss after mini-batch   621: 0.087\n",
      "Loss after mini-batch   631: 40.105\n",
      "Loss after mini-batch   641: 0.093\n",
      "Loss after mini-batch   651: 1.020\n",
      "Loss after mini-batch   661: 0.532\n",
      "Loss after mini-batch   671: 2.068\n",
      "Loss after mini-batch   681: 0.139\n",
      "Loss after mini-batch   691: 0.284\n",
      "Loss after mini-batch   701: 0.291\n",
      "Loss after mini-batch   711: 0.056\n",
      "Loss after mini-batch   721: 0.630\n",
      "Loss after mini-batch   731: 0.035\n",
      "Loss after mini-batch   741: 0.182\n",
      "Training Loss: 0.086 \t\t Validation Loss:0.504\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 0.035\n",
      "Loss after mini-batch    11: 48.489\n",
      "Loss after mini-batch    21: 5.769\n",
      "Loss after mini-batch    31: 0.129\n",
      "Loss after mini-batch    41: 0.814\n",
      "Loss after mini-batch    51: 0.489\n",
      "Loss after mini-batch    61: 5.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch    71: 0.025\n",
      "Loss after mini-batch    81: 1.666\n",
      "Loss after mini-batch    91: 3.396\n",
      "Loss after mini-batch   101: 3.629\n",
      "Loss after mini-batch   111: 1.565\n",
      "Loss after mini-batch   121: 12.187\n",
      "Loss after mini-batch   131: 2.530\n",
      "Loss after mini-batch   141: 0.190\n",
      "Loss after mini-batch   151: 4.434\n",
      "Loss after mini-batch   161: 0.832\n",
      "Loss after mini-batch   171: 0.144\n",
      "Loss after mini-batch   181: 7.094\n",
      "Loss after mini-batch   191: 0.379\n",
      "Loss after mini-batch   201: 2.651\n",
      "Loss after mini-batch   211: 0.158\n",
      "Loss after mini-batch   221: 0.190\n",
      "Loss after mini-batch   231: 0.002\n",
      "Loss after mini-batch   241: 0.003\n",
      "Loss after mini-batch   251: 0.082\n",
      "Loss after mini-batch   261: 13.846\n",
      "Loss after mini-batch   271: 15.774\n",
      "Loss after mini-batch   281: 3.220\n",
      "Loss after mini-batch   291: 0.086\n",
      "Loss after mini-batch   301: 1.350\n",
      "Loss after mini-batch   311: 4.018\n",
      "Loss after mini-batch   321: 0.960\n",
      "Loss after mini-batch   331: 17.728\n",
      "Loss after mini-batch   341: 0.575\n",
      "Loss after mini-batch   351: 2.160\n",
      "Loss after mini-batch   361: 49.141\n",
      "Loss after mini-batch   371: 0.045\n",
      "Loss after mini-batch   381: 0.260\n",
      "Loss after mini-batch   391: 2.541\n",
      "Loss after mini-batch   401: 0.555\n",
      "Loss after mini-batch   411: 46.930\n",
      "Loss after mini-batch   421: 0.018\n",
      "Loss after mini-batch   431: 25.896\n",
      "Loss after mini-batch   441: 3.815\n",
      "Loss after mini-batch   451: 0.250\n",
      "Loss after mini-batch   461: 0.104\n",
      "Loss after mini-batch   471: 2.157\n",
      "Loss after mini-batch   481: 0.332\n",
      "Loss after mini-batch   491: 0.332\n",
      "Loss after mini-batch   501: 3.690\n",
      "Loss after mini-batch   511: 0.374\n",
      "Loss after mini-batch   521: 0.037\n",
      "Loss after mini-batch   531: 1.566\n",
      "Loss after mini-batch   541: 0.020\n",
      "Loss after mini-batch   551: 1.226\n",
      "Loss after mini-batch   561: 4.365\n",
      "Loss after mini-batch   571: 0.131\n",
      "Loss after mini-batch   581: 0.310\n",
      "Loss after mini-batch   591: 23.893\n",
      "Loss after mini-batch   601: 0.231\n",
      "Loss after mini-batch   611: 0.043\n",
      "Loss after mini-batch   621: 2.065\n",
      "Loss after mini-batch   631: 0.211\n",
      "Loss after mini-batch   641: 1.435\n",
      "Loss after mini-batch   651: 0.824\n",
      "Loss after mini-batch   661: 0.328\n",
      "Loss after mini-batch   671: 0.210\n",
      "Loss after mini-batch   681: 0.245\n",
      "Loss after mini-batch   691: 0.097\n",
      "Loss after mini-batch   701: 0.899\n",
      "Loss after mini-batch   711: 1.321\n",
      "Loss after mini-batch   721: 0.307\n",
      "Loss after mini-batch   731: 0.989\n",
      "Loss after mini-batch   741: 0.361\n",
      "Training Loss: 7.816 \t\t Validation Loss:8.435\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 0.262\n",
      "Loss after mini-batch    11: 6.257\n",
      "Loss after mini-batch    21: 0.094\n",
      "Loss after mini-batch    31: 0.266\n",
      "Loss after mini-batch    41: 0.606\n",
      "Loss after mini-batch    51: 0.967\n",
      "Loss after mini-batch    61: 1.681\n",
      "Loss after mini-batch    71: 0.411\n",
      "Loss after mini-batch    81: 0.011\n",
      "Loss after mini-batch    91: 0.016\n",
      "Loss after mini-batch   101: 0.183\n",
      "Loss after mini-batch   111: 0.208\n",
      "Loss after mini-batch   121: 0.068\n",
      "Loss after mini-batch   131: 11.321\n",
      "Loss after mini-batch   141: 12.510\n",
      "Loss after mini-batch   151: 8.502\n",
      "Loss after mini-batch   161: 0.000\n",
      "Loss after mini-batch   171: 0.112\n",
      "Loss after mini-batch   181: 2.501\n",
      "Loss after mini-batch   191: 0.045\n",
      "Loss after mini-batch   201: 0.097\n",
      "Loss after mini-batch   211: 0.465\n",
      "Loss after mini-batch   221: 0.133\n",
      "Loss after mini-batch   231: 4.540\n",
      "Loss after mini-batch   241: 0.325\n",
      "Loss after mini-batch   251: 0.388\n",
      "Loss after mini-batch   261: 12.528\n",
      "Loss after mini-batch   271: 0.409\n",
      "Loss after mini-batch   281: 0.191\n",
      "Loss after mini-batch   291: 2.176\n",
      "Loss after mini-batch   301: 0.727\n",
      "Loss after mini-batch   311: 0.364\n",
      "Loss after mini-batch   321: 0.335\n",
      "Loss after mini-batch   331: 0.179\n",
      "Loss after mini-batch   341: 26.726\n",
      "Loss after mini-batch   351: 0.443\n",
      "Loss after mini-batch   361: 0.065\n",
      "Loss after mini-batch   371: 0.016\n",
      "Loss after mini-batch   381: 0.093\n",
      "Loss after mini-batch   391: 0.144\n",
      "Loss after mini-batch   401: 0.301\n",
      "Loss after mini-batch   411: 54.761\n",
      "Loss after mini-batch   421: 0.100\n",
      "Loss after mini-batch   431: 3.576\n",
      "Loss after mini-batch   441: 11.089\n",
      "Loss after mini-batch   451: 21.759\n",
      "Loss after mini-batch   461: 1.214\n",
      "Loss after mini-batch   471: 2.523\n",
      "Loss after mini-batch   481: 0.160\n",
      "Loss after mini-batch   491: 0.865\n",
      "Loss after mini-batch   501: 2.507\n",
      "Loss after mini-batch   511: 0.126\n",
      "Loss after mini-batch   521: 0.059\n",
      "Loss after mini-batch   531: 0.129\n",
      "Loss after mini-batch   541: 1.757\n",
      "Loss after mini-batch   551: 0.404\n",
      "Loss after mini-batch   561: 0.377\n",
      "Loss after mini-batch   571: 0.300\n",
      "Loss after mini-batch   581: 0.067\n",
      "Loss after mini-batch   591: 1.895\n",
      "Loss after mini-batch   601: 23.522\n",
      "Loss after mini-batch   611: 0.043\n",
      "Loss after mini-batch   621: 0.218\n",
      "Loss after mini-batch   631: 3.344\n",
      "Loss after mini-batch   641: 7.788\n",
      "Loss after mini-batch   651: 0.486\n",
      "Loss after mini-batch   661: 0.232\n",
      "Loss after mini-batch   671: 1.664\n",
      "Loss after mini-batch   681: 0.050\n",
      "Loss after mini-batch   691: 4.924\n",
      "Loss after mini-batch   701: 0.340\n",
      "Loss after mini-batch   711: 0.093\n",
      "Loss after mini-batch   721: 0.204\n",
      "Loss after mini-batch   731: 2.298\n",
      "Loss after mini-batch   741: 0.088\n",
      "Training Loss: 5.549 \t\t Validation Loss:6.125\n",
      "Starting epoch 46\n",
      "Loss after mini-batch     1: 0.281\n",
      "Loss after mini-batch    11: 0.070\n",
      "Loss after mini-batch    21: 11.922\n",
      "Loss after mini-batch    31: 0.805\n",
      "Loss after mini-batch    41: 1.639\n",
      "Loss after mini-batch    51: 0.257\n",
      "Loss after mini-batch    61: 11.696\n",
      "Loss after mini-batch    71: 0.139\n",
      "Loss after mini-batch    81: 26.603\n",
      "Loss after mini-batch    91: 0.125\n",
      "Loss after mini-batch   101: 0.330\n",
      "Loss after mini-batch   111: 8.377\n",
      "Loss after mini-batch   121: 0.125\n",
      "Loss after mini-batch   131: 0.114\n",
      "Loss after mini-batch   141: 0.159\n",
      "Loss after mini-batch   151: 0.422\n",
      "Loss after mini-batch   161: 0.465\n",
      "Loss after mini-batch   171: 12.257\n",
      "Loss after mini-batch   181: 0.394\n",
      "Loss after mini-batch   191: 0.092\n",
      "Loss after mini-batch   201: 10.328\n",
      "Loss after mini-batch   211: 0.466\n",
      "Loss after mini-batch   221: 0.846\n",
      "Loss after mini-batch   231: 53.747\n",
      "Loss after mini-batch   241: 0.441\n",
      "Loss after mini-batch   251: 0.097\n",
      "Loss after mini-batch   261: 3.327\n",
      "Loss after mini-batch   271: 0.673\n",
      "Loss after mini-batch   281: 0.415\n",
      "Loss after mini-batch   291: 0.190\n",
      "Loss after mini-batch   301: 19.552\n",
      "Loss after mini-batch   311: 0.632\n",
      "Loss after mini-batch   321: 12.197\n",
      "Loss after mini-batch   331: 0.261\n",
      "Loss after mini-batch   341: 0.052\n",
      "Loss after mini-batch   351: 0.499\n",
      "Loss after mini-batch   361: 6.654\n",
      "Loss after mini-batch   371: 6.510\n",
      "Loss after mini-batch   381: 0.016\n",
      "Loss after mini-batch   391: 0.281\n",
      "Loss after mini-batch   401: 0.045\n",
      "Loss after mini-batch   411: 0.245\n",
      "Loss after mini-batch   421: 2.344\n",
      "Loss after mini-batch   431: 2.073\n",
      "Loss after mini-batch   441: 0.005\n",
      "Loss after mini-batch   451: 0.018\n",
      "Loss after mini-batch   461: 0.123\n",
      "Loss after mini-batch   471: 6.986\n",
      "Loss after mini-batch   481: 0.016\n",
      "Loss after mini-batch   491: 3.423\n",
      "Loss after mini-batch   501: 0.054\n",
      "Loss after mini-batch   511: 2.802\n",
      "Loss after mini-batch   521: 0.709\n",
      "Loss after mini-batch   531: 2.100\n",
      "Loss after mini-batch   541: 0.078\n",
      "Loss after mini-batch   551: 0.699\n",
      "Loss after mini-batch   561: 0.322\n",
      "Loss after mini-batch   571: 3.428\n",
      "Loss after mini-batch   581: 0.158\n",
      "Loss after mini-batch   591: 20.081\n",
      "Loss after mini-batch   601: 0.150\n",
      "Loss after mini-batch   611: 0.189\n",
      "Loss after mini-batch   621: 2.339\n",
      "Loss after mini-batch   631: 1.919\n",
      "Loss after mini-batch   641: 14.097\n",
      "Loss after mini-batch   651: 2.531\n",
      "Loss after mini-batch   661: 2.381\n",
      "Loss after mini-batch   671: 0.394\n",
      "Loss after mini-batch   681: 0.207\n",
      "Loss after mini-batch   691: 0.073\n",
      "Loss after mini-batch   701: 2.835\n",
      "Loss after mini-batch   711: 4.946\n",
      "Loss after mini-batch   721: 3.980\n",
      "Loss after mini-batch   731: 0.076\n",
      "Loss after mini-batch   741: 0.066\n",
      "Training Loss: 0.159 \t\t Validation Loss:0.325\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 0.546\n",
      "Loss after mini-batch    11: 2.470\n",
      "Loss after mini-batch    21: 0.180\n",
      "Loss after mini-batch    31: 0.113\n",
      "Loss after mini-batch    41: 0.100\n",
      "Loss after mini-batch    51: 0.015\n",
      "Loss after mini-batch    61: 7.059\n",
      "Loss after mini-batch    71: 0.164\n",
      "Loss after mini-batch    81: 2.747\n",
      "Loss after mini-batch    91: 0.188\n",
      "Loss after mini-batch   101: 17.196\n",
      "Loss after mini-batch   111: 0.293\n",
      "Loss after mini-batch   121: 2.703\n",
      "Loss after mini-batch   131: 0.899\n",
      "Loss after mini-batch   141: 0.005\n",
      "Loss after mini-batch   151: 9.743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   161: 3.873\n",
      "Loss after mini-batch   171: 0.267\n",
      "Loss after mini-batch   181: 3.301\n",
      "Loss after mini-batch   191: 30.987\n",
      "Loss after mini-batch   201: 0.014\n",
      "Loss after mini-batch   211: 0.310\n",
      "Loss after mini-batch   221: 2.046\n",
      "Loss after mini-batch   231: 0.064\n",
      "Loss after mini-batch   241: 6.126\n",
      "Loss after mini-batch   251: 0.267\n",
      "Loss after mini-batch   261: 2.767\n",
      "Loss after mini-batch   271: 0.024\n",
      "Loss after mini-batch   281: 16.247\n",
      "Loss after mini-batch   291: 1.805\n",
      "Loss after mini-batch   301: 0.229\n",
      "Loss after mini-batch   311: 1.797\n",
      "Loss after mini-batch   321: 9.080\n",
      "Loss after mini-batch   331: 0.485\n",
      "Loss after mini-batch   341: 2.028\n",
      "Loss after mini-batch   351: 0.027\n",
      "Loss after mini-batch   361: 4.152\n",
      "Loss after mini-batch   371: 0.106\n",
      "Loss after mini-batch   381: 3.495\n",
      "Loss after mini-batch   391: 1.111\n",
      "Loss after mini-batch   401: 0.958\n",
      "Loss after mini-batch   411: 0.427\n",
      "Loss after mini-batch   421: 0.035\n",
      "Loss after mini-batch   431: 13.692\n",
      "Loss after mini-batch   441: 0.170\n",
      "Loss after mini-batch   451: 5.174\n",
      "Loss after mini-batch   461: 0.031\n",
      "Loss after mini-batch   471: 0.497\n",
      "Loss after mini-batch   481: 0.379\n",
      "Loss after mini-batch   491: 0.297\n",
      "Loss after mini-batch   501: 4.590\n",
      "Loss after mini-batch   511: 3.898\n",
      "Loss after mini-batch   521: 1.301\n",
      "Loss after mini-batch   531: 0.039\n",
      "Loss after mini-batch   541: 1.126\n",
      "Loss after mini-batch   551: 11.853\n",
      "Loss after mini-batch   561: 0.886\n",
      "Loss after mini-batch   571: 0.330\n",
      "Loss after mini-batch   581: 36.822\n",
      "Loss after mini-batch   591: 18.107\n",
      "Loss after mini-batch   601: 2.216\n",
      "Loss after mini-batch   611: 0.034\n",
      "Loss after mini-batch   621: 2.966\n",
      "Loss after mini-batch   631: 0.669\n",
      "Loss after mini-batch   641: 0.031\n",
      "Loss after mini-batch   651: 7.018\n",
      "Loss after mini-batch   661: 0.114\n",
      "Loss after mini-batch   671: 0.075\n",
      "Loss after mini-batch   681: 4.202\n",
      "Loss after mini-batch   691: 2.074\n",
      "Loss after mini-batch   701: 0.229\n",
      "Loss after mini-batch   711: 7.130\n",
      "Loss after mini-batch   721: 19.992\n",
      "Loss after mini-batch   731: 0.378\n",
      "Loss after mini-batch   741: 0.056\n",
      "Training Loss: 3.997 \t\t Validation Loss:4.182\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.338\n",
      "Loss after mini-batch    11: 0.052\n",
      "Loss after mini-batch    21: 0.343\n",
      "Loss after mini-batch    31: 1.591\n",
      "Loss after mini-batch    41: 2.094\n",
      "Loss after mini-batch    51: 0.357\n",
      "Loss after mini-batch    61: 0.369\n",
      "Loss after mini-batch    71: 0.131\n",
      "Loss after mini-batch    81: 0.203\n",
      "Loss after mini-batch    91: 2.070\n",
      "Loss after mini-batch   101: 0.307\n",
      "Loss after mini-batch   111: 0.046\n",
      "Loss after mini-batch   121: 0.212\n",
      "Loss after mini-batch   131: 17.629\n",
      "Loss after mini-batch   141: 0.019\n",
      "Loss after mini-batch   151: 0.169\n",
      "Loss after mini-batch   161: 2.105\n",
      "Loss after mini-batch   171: 0.545\n",
      "Loss after mini-batch   181: 2.975\n",
      "Loss after mini-batch   191: 2.414\n",
      "Loss after mini-batch   201: 0.081\n",
      "Loss after mini-batch   211: 0.207\n",
      "Loss after mini-batch   221: 12.210\n",
      "Loss after mini-batch   231: 6.004\n",
      "Loss after mini-batch   241: 11.843\n",
      "Loss after mini-batch   251: 0.757\n",
      "Loss after mini-batch   261: 35.883\n",
      "Loss after mini-batch   271: 0.012\n",
      "Loss after mini-batch   281: 0.287\n",
      "Loss after mini-batch   291: 0.339\n",
      "Loss after mini-batch   301: 0.288\n",
      "Loss after mini-batch   311: 0.330\n",
      "Loss after mini-batch   321: 0.006\n",
      "Loss after mini-batch   331: 65.197\n",
      "Loss after mini-batch   341: 43.218\n",
      "Loss after mini-batch   351: 0.083\n",
      "Loss after mini-batch   361: 0.311\n",
      "Loss after mini-batch   371: 0.086\n",
      "Loss after mini-batch   381: 6.068\n",
      "Loss after mini-batch   391: 0.137\n",
      "Loss after mini-batch   401: 4.826\n",
      "Loss after mini-batch   411: 6.080\n",
      "Loss after mini-batch   421: 10.356\n",
      "Loss after mini-batch   431: 0.096\n",
      "Loss after mini-batch   441: 35.488\n",
      "Loss after mini-batch   451: 0.058\n",
      "Loss after mini-batch   461: 0.072\n",
      "Loss after mini-batch   471: 0.019\n",
      "Loss after mini-batch   481: 0.621\n",
      "Loss after mini-batch   491: 0.043\n",
      "Loss after mini-batch   501: 0.063\n",
      "Loss after mini-batch   511: 11.670\n",
      "Loss after mini-batch   521: 0.382\n",
      "Loss after mini-batch   531: 0.293\n",
      "Loss after mini-batch   541: 3.547\n",
      "Loss after mini-batch   551: 0.235\n",
      "Loss after mini-batch   561: 0.193\n",
      "Loss after mini-batch   571: 2.222\n",
      "Loss after mini-batch   581: 0.046\n",
      "Loss after mini-batch   591: 3.456\n",
      "Loss after mini-batch   601: 0.250\n",
      "Loss after mini-batch   611: 0.081\n",
      "Loss after mini-batch   621: 0.021\n",
      "Loss after mini-batch   631: 0.048\n",
      "Loss after mini-batch   641: 0.503\n",
      "Loss after mini-batch   651: 0.015\n",
      "Loss after mini-batch   661: 0.019\n",
      "Loss after mini-batch   671: 0.188\n",
      "Loss after mini-batch   681: 3.305\n",
      "Loss after mini-batch   691: 0.254\n",
      "Loss after mini-batch   701: 0.360\n",
      "Loss after mini-batch   711: 0.344\n",
      "Loss after mini-batch   721: 3.306\n",
      "Loss after mini-batch   731: 0.084\n",
      "Loss after mini-batch   741: 0.016\n",
      "Training Loss: 4.395 \t\t Validation Loss:12.958\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 0.143\n",
      "Loss after mini-batch    11: 3.367\n",
      "Loss after mini-batch    21: 0.055\n",
      "Loss after mini-batch    31: 3.313\n",
      "Loss after mini-batch    41: 0.069\n",
      "Loss after mini-batch    51: 0.120\n",
      "Loss after mini-batch    61: 0.022\n",
      "Loss after mini-batch    71: 0.127\n",
      "Loss after mini-batch    81: 0.120\n",
      "Loss after mini-batch    91: 3.781\n",
      "Loss after mini-batch   101: 0.170\n",
      "Loss after mini-batch   111: 1.374\n",
      "Loss after mini-batch   121: 0.485\n",
      "Loss after mini-batch   131: 0.863\n",
      "Loss after mini-batch   141: 0.597\n",
      "Loss after mini-batch   151: 0.480\n",
      "Loss after mini-batch   161: 8.499\n",
      "Loss after mini-batch   171: 35.124\n",
      "Loss after mini-batch   181: 0.163\n",
      "Loss after mini-batch   191: 20.984\n",
      "Loss after mini-batch   201: 0.870\n",
      "Loss after mini-batch   211: 10.245\n",
      "Loss after mini-batch   221: 1.785\n",
      "Loss after mini-batch   231: 3.467\n",
      "Loss after mini-batch   241: 0.516\n",
      "Loss after mini-batch   251: 2.539\n",
      "Loss after mini-batch   261: 1.582\n",
      "Loss after mini-batch   271: 25.187\n",
      "Loss after mini-batch   281: 0.392\n",
      "Loss after mini-batch   291: 0.146\n",
      "Loss after mini-batch   301: 2.566\n",
      "Loss after mini-batch   311: 0.086\n",
      "Loss after mini-batch   321: 0.207\n",
      "Loss after mini-batch   331: 0.301\n",
      "Loss after mini-batch   341: 52.641\n",
      "Loss after mini-batch   351: 0.210\n",
      "Loss after mini-batch   361: 7.159\n",
      "Loss after mini-batch   371: 17.128\n",
      "Loss after mini-batch   381: 16.592\n",
      "Loss after mini-batch   391: 0.734\n",
      "Loss after mini-batch   401: 2.078\n",
      "Loss after mini-batch   411: 7.932\n",
      "Loss after mini-batch   421: 0.739\n",
      "Loss after mini-batch   431: 0.026\n",
      "Loss after mini-batch   441: 5.456\n",
      "Loss after mini-batch   451: 0.066\n",
      "Loss after mini-batch   461: 0.038\n",
      "Loss after mini-batch   471: 0.499\n",
      "Loss after mini-batch   481: 0.014\n",
      "Loss after mini-batch   491: 0.260\n",
      "Loss after mini-batch   501: 28.394\n",
      "Loss after mini-batch   511: 0.500\n",
      "Loss after mini-batch   521: 15.545\n",
      "Loss after mini-batch   531: 7.252\n",
      "Loss after mini-batch   541: 0.016\n",
      "Loss after mini-batch   551: 2.303\n",
      "Loss after mini-batch   561: 5.284\n",
      "Loss after mini-batch   571: 3.558\n",
      "Loss after mini-batch   581: 0.094\n",
      "Loss after mini-batch   591: 0.506\n",
      "Loss after mini-batch   601: 3.518\n",
      "Loss after mini-batch   611: 0.178\n",
      "Loss after mini-batch   621: 0.333\n",
      "Loss after mini-batch   631: 0.367\n",
      "Loss after mini-batch   641: 3.145\n",
      "Loss after mini-batch   651: 0.003\n",
      "Loss after mini-batch   661: 0.231\n",
      "Loss after mini-batch   671: 0.291\n",
      "Loss after mini-batch   681: 0.112\n",
      "Loss after mini-batch   691: 0.127\n",
      "Loss after mini-batch   701: 3.234\n",
      "Loss after mini-batch   711: 15.079\n",
      "Loss after mini-batch   721: 11.125\n",
      "Loss after mini-batch   731: 1.732\n",
      "Loss after mini-batch   741: 25.603\n",
      "Training Loss: 0.192 \t\t Validation Loss:1.637\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 9.645\n",
      "Loss after mini-batch    11: 4.444\n",
      "Loss after mini-batch    21: 0.126\n",
      "Loss after mini-batch    31: 0.706\n",
      "Loss after mini-batch    41: 0.017\n",
      "Loss after mini-batch    51: 0.093\n",
      "Loss after mini-batch    61: 35.009\n",
      "Loss after mini-batch    71: 1.442\n",
      "Loss after mini-batch    81: 28.147\n",
      "Loss after mini-batch    91: 0.296\n",
      "Loss after mini-batch   101: 0.144\n",
      "Loss after mini-batch   111: 2.392\n",
      "Loss after mini-batch   121: 4.612\n",
      "Loss after mini-batch   131: 19.537\n",
      "Loss after mini-batch   141: 2.328\n",
      "Loss after mini-batch   151: 0.041\n",
      "Loss after mini-batch   161: 0.721\n",
      "Loss after mini-batch   171: 10.377\n",
      "Loss after mini-batch   181: 0.044\n",
      "Loss after mini-batch   191: 0.291\n",
      "Loss after mini-batch   201: 0.176\n",
      "Loss after mini-batch   211: 0.222\n",
      "Loss after mini-batch   221: 0.012\n",
      "Loss after mini-batch   231: 0.015\n",
      "Loss after mini-batch   241: 0.043\n",
      "Loss after mini-batch   251: 0.107\n",
      "Loss after mini-batch   261: 0.031\n",
      "Loss after mini-batch   271: 0.009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after mini-batch   281: 0.217\n",
      "Loss after mini-batch   291: 0.528\n",
      "Loss after mini-batch   301: 0.840\n",
      "Loss after mini-batch   311: 2.565\n",
      "Loss after mini-batch   321: 3.557\n",
      "Loss after mini-batch   331: 7.951\n",
      "Loss after mini-batch   341: 0.363\n",
      "Loss after mini-batch   351: 49.288\n",
      "Loss after mini-batch   361: 11.198\n",
      "Loss after mini-batch   371: 0.067\n",
      "Loss after mini-batch   381: 11.337\n",
      "Loss after mini-batch   391: 20.456\n",
      "Loss after mini-batch   401: 0.528\n",
      "Loss after mini-batch   411: 0.754\n",
      "Loss after mini-batch   421: 0.865\n",
      "Loss after mini-batch   431: 0.895\n",
      "Loss after mini-batch   441: 39.739\n",
      "Loss after mini-batch   451: 0.239\n",
      "Loss after mini-batch   461: 0.249\n",
      "Loss after mini-batch   471: 0.060\n",
      "Loss after mini-batch   481: 3.341\n",
      "Loss after mini-batch   491: 0.373\n",
      "Loss after mini-batch   501: 0.608\n",
      "Loss after mini-batch   511: 4.517\n",
      "Loss after mini-batch   521: 15.856\n",
      "Loss after mini-batch   531: 0.366\n",
      "Loss after mini-batch   541: 2.457\n",
      "Loss after mini-batch   551: 0.366\n",
      "Loss after mini-batch   561: 1.025\n",
      "Loss after mini-batch   571: 2.076\n",
      "Loss after mini-batch   581: 0.076\n",
      "Loss after mini-batch   591: 1.139\n",
      "Loss after mini-batch   601: 0.284\n",
      "Loss after mini-batch   611: 0.073\n",
      "Loss after mini-batch   621: 0.258\n",
      "Loss after mini-batch   631: 7.665\n",
      "Loss after mini-batch   641: 0.387\n",
      "Loss after mini-batch   651: 0.405\n",
      "Loss after mini-batch   661: 33.485\n",
      "Loss after mini-batch   671: 0.026\n",
      "Loss after mini-batch   681: 0.128\n",
      "Loss after mini-batch   691: 0.804\n",
      "Loss after mini-batch   701: 3.366\n",
      "Loss after mini-batch   711: 22.717\n",
      "Loss after mini-batch   721: 7.690\n",
      "Loss after mini-batch   731: 0.193\n",
      "Loss after mini-batch   741: 0.095\n",
      "Training Loss: 0.003 \t\t Validation Loss:0.500\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "history_train = np.empty((1,))\n",
    "history_val = np.empty((1,))\n",
    "nepochs=50\n",
    "for epoch in range(0, nepochs): # 5 epochs at maximum  \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "          # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "#                 (i + 1, current_loss / 500))\n",
    "                  (i + 1, loss.item()))\n",
    "            current_loss = 0.0\n",
    "    history_train = np.append(history_train, current_loss)\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    mlp.eval()     # Optional when not using Model Specific layer\n",
    "    for i, data in enumerate(validloader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        output_val = mlp(inputs)\n",
    "        valid_loss = loss_function(output_val, targets)\n",
    "    \n",
    "        valid_loss += loss.item()\n",
    "    history_val = np.append(history_val, valid_loss.item())\n",
    "    print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "         '{:.3f}'.format(loss.item(), valid_loss.item()))\n",
    "#     print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "#           '{:.3f}'.format(current_loss / len(trainloader), valid_loss / len(validloader)))\n",
    "#     if min_valid_loss > valid_loss:\n",
    "#         print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "#         min_valid_loss = valid_loss\n",
    "#         # Saving State Dict\n",
    "#         torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b9d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.random.randn(13)\n",
    "# torch usa tensores de torch y no numpy.darrays\n",
    "dtype = torch.float\n",
    "test = torch.randn((1, 3), device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3332d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c6a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.240553855895996"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27fc4848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0, 0], X_test[0]\n",
    "# xtest = [x[0] for x in X_test]\n",
    "ypred = [y[0].item() for y in y_pred]\n",
    "ytest = [y[0].item() for y in y_test]\n",
    "diff=np.array(ytest)-np.array(ypred)\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b7e8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diff,bins=100)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0,4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f558b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4e4160abe0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = np.arange(0, nepochs+1)\n",
    "plt.plot(ep, history_train, label='train', color='green')\n",
    "plt.plot(ep, history_val, label='val', color='orange')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1355f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[113.162445] [42.093952]\n",
      "[7.2725167] [7.240554]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAD4CAYAAAAD6PrjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAABJK0lEQVR4nO2dd5hV1dX/P3s602kzDEMblCIgIE0BCxFb7BpbYsHEkhgTTWIsiTHlF01MXmNezWussUXsDVuMDUQBURCV3mEYGGB6L7es3x/71qn3TruXmfV5nvuce/Zpe997zvnutdYuRkRQFEVRFICYSGdAURRFiR5UFBRFURQfKgqKoiiKDxUFRVEUxYeKgqIoiuIjLtIZ6AyDBg2SUaNGRTobiqIohxSrV68uFpHBLW07pEVh1KhRrFq1KtLZUBRFOaQwxuxubZu6jxRFURQfKgqKoiiKDxUFRVEUxcchHVNoCYfDQUFBAfX19ZHOitKFJCUlMWzYMOLj4yOdFUXp1fQ6USgoKCAtLY1Ro0ZhjIl0dpQuQEQoKSmhoKCAvLy8SGdHUXo1vc59VF9fz8CBA1UQehHGGAYOHKjWn6L0AL1OFAAVhF6I/qeK0jP0SlFQFOXQZ/duePfdSOei76Gi0MWUl5fzz3/+s9vO/+STT/KTn/wEgIceeoinn34agHnz5oXckW/Xrl1MmjQJgFWrVnHDDTcA8Pvf/5577rmny/PcXedVejf/+Adcckmkc9H36HWB5kjjFYUf//jHzba5XC5iY2O77Fo/+tGPOn2OGTNmMGPGjC7IjaJ0LfX10NAQ6Vz0PdRS6GJuu+02tm/fztSpU7n55ptZsmQJ3/rWt/je977HkUceGVRLB7jnnnv4/e9/D8D27ds57bTTmD59OscddxybNm1q81ot1cDdbjcLFizgN7/5DS6Xi5tvvpmZM2cyefJkHn744WbnWLJkCWeeeaZvfcOGDcybN4/Ro0dz//33+9LvvfdeJk2axKRJk/jf//3fdtPvuusuxo0bx0knncTmzZtD+ekUJQin036UnqVXWwo/+xl89VXXnnPqVAh49zXj7rvvZt26dXzlufCSJUv4/PPPWbduHXl5eezatavVY6+99loeeughxowZw8qVK/nxj3/MRx99FHLenE4nl156KZMmTeL222/nkUceISMjgy+++IKGhgbmzp3LKaec0mbQdtOmTSxevJiqqirGjRvHddddxzfffMMTTzzBypUrERGOPvpoTjjhBNxud6vpzz//PGvWrMHpdDJt2jSmT58ecjkUBfyiIALazqDn6NWiEC3MmjWr3fb11dXVLF++nAsvvNCX1hCm7fzDH/6Qiy66iNtvvx2A9957j2+++YaXX34ZgIqKCrZu3crYsWNbPccZZ5xBYmIiiYmJZGVlceDAAT799FPOO+88UlJSADj//PP55JNPEJEW091uN+eddx7JyckAnH322WGVQ1HAbyW43dCFXlelHXq1KLRVo+9JvC9NgLi4ONxut2/d2/be7XaTmZnpszA6wpw5c1i8eDE33XQTSUlJiAj/+Mc/OPXUU4P2a8taSUxM9H2PjY3F6XQiIi3u21o6aBNSpfN4RcHpVFHoSTSm0MWkpaVRVVXV6vbs7GwOHjxISUkJDQ0NvPXWWwCkp6eTl5fHSy+9BNgX7tdffx3Wta+66ipOP/10LrzwQpxOJ6eeeioPPvggDocDgC1btlBTUxN2mY4//nhef/11amtrqamp4bXXXuO4445rM/21116jrq6Oqqoq3nzzzbCvqSiBoqD0HL3aUogEAwcOZO7cuUyaNIlvf/vbnHHGGUHb4+Pj+e1vf8vRRx9NXl4e48eP921buHAh1113HXfeeScOh4NLLrmEKVOmhHX9X/ziF1RUVHD55ZezcOFCdu3axbRp0xARBg8ezOuvvx52maZNm8aVV17JrFmzALj66qs56qijAFpNv/jii5k6dSojR47kuOOOC/uaiqKiEBlMWy6AaGfGjBnStG3+xo0bOeKIIyKUI6U70f+2b3HuubBoERQXw8CBkc5N78IYs1pEWmyLru4jRVGiErUUIoOKgqIoUYknFKai0MOoKCiKEpWopRAZVBQURYlKVBQig4qCoihRiVcMXK7I5qOvoaKgKEpUopZCZFBR6CWMGjWK4uLiNtPnzJkDNB8Erz2uvPJK31AZV199NRs2bAAgNTW1s9luke46r3JooaIQGbTz2iFCVwy7vXz58k7n47HHHuv0ORQlFFQUIoNaCl3Mrl27OOKII7jmmmuYOHEip5xyCnV1dUDrQ2MH1sTBX1NuOuw2wLnnnsv06dOZOHEijzzySFh5a6kG/sUXX3DUUUexY8cOVq9ezQknnMD06dM59dRTKSwsbLZ/08l8br/9dqZMmcIxxxzDgQMHANi9ezfz589n8uTJzJ8/n/z8/DbTd+7cyezZs5k5cyZ33HFHWGVSei8qCpGhd1sKkRg7G9i6dSvPPfccjz76KBdddBGvvPIKl112WYeGxg4cdhvg8ccfZ8CAAdTV1TFz5ky+853vMLCD3T2XL1/OT3/6UxYtWkROTg6XXXYZixYtYvDgwbzwwgvcfvvtPP74460eX1NTwzHHHMNdd93FLbfcwqOPPspvfvMbfvKTn3DFFVewYMECHn/8cW644QZef/31VtNvvPFGrrvuOq644goeeOCBDpVF6X2oKESG3i0KESIvL4+pU6cCMH36dHbt2tXhobGbDrt9//3389prrwGwZ88etm7d2iFR2LhxI9deey3vvfceQ4cOZd26daxbt46TTz4ZsO6qnJycNs+RkJDgi01Mnz6d999/H4AVK1bw6quvAnD55Zdzyy23tJm+bNkyXnnlFV/6rbfeGnZ5lN6HikJk6N2iEKGxs5sOP11XV9fm0NiBw2mLCI2Njb5tgcNuL1myhA8++IAVK1aQnJzMvHnzfENvh0tOTg719fWsWbOGoUOHIiJMnDiRFStWhHyO+Ph43xDZ3mG2W6K1YbQD03WobaUp2qM5MmhMoYdoa2jsUaNGsXr1agAWLVrkG+q6KRUVFfTv35/k5GQ2bdrEZ5991uH8ZGZm8vbbb/PrX/+aJUuWMG7cOIqKinyi4HA4WL9+fYfOPWfOHJ5//nnAjvx67LHHtpk+d+7coHRFAbUUIkW3iYIx5nFjzEFjzLqAtAHGmPeNMVs9y/4B235ljNlmjNlsjDm15bMe2ixcuJB//etfTJkyhYkTJ7Jo0SIArrnmGj7++GNmzZrFypUrg6yDQE477TScTieTJ0/mjjvu4JhjjulUfrKzs3nzzTe5/vrrWbNmDS+//DK33norU6ZMYerUqR1urXT//ffzxBNPMHnyZP79739z3333tZl+33338cADDzBz5kwqKio6VSal96CiEBm6behsY8zxQDXwtIhM8qT9FSgVkbuNMbcB/UXkVmPMBOA5YBYwFPgAGCsibfZl1KGz+xb63/YtBgyAsjI7fLbO6Nq1RGTobBFZCpQ2ST4HeMrz/Sng3ID050WkQUR2AtuwAqEoSh9FLYXI0NMxhWwRKQTwLLM86bnAnoD9CjxpzTDGXGuMWWWMWVVUVNStmVUUJXKoKESGaAk0t9T0pEW/log8IiIzRGTG4MGDWzzZoTybnNIy+p/2PVQUIkNPi8IBY0wOgGd50JNeAAwP2G8YsK8jF0hKSqKkpERfIr0IEaGkpISkpKRIZ0XpQVQUIkNP91N4A1gA3O1ZLgpIf9YYcy820DwG+LwjFxg2bBgFBQWoa6l3kZSUxLBhwyKdDaWHcLvBW69TUehZuk0UjDHPAfOAQcaYAuB3WDF40RhzFZAPXAggIuuNMS8CGwAncH17LY9aIz4+PqgHsKIohx6BQqCi0LN0myiIyHdb2TS/lf3vAu7qrvwoinLooKIQOaIl0KwoiuIjsFO/ikLPoqKgKErUoZZC5FBRUBQl6lBRiBwqCoqiRB0qCpFDRUFRlKhDRSFyqCgoihJ1qChEDhUFRVGiDhWFyKGioChK1KGiEDlUFBRFiTpUFCKHioKiKFGHikLkUFFQFCXqUFGIHCoKiqJEHTrMReRQUVAUJepQSyFyqCgoihJ1qChEDhUFRVGiDhWFyKGioChK1KGiEDlUFBRFiTpUFCKHioKiKFGHVwji41UUehoVBUVRog6vECQlqSj0NCoKiqJEHSoKkUNFQVGUqENFIXKoKCiKEnWoKEQOFQVFUaIO7zAXKgo9j4qCoihRh1oKkUNFQVGUqENFIXKoKCiKEnWoKEQOFQVFUaIOFYXIoaKgKErUoaIQOSIiCsaYnxtj1htj1hljnjPGJBljBhhj3jfGbPUs+0cib4qiRB6vECQmqij0ND0uCsaYXOAGYIaITAJigUuA24APRWQM8KFnXekB3nsPbr890rlQFD9OJ8TF6dhHkSBS7qM4oJ8xJg5IBvYB5wBPebY/BZwbmaz1PV5/Hf7v/yKdC0Xx4xWFuDgVhZ6mx0VBRPYC9wD5QCFQISLvAdkiUujZpxDIaul4Y8y1xphVxphVRUVFPZXtXk1jo/0oSrSgohA5IuE+6o+1CvKAoUCKMeayUI8XkUdEZIaIzBg8eHB3ZbNP0dgYPFG6okQaFYXIEQn30UnAThEpEhEH8CowBzhgjMkB8CwPRiBvfRKHA1wu+1GUaMDhUFGIFJEQhXzgGGNMsjHGAPOBjcAbwALPPguARRHIW5/E6zpSa0GJFtRSiBxxPX1BEVlpjHkZ+BJwAmuAR4BU4EVjzFVY4biwp/PWV/GKQmOjbReuKJFGRSFy9LgoAIjI74DfNUluwFoNSg+jloISbagoRA7t0awEWQqKEg0EioIIuN2RzlHfQUVBUVFQoo5AUfCuKz1Dm+4jY8yAtraLSGnXZkeJBCoKSrThdNrezIGikJAQ2Tz1FdqLKawGBDDACKDM8z0TGwzO687MKT2DxhSUaEMthcjRpvtIRPJEZDTwX+AsERkkIgOBM7H9C5RegFoKSrShohA5Qo0pzBSRd7wrIvIf4ITuyZLS06goKNGGikLkCLVJarEx5jfAM1h30mVASbflSulRvG4jFQUlWgjs0QwqCj1JqJbCd4HBwGvA69jB6r7bTXlSehiNKSjRhloKkSMkS8HTyuhGY0w64BaR6u7NltKTqPtIiTacTujXT0UhEoRkKRhjjjTGrAHWAuuNMauNMZO6N2tKT6GioEQbailEjlDdRw8DvxCRkSIyErgJO16R0gtQUVCiDRWFyBGqKKSIyGLviogsAVK6JUdKjyLijyVoTEGJFlQUIkeorY92GGPuAP7tWb8M2Nk9WVJ6kkAhUEtBiRZa6tGs9AyhWgo/wLY+ehXbAmkw8P3uypTScwQKgYqCEi2opRA5Qm19VAbcoK2Peh8qCko0oqIQObT1UR8n0H2kMQUlWlBRiBza+qiPo5aCEo2oKEQObX3Ux1FRUKIRHeYicmjroz6OioISjailEDm09VEfJ1AINKagRAsqCpEjrNZH3ZwXpRv4wQ9g8GD4y19a3q6WghKNqChEjpBEwRgzFvglMCrwGBE5sXuypXQVq1ZBVlbr21UUlGhERSFyhBpTeAl4CHgMcHVfdpSupqEB6upa366ioEQbIuByqShEilBFwSkiD3ZrTpRuoaEBamtb364xBSXacHmqnTrMRWRoUxSMMQM8X980xvwYG2Ru8G73zLOgRDHtWQo69pESbXgFQC2FyNCepbAaO/2m8azfHLBNgNHdkSml66ivh4SE1rer+0iJNlQUIkuboiAieT2VEaV7CDWmkJCgoqBEByoKkaU999GJIvKRMeb8lraLyKvdky2lq2hogNjY1rd7hSAlRWMKSnSgohBZ2nMfnQB8BJzVwjbBdmYLG2NMJrYl0yTPeX4AbAZewDZ73QVc5OkfoXQQpxPcbhtoFgFjmu8TKApqKSjRgLdyoqIQGdpzH/3Os+zq3sv3Ae+KyAXGmAQgGfg18KGI3G2MuQ24Dbi1i6/bp6ivt0u32z5oLcUWvEKQmqqioEQHailElvbcR79oa7uI3BvuBT1zMhwPXOk5RyPQaIw5B5jn2e0pYAkqCp2iocH/va5ORUE5NFBRiCztjX2U1s6nI4wGioAnjDFrjDGPGWNSgGwRKQTwLFvsh2uMudYYs8oYs6qoqKiDWegbNBWFltCYghJtqChElvbcR3/opmtOA34qIiuNMfdhXUUhISKP4JnLYcaMGdIN+es1hCMKqalQphEcJQoIFIWYGBsLU1HoOUKdeW2sMeZDY8w6z/pkY8xvOnjNAqBARFZ61l/GisQBY0yO5/w5wMEOnl/x4I0pQOu9mr3WQXKyuo+U6MArAPHxdhkXp6LQk4Q6dPajwK8AB4CIfANc0pELish+YI8xZpwnaT6wAXgDWOBJWwAs6sj5FT+hWgoJCZCYqKKgRAeBloJ3qaLQc4Q69lGyiHxugts0duZv+imw0NPyaAd2boYY4EVjzFVAPnBhJ86vEJ4oxMdrTEGJDlQUIkuoolBsjDkM26cAY8wFQGFHLyoiXwEzWtg0v6PnVJoTjihoj2YlWlBRiCyhisL12ODueGPMXuxUnJd2W66ULiEwpqCioBwqqChEllBFob+InORpOhojIlXGmLOA3d2YN6WTBFoKrQWaGxut6yg+XkVBiQ5UFCJLyIFmY8yRIlLjEYRLgI62PlJ6iHDdRxpTUKKBwGEuvEsVhZ4jVEvhAuBlY8ylwLHAFcAp3ZYrpUvoSEyhtTGSFKWnUEshsoQkCiKyw2MdvA7sAU4RkTYGZFaigXBjCmAfPm/7cEWJBCoKkaW9sY/W4mlx5GEAEAusNMYgIpO7M3NK5wi3Sap3XUVBiSQqCpGlPUvhzB7JhdIthBJo9o6e6rUUNK6gRBoVhcjSniiUiUhlwFzNyiGEVxTi40N3H2kLJCXS6DAXkaU9UXgWay00nasZdI7mqMcbU8jMbFsUkpNVFJToQS2FyNLeKKlnepY6V/MhSEODrW2lpIQXU1CUSKKiEFnaCzRPa2u7iHzZtdlRupKGBjvQXXKyuo+UQwcVhcjSnvvob21sE+DELsyL0sV4RaFfv/Z7NGugWYkWVBQiS3vuo2/1VEaUrqe+HpKSrCiopaAcKrQkCoF9bpTuJaTOa8aY81tIrgDWiohOhhOlBFoKVVUt76MxBSXa0GEuIkuow1xcBcwGFnvW5wGfAWONMf9PRP7dDXlTOkmgKBxsRbrVUlCiDXUfRZZQRcENHCEiBwCMMdnAg8DRwFJARSEKCSXQrJ3XlGhDRSGyhDpK6iivIHg4CIwVkVI8U3Qq0UdgTKGtQLNaCko0oaIQWUK1FD4xxrwFvORZvwBY6plfobw7MqZ0nkD3kfZTUA4VnE6IibEfUFHoacKZee187LDZBngKeEVEBNAWSlFKQwNkZLQuCi4XuN1qKSjRhdPptxJARaGnCXXobDHGfAo0YvsnfO4RBCWKaWopNJ0rwSsAGlNQogkVhcgSUkzBGHMR8DnWbXQRdujsC7ozY0rn8cYUkpOtIDS1ArzrgZ3X1FJQIo2KQmQJ1X10OzDT2yfBGDMY+AB4ubsypnSeQEsBbLA5MdG/PdBS0JiCEi2oKESWUFsfxTTppFYSxrFKhGgqCk3jCi25j1QUlEijohBZQrUU3jXG/Bd4zrN+MfBO92RJ6So6IgoaU1AijcMRLAqxsSoKPUmogeabjTHfAeZiWx89IiKvdWvOlE4T2E8BmouCVwDUUlCiCbUUIkuolgIi8grwSjfmReliAns0Q9uWgsYUlGhBRSGytDefQhW2CWqzTdiWqundkiul07jd9kFqGmgOJFAUYmNtZyEVBSXSqChElvaGzk7rqYwoXYt3fuZQYwrepcYUlEjjE4XXXoM33iBu1BOI2IpOjDZv6Xb0J+6leMefbyum0JIoqKWgRBqn0+POfO89WLjQZzWotdAzREwUjDGxxpg1njGVMMYMMMa8b4zZ6ln2j1TeegMdsRTi41UUlMjjsxSqq8HhIIFGX7rS/UTSUrgR2BiwfhvwoYiMAT70rCsdJFAU2gs0e4PMaiko0UCQKADJ7mpfutL9REQUjDHDgDOAxwKSz8EOtIdneW4PZ6tX0ZKl0Fag2bvUmIISaXyi4JkusJ9LRaEniZSl8L/ALdjJe7xki0ghgGeZ1dKBxphrjTGrjDGrioqKuj2jhyoaU1AOVZpaCklOFYWepMdFwRhzJnBQRFZ35HgReUREZojIjMGDB3dx7noPGlNQDlVUFCJLyJ3XupC5wNnGmNOBJCDdGPMMcMAYkyMihcaYHOzsbkoHCRSFmBj74m+rR7N3qaKgRBqHA1JT8bmPVBR6lh63FETkVyIyTERGAZcAH4nIZcAbwALPbguART2dt95EoChAy/M0a0xBiUbUUogs0dRP4W7gZGPMVuBkz7rSQQJjCtDyPM0aU1CikaaikNhY5UtXup9IuI98iMgSYInnewkwP5L56U00tRRampJTYwpKNOJ0QlJMo+9mTHSopdCTRJOloHQhHREFtRSUaMDphFSqfesJjSoKPYmKQi8lnJhCYOe1cGMKLhfce29z15SidBQVhfb5wx/g8ce759wqCocYTz8NpaXt79dSTKElUfCOjgodsxRWrYKbboJ33w3vOEVpDacTUsQvCvEqCs148klYvLh7zq2icAixbx8sWADPPtv+vi25j1oKNCckYKv6M2d2KKZQVmaX5eXhHacoreF0Qoq7yrce36Ci0JSyMhgwoHvOraJwCOG1EEpK2t831JhCQgKwZg189RUJ8RK2KHjFoKIivOMUpTWcTv94R6Ci0BSn0z5vKgqK7wUcivsoFFFwODyiUFICTiepMbVhxxS8YqCioHQVzUShXkUhEO97oH83jSOtonAI4X3xhhpTiI31z2DVWqDZJwpABhVqKSgRx+mEZJfHfTRoEHF12k8hEK/LVi0FJWxLwWslQDvuI48opLnDFwWvGGhMQekqHA7/yKjk5BCnlkIQ3udfRUHxvYC9NYW2aEkUWg00e+6yNFe5WgpKxHE6g0Uhtk5FIRCvKKj7SAnLfdSSKNTXg4g/rbERkuJdvjd7mrsi7JiCioLS1Tid/vGOyM5WUWiCuo8UH+G4j+rr/X0UwD98trf/AlhRGBhb5lOKVGc5LpftkBYqGmhWuhK3296O/ZxVkJIC6enEqCgEoZZCD/PUU3DgQKRz0TKBlkJgjb8lmloKLU3J2dgIg/C3b0122AuEYy2opaC43XDrrbB1a+fP5X3xJzmr7fjZqanE1KooBOK1FFQUeoDiYrjySisM0Yj3Bexy+QaQbJWW3EfQXBQGBohCiqPclx4qGmhW9u6Fv/4VXnih+Tans/0KTNP9wTMInlcUGhuIw6Gi4KG0FNLS/MPTdDUqCgF4Z/csLo5sPlojsDbengupNVEIDDY3NsIA/CfqrKUQzsOv9B689+Lu3c23TZkCd4cxCL5PFBqr7JsvNRWAFGpUFDyUlnaflQAqCkF4ewqH4rOPBIG18fby2FpMIdBScDhggPgthX4N9gLhWArePDkcwfEKpe/gfW6aikJ1NWzYAOvXh34u74s/IcBSAEijSkXBQ3cOcQEqCkF4b+5QhpGIBBUVkJVlv3fUUmjqPurv9hR24ECSGip86aHgdEJNDeTk+POn9D1asxR27bLLcJ4nnyg0BotCKtUqCh5KS1UUeoxoF4XycsjLs9/b66sQaqA5w1Viuz6PGEFimKLgFYERI4LXlb6F93nJzw92Ie7cGbw9FFQU2kfdRz1ItLuPKir8otBVlkKGs8RWOzIzSawrB0KPKXhFYORIu9Rgc9/Eey/W1/vjctA5UYiv98QU0tIAFYVA1H3Ug0SzpeBw2CBxqKLQWkyhaaA5w1ECAwdCZiYJdeFZCl4R8IqCWgp9k8DnJdCF1BFR8FZI4hvUUmgJEXUf9SiBlkK0taTxvnBzcuzLvqsshVSH5w7LyCChttyXHk6eVBT6NoH3Yn6+/7s3plBREXofA7ufqCi0Qm2tJxao7qOeoazYxb+5jCMbV1FTE+ncBOOtlWdk2Buiy0ShwWMpZGQQXxu+pRBPIyd/cgdpVKoo9FFKS2HYMPu9JUvBu08oOJ2QSAMxLmdQk1QVBUt3D3EBKgpBmL0FXMZCzuDtqIsreF+4mZn2huiqQHNqvd99FFdbRQyukGMK5eUwmxWMfeFOvs1/NKbQRykpgcMOs+9vryiIWFEYPNiuh9r3J2h+ZrUUmuF9Lx254YVum49TRSGAuKJCALI5EHVxBa8oZGRYUeiKfgqNjZBc77cUANKpDMt9NIwCAHLY3/stBYcjvIGh+gilpfYWGjnSLwplZVBZCTNm2PVQn6dmopCSAkBGjPZTAH9lcMoLv4bHHuuWa6goBJBUFr2i4K2Fey2FtkRBxL7wAy0Fr0B4A80iEOesI8FZ57MUADIJffjs8nK/KIxM7AOi8J3vwNVXRzoXUUdJiV8UvDEFr+uoU6KQlmZniUpKIt2opQDe515ILNkLQ4d2yzVUFDyIQGqVFYUh7I9a91EoloL3pR4oCsZYYfBaCg5HwBAXAZZCOLOvVVTA6HgrCsPjCnu/KHz+OXz9daRzEVUEtoYJtBS8QeaOiEIanlnXPK4jUlNJ6ypRcLvh4ovhk0+64GQ9T2kp9KeMmMaGbhOFuG456yFIVRVkuf2WwjdRbCn07992TKHp/MxeAmdfCxoMz9NPAawohBNTGBG3FxyQY3q5pVBTY4fPNSbSOYkqqqttBcNrbJaW2rQusRS8opCWRlpFF4nC/v3w4otUDsoj/bjjuuCEPUtZGQxln13Jze2Wa6il4KGkBHKIfkshLc2+w2tq/C//pnjHIAqMKUDwPM1BohBgKXTUfZQl+3t3oNn7ljtwQMdwDiBwakhv0+Tdu+3PlZlpm1AnJnbCfQSQmkpqF1kKFevt/brzk72dP1kEKC2FETGevKv7qHsJFIVUaqjaH11tUsvLIT3djkjhbY7WmrUQtqUQEFMI132U47IP2WBHL3cf7dhhlyLRO+FGBPC+7Cduf4PRgyoBG1fYudN2tDTG3l7htD5q0X3URa2PSr+x92tC8aErCoen9DJLwRgz3Biz2Biz0Riz3hhzoyd9gDHmfWPMVs+yG7tnNCdQFABc+6Lrwa+o8FXmfaLQmjXTlih4A81dYSlUlzkY0Lgf4uJIbyyiurwX16C9ogBQWNj6fn2M0lIYQiHH/Pkcxr93P+C3FLy97wcO7KT7KDWVlC4ShZrNVhRSKw5NUSgrg7xEjyh4R6LsYiJhKTiBm0TkCOAY4HpjzATgNuBDERkDfOhZ7zG8ouAY4umFs39/T16+XcrLfZV5X2/GzloKvkCzp0czhBdTSCjdTwwCRx5JDEJ8eVH7BzXhzDPh5z8P+7Aex709QBT27YtcRqKM0lIYxS4A0tcuIy7OBpl37fKLwqBBoYuCw9GyKKRK14iCY6cVhYF1e6Nv2IIQKC2FkbF77TPb1D/cRfS4KIhIoYh86fleBWwEcoFzAO+cZ08B5/ZkvkqLXGRxEJk8FYC4kii1FETatRRaiym05D5yJvSzG+LjkeTksCyFlDL7gHmjicmV+8N+zpYtg48+Cu+YSNCwcScHsT2xHPlqKXgpKYER2Hao5rMVjBjm5osv7D3YUUshjSrEGH+Py9RUUqRr+imYffaeTZYa25HiEKO0FHLY122uI4hwTMEYMwo4ClgJZItIIVjhALJaOeZaY8wqY8yqoqLwa6atUbf7ILG4iZt5FACJZdFnKQxNqYABAxi26nUgfPdR//7+USy9otCYNtC/Q0ZGyDEFEcisCRaFLHdhWMODVFTYcm3eHP19wtzbd/AZx+DGUL1FLQUvpaV+UaCiguMHbWD5crs6apRddsR95E5O9bf0Sk0lpYsshcSAWEL15kPPhVRWBlnOfd0WZIYIioIxJhV4BfiZiIQs2SLyiIjMEJEZg7196LsA115b+4uZciRuDMlV0WcpjGcjlJfTf42tWocrCmPHwvbt9gXsFQVHeoAoZGaGbCnU1ECO2/NQzZwJ2FZb4QSbvW3aGxr87dqjEhES9+5gK2M4QDZ1O1QUvJSUwGFx+RBjXyXHxy33WapNLYVQrEivKEhKqj8xNZVkd3WXVBzSKwvIZ7jN+zedE4XHHoNLLul8nsKhtBQG1ndfxzWIkCgYY+KxgrBQRF71JB8wxuR4tucAB3syTzH7PS6B4cOpTR5Eet0B3O6ezEHbVFTAaOdWABI2f4Mx4ccUxo2zYrBrl6dtOSU4M/yiYDIyyDShxRS8zVGd8UlwxBFAx0UB7LSNreJVsUixfz9xjnp2MJpCcnAXdI/7yOmEv/zF9pk5VCgthdHx+TBhAmRlMbV2uW/bqFHA+++TZ3bhcoU2iq5fFNL8iWlpJEk97sZOmgoiDKovYG2/owGo2tQ5UXjiCXjxxZ6bhtblgqoKF+m1+3uX+8gYY4B/ARtF5N6ATW8ACzzfFwCLejJf3nGPGDqU+vRsstkfNS5HEfsSHl5vRcGsXUv/TAk7pjB2rF1u2eIPNLsygy2F/iY0S8E77lHdwGGQlIQjNZMcwmuWGmgdbNzY+n6rTv8tO7NmhX7irsbT8qgodTT7GErswe6xFD75BG67DV57rVtO3y34YgojR8KcOYw+YEUhOxuSy/fBt7/N8Z/c6du3PXwxhdRgSwEgrqFzzcTdB4tJpJHyMdayrd/ecVGorYUvvrDP5vbtncpWyJSX2461MeLudZbCXOBy4ERjzFeez+nA3cDJxpitwMme9R4judzzoA8ZQuOAbLI5EDUd2GprbS0hp9qKAqWlHJGxL2z30bhxdrl5s9995O7fsZhCeTnkshdHlm2t5RiUwxDC68C2e7cVruzstkUh/vNl5FV8TUNJdegn70o8ohA7ZjTlSTkkl3fOUnAdKGbfVXc0m+Juy5bg5aFAaSkMdeTbOVnnzCHjwFYGc9C6jv71L3C5GFRk/9xQRSGVakhpLgrxDZ37/8vW2hhY2lGHU8xApKDjorBypf/v66n/q7Q0oDdzbxIFEflURIyITBaRqZ7POyJSIiLzRWSMZ9mjr+TUqkKqEgdCQgLuwUMYwv6oGRTP+6IdWL7N9mADZiR8E7YoDBpkg82bN0NjgzCAUiQzYGD2zEwyJHRRGEYB7qFWFCR7SNjuo127rIthwoS2RWFI5WYACj6K0Nty507cGBLHjqSu/1DS6jrXq/mrX73A0MfvZOfzK4PSD0VRqC+uJt1Z6hMFsMOpjx7hhEceASCt0BYoLFFIC3AfdZEoeDuuZUwcxoG4XOIPdlwUPvnEHwfvqf+rJ4a4AO3R7COzrpCqVNsZJCYnuiwF+6IVMou22ob9wJHSvig0dR8ZY62FzZtByiuIw2WjgF4yMkiXEN1HZW5y2YsZbm9OkzMkbPfR7t1wcsbnnJT1DRs2tByIrNhdTrbbtgQrXr459JN3Ie5tO9hLLrmHJeHKHmr7ZnSiV3PjmnUAHPw4OJByKIpCUtEe+2XECJg+HUlI4LiY5Zxu/gMFBTB3LvHlxfSnNCz3ES24jxIaOycK1VusCAyYPIzy5FySyzsuCkuXwpQp1srtSUshl+4d4gJUFADrShnsKqQ+04pC3LAhpFBLxd4IuSuaUF4OgygmobbCtvQZMYLxjd+0Gmj2xhSaWgrgFwVTGtCb2UtmJok0hhQ5aygoIgEH8aOspRA/LHxLYfdu+N36C7n51WM4uur9FvuE5b/vF4LGrzeFfvIupHHzDnYwmrw8iB1u7xHZ13EXUtpuKwrOb1oWha1biapGDq0hAhkVnuaoI0ZAUhJm+nSuPXI5F5U8aHvc/vKXAIxjc1iWgklrLgqJjZ2LwDt2FuAkliFTsqntn0v/2o6JQmMjLF8Oxx9v43Q97T6SmBirRt2EigIBvZkH2Qc+aaT9wet3RUdfhYoKOJxtdmXMGDjySEZVrw3bfQT2Jt63D+r22oPNoGBLASCuurzdPHn9sUmHW1GIG55DKjXUFVkhdbngsstg9myYNg0mT4Y33/QfX1MDrqISBlbnEytO3uQsDj71n2bXKVthhaCOJBJ2dJ8oLFvWemsus2MHO8lj1ChIPszW0Gq2djDYLMLwcisKKbv8ouBw2NDFoEE2hrT3EGhCX1kJue4AUQCYM4f0DSuJ//BduOYamDgRgCPM5pDGP/KJQnpz91FnLYWYvQXsJ4dB2bE4snIZ6DzQLK4TCl9+aTuBHndcz4qC133kzhpiB0HrJlQUgJJiYQj7kSFWFFJGW1Fw7o2Ovgrl5TAGT5D58MNh8mSGlG2kurSxxRplW6LgDTYf2GCrbTGDm4uCd67mtojx9AxNGG1FweQMAfw16HXrYOFCW6vKzbWDpD3zjP/4/HyYgp2boPL//s0GJjD5t+fCf4KFwbFuMw7iWDvgBAYc7B5RKCmxtb6//KWFjfX1JBbvZQejGTUKMsbbe6RsY8csharN+8iQchzEMaTULwo7d8JA536+dE5mIusOCReSt+Oa28T43Rlz5tgXrTF2QqK8PIiPZ3JiaJaCo1FIpZqY9BYsBUfnRCGxuICixGEYAzHDc4lBqN4WfsXPOxWDVxQOHqRHRgj2uo9Mbve5jkBFAYCKHSUk4CAm1z7wsbmeF1xh9FgKY9hqzca8PJg8mVi3k3FsatFd09Bgn8n4+ObbvKJQts0+obGDgwPNAIl15e3mKbHI05vZG/AaYn+z2CL7m332mU1+6SVrIZx8sm3C52X3bpjKVwBknHci56d/yIH0w22bzACSdm9iT+LhVI2cxLC6LYiz67s+f/KJddcsX97CRk9nip2MZsQIyDoyGzeG2m0dsxT2vLsegM/TTmKIay91++0fuGULnMQHDC9fy9m8EbIofPGFdd1HAm9z1IYBQ+0MaeALNnPWWTB8uE0/7DAmxIUmCqa+jljcLbuPOikK6ZUFVKTbSkziaHvfFn8dvkm2dKl9jrKz/c28t27tVNZCorQUhsXsI2ZY9wWZQUUBgNrtttaXMNIz6qDHXxdbHF2WgowcBQkJ1hcDTKbluEJ9vbUSjAHOOw/Gj4ennwank8MPt+k1+fYJjctubikk1rdvKSSV7MVBHGR5RiPxjNgYX2JFYcUKO2m7t1frzJm2Nux1IezaBUexBteQXEzWYIZO7M+76RfD2rVB1a6s0k2UDB6POWI8/ajn4Kr8UH4yP0VFdhrNNkY2XbrULlevbqFRkac5atXg0SQkwIjRcRwkC2d+x0Shcrl1HVWcehEAe96zza62bIFjsEo6N/azkETB6bRi+4tfdCgrnaa0FIazB8fQEf7EIUNsq6P/+R9/2tixjHGFJgoxtfbFHyQKnpZISc5OiIKn41r9QCsK6UfYF2vlxvBEwe2GTz+1liUE9/3pbsrKPIHmbgwyg4oCAI2eAc6SD/OIwqBBuDEklEaRpWC2YcaOsQljx+KKS2AyLbdAamjwuI4qK201fe9eWLAAxo8n6d+PMm54Lcn1JbgxxGcFjFAehqWQWlFAccJQv2/TYykklftFYfZsf7M9z0gYrFpll7t3w1F8Rcy0qYBtlvpW+bE2eukxM6rLnYx0bKMxbxzpM8cDULg4TBfSa6/Bq6/arqetsHSpLUZtrXV7BeERBckbDVgNLDRD/T3gw8SsX8d+M4TDf2DfKiWfWhfSli1wXOwKAGabz9iyuf0xIdassffGkiWRGfDTaynI8BHBG665xsa+vIwbx7CGbZQVt2/lxdY1mWAHICUF6JwouMoqSZEa3LlWFAZOtqJQuzU8UVi3ztZZvJO2HXaYvcd7QhSqiurp7y5VUegJxDPuUfo4jyjExVGRMJh+lVFiKZQJY9iKOfxwmxAXR13eBI6k5WCzTxQ+/thGfBctsp/MTLj2Wj7fl8t3eY5yMknoFxCw8lgKSQ3tWwr9qwso7TfMnzBgAE4TR2pVISUl9iGZPdu/efp0+/B4XUh7t9czno2Yo6YCdqSM/1YcjcTG2qoYsGvxThJwkDh5PLnzrSjUfBlms9QPPwxeNqGy0r5cvWPYrFzZZIcdO6gz/cgYa63HmBgo6zeUpLKOWQoZBevYnTaJUfNGUUcSzq+tKOzeWMtE19eQk8MAZxH1G3a0cyYrBmCNobb6ebRKbS1s29aBAy2lxW6Gs4e40SPa3nHcOOLdjfQ7GDCuyYMPwpNPNts1tq7JBDsA8fE0xiR2ShSKv7I+Nm9ruSETB1JPIu49AaKwb1+7Y4x4rUqvpZCYaPva9IT7KOaApyLSjX0UQEUBAOOp9SXl+SetqOqXTWpNdIiCHCwiXSqDal/OIya3aSkkJQHvv2+HxZ47F84+276Rly5la96p5LGT3YwMbsTgsRRCEYWB9QVUpgXcnDExVPbLJr12vy+ecMwx/s3p6dYP6xWF2E3rbT+JqVMBKwq1pFA95iifKBR9Yq2CQceOZ8ikQZQwEDaFYSm43f5xuZcsabHD2bJldrfvf9+2zm0qCu5tO9guoxmV55+buTYjh7TqDlgKbjcjKtdTOWwiCf1i2ZU0nn6eFkj9Nqy2v8f11wMwNP+zdvuLfPyxf24Nr0AEXIrPP4ff/c5aaZdf3sIJfvUr64oMdVq0JjTsOUgijSSNaV8UAAaVeAS9vh73zbfg/uXNzf6TOK+lECgKQEN8Kv1cHRcFb8e1lLH2nk3qZzgQM5TY/R5RcDjsaL/XXdfmef77X9vQyjv1KPRcC6Skku7vowAqCgAklBRSadJ9ZipAbXo2/Ruiw32UWuiphgSIQsxRk8llH3V7mj/Q3pgCH3xgqzTeZkjGwHHH8dnPnmcYBZyb8J/geehTU3ERQ7KjvO0MiZDl2Et15rCg5OrUHDIbrCjExvpdRl5mzvSPFzMg/yub6BGFCRPs6q7cY+2bubGRuq+sAOSeOA5joCBlHKl7wxCFtWvtC++ss2wNMDDS7WHpUhsLnT0bZs1qLgrOTVt9zVF9aYOHMsARfq/mktW7SKYWmTgJgOLBExhSuoHqajis2KOkV12FIzGFWfJZ0GRvTXG5bID8wgttxfHjj4O3X345HH003Hmn7Wf3zDP+aaYB+xJ89lnbtvKpp+gQ+Ta+E5sXmiiMatxMXR3I+x8QU1NNTEkx8mHwZBpx9S24j7CikOzqeD+Fas+Ma/2P9N+zpf1y6VfqedG+956NO732Gq2N/757N7zzjm1qHYhXFFpy4W3d7OZXF2wNa0j5lnC5IL6o+4e4ABUFAPqVF1KSEDy1XUPmEAa6DnTJcL2Vm/bx1Z//Q2Fhx3y/mcUBfRQ89Jtlg801n61ttn9DAwyP2Wt9Cied1Gz7uHFwkGzKEocEbzCGuvgMkhvbsRQqKkiRGuoHBYtCXcYQBjsLWb7cVkADNBaworB/v305jar4ioaENBhtffUjRtg5VVYlHWtVbc0a4rdvoiQ2yxf3KBsynpyKMETBYyXM/uCPdtKWDz5otsvSpTZfycnWstm4MWDulXXrSNiyno85IUgUYnJziEHCbrK87z0bsMiYa0Wh8fAJDHPuZv3Kamazgursw2DIEOomzuQY2g42f/WVzee8efbz8cf+e2vPHnjuOdsi9OBBfxPKZ58NOMF771nBTE+3geEO3JjxhU36KLTGoEE0JGf6OrAVP/oq5WRQSRqF970QfM76FtxHQGN8Kv3cHbcUHDusKAyZ5n+hVmfkklHtEYVnnvEHlt5+u8VzPPSQXf7wh8HpY8faOkdLndxXfv8h/vzKWP7722UdzjvY0GB6TfcPcQEqCgCkVRdS0S9YFFyDshnCfspKOx/B2zXvSqb++nQuHrqUrCxb0winz8zg8q24TCyBb6b46VYUSt5ZSXWTZ6WhAY5t8LwATz652fm8LSYSEppfqzYhgxRnefMN779vo6wnnojrZttstDErWBQaB9hezZ9+Guw68uK1HF55xTZHLR81xTcOf0yMbST14t65dqdPP6X/wc0c6D/ed7zz8PFkuQ9Qu7eVXmZN+fBDCtPG8FndFLanHdVMFLwjXXr9w0cfbd+NPoPi4YdxxSXwJFf6WlEBJObZF0vx2vBcSDUrrSiMOM2aRUnT7fKLpzcymxU4ptsfLf6E2UzlK3asr2v1XF530Qkn2M+BA7anOtiGZiLWOzRwoHV1HH+8fe/53v0LF9opHe+5x1Zzvc5y7Hluvtk/n3drJB0MURSMoSZ3nBWF/Q76vbeId2LO4t3Ec0l//9WgYdF94xs1dR8lpJLcCfeR2beXA2STmeW/6RsH5zKoca9V19dfh6uusg0mXnih2fH19Xb+hLPPbl7c1logVVUKMz/7BwA5D97RqV7q99wDE9L3IomJfp9hN6GiAGTWF1Kd1mQS7CFD6Ec95Xs617V+96urmXzgfdwmhtezf8RpJzaycKEdQDJUcmq2Upw6KrjjQXY2lVOO40f1f+fZh4Pz2NAAs2s+sG1Cjzyy2flyc23NuCVRqE/MINXZxFJwu+Gmm6yfpbycmMcfBcCRNzZoN+fgHLI4iLPRFRRk9jJ1qj3Fyy+6mcLXOCZODdp+9dXw7ldDqMw6DNfHnzKqfhO1I/yi0G+q/V7wYcvB5qDKrsOBe8nHLKqeT1YWvFJ5ErJ8RZBrwDvS5Qkn2PVZs/zpVFfD00+zbvyFlMUMYliA/nkbJJSuCy/YHLtxHfkxIxky1g5qmD3PioLz7f+Sw35S5ltR6DfvGOJx0rhidavn+vhjazgOXfEKJ2d/40tzu+Hxx+Hq6WsYve0933gZl11mwzGrV3vKtmgRO6ZfyOXvXopkZMDDDwPWm/Sd79iX0D/+0XZ50svzqY1N9U8e3gaNo8cxli00vL+U1IZS9s/9Dq4LLibVWc7uR98D7Iv3wI6W3UeOhFSSJTxR2LXLP1xIYMc1L5KbSwq1NDz0hL34ggXWH/fOO82m6nzpJWtYeUI+QbQmCkt//xHjZBPbhx7L7LrFrPxzx+ad/ewzG/s6fsw+zNChBPt8ux4VBREGOQqp7x8sCrFDbWuTqm3tuAjaMbuLbvoz5WRQ9c9nGHBgI09P+RvHHgt/+EOrrstmjGjYSunAMc3S0x6+h2wO4rzrL0G1kPo6YWbFB9Z1FNP8L46JsTdyS53b6hIzSXWVByc+/7z1z997L3z5JdtXlXMEG3BOmhq0mwwZQixuBlPUoigkJVmNKl61k3SqSDo6+Pgf/tD69t+uOBb54AMGUULsRL8oDDrWfi9d0VwU7v2bMH5YNWu93rQvviCmppqPmM8778CnCfMxToffl4J9icbE+Ptb9e9vf5eVKz1lrqzkjdzrGDYs+LcaNNlaCtVbw7MU+u9bT0HmJN8zPfLEw2gknrNKngAg4QTPj3a0nQQmbf1nLZ7H5bIV+2vGfgwXXMDIH53GhOwSPv7YFs+xI5//23ginHqqNb/+8Q8uPK2KhARPr/LXX4faWn70yaU882oyb2RcgbzyChQXc9ttsH49TBzv4s9/bn12P4D+VfmUpIwI6SVlxo1jGHupf/gpakhm8i9P4eS/nkwp/dl37/OI2EqBb7yxpu6jxDRSwnAfPfus7SPzgx/Y3yu9wt9xzUtinnXDxNx3L5KXx+/enc1Hgy+2AvHGG0H7PvAAzBudz/x3boL58+GMM6x63norw2P2kpjYXBSSn3yA0thBjPjqTQpjc0n7nzs65Kb729+s7h7eb2+3u44AEJFD9jN9+nTpLK6yChGQd+b/T1D6xvv+KwKy7O6lrR9cUSEyb57I/PkilZXNNu/9aJO4MPLu9F/bhPPPF0lKki9e2C4gctdd7efP0eiWCtJk5ayftLh915zvSi1J8v7j+b60iyeuFQGRf/2r1fNedZXIjBnN07/OO1vWxk72JzQ2ihx2mNSOmSzXXOWSu+4S+fvf7enfeCP42K9++4oIyLzMNeJ2t3zda68V+Q4viYA0fraq2fa1a0WujXnUXgBk5wNv+7bVVTmkgXhZOvc2/wElJfLBOffJWiZKPQly3dDXpaJCpOGO/ycujFxxRrGIiNx4TY3UkyDV1/3Sd+gFc/bKH0c8InLOOSIDB4pcf70suNQh2VlucU+bJjJpkhw71y0nnBCcx6oyh7gw8smJvxWHQ+R3vxO55BKRb75pucwiIu6GRqknQd496tag9M2Jk0RA6mOS7G/t4WBqnryZ+J0Wz/XllyLJVEtl1miR4cNF4uNlxfALJGeIWxZc5pRPYo8Xd2qqyD//KXLMMfa3zM2VW7+1UrKyRFynnCaFCSMkM90ld94pMolvREA2XH2PHM8S2TDiVHH2S5FTeVd++csWsyAiImtip8u6Eae1vkMApY94/nPi5M2kC8TptOnLjrhKKkmVW35aKyDyyfG/FomNlaY30JcTL5PtjA7pWtu2iaSliQwdaot+0UUipaa/fDD++uBz3v+J7z57ZdIdAiIxuKR6wDCRM8/07bf2pY3yFJeLMybO5u3oo0WmTxeZMEEkLk4kIUEWDrherjrF/wxu+G++OImRL+bb//s/5z0kArLjgXfaznxxsciVV4o8+KBIQ4Ns3y4SEyPy25+Vi4wYYQvTBQCrpJX3asRf7J35dFoUKiul7tSz7U1xzX+CNu1+82sRkMU/eanlY6uqRObOFXdsrLhjY0VmzxYpLw/aZfmEH0gtSbJn9QGbsGePSGqqyIknyqWnHJT0dHsPtEpZmVTd/7jNx3n3tbhL45adUkei/HfIFb60v+T83f61u3e3VXQ5eLB5+ucTrpDdZoQ/4eGHRUDOT3xT+vXzPUMCIsuXBx+74bFlIiC/m9X6jf/ooyJ/5HZxECtSV9fiPv/7ww2+i9Rv2B60bUv8EbIy91yR/HyR664TR1yiCMjWATOl8vCp0kC8/Gn2G1Iwdp6s5ihZ6tH0rVtFPmKe7M2eKu66etn23d9II3H2OiNGiJxxhhWhiafLPD6y98T8BwREfvaz5nncb7LlvZFXy/HHi4xmm0xO3CTGiHzveyJffy2+l55v/8W2TO9d8e+g9E+HX2zzn3NsUPq6Kd+VPeRKRUXza997r8j9/ETcxogsXSpy990iIJfyb/l1zJ9smZ580n/AsmUio0aJMz5Rfs7fxGli5U/cJgsX2s2//73IMmb7fg93VpbIuHHSGJMoZ8S/2+Jt5HKJHGCwfH7UtS3+h01pXP2N7z997uxnfel7n3rf3l+8LJdcIuL+6Q0imZnNjl857Ueyn6x2r9PQIDJzpsictG+k/PpfyxfH/Vz+xs9FQN4/8U9B++78aIcvT2PZJL//vcjxx4vca34hrrh4cRcVy47r75EGkyBVpEj9dT8T2bUr+II7d4pcc404TJzUmGSRxx8Xcbvl3Rm3iwsjpV/uFBGRksIG2WlGyc6B05rfHF4KCqT+8Am+PFUOGiX/mvWQ/D3m5+JKTbPpDz3U7m8QCioKTXC5RFY8v0vyB0wWB7HyY/5PXn01eJ+yTftFQJYcc4vseWaJFD72lpS+tUwa9x4Ud1W1lB91gjhNrHwv/kW5JOEVcZg4qZo4S6SsTEREDn65RxqIlw+OaFLDf8jWGFyJSfIw18rfL19tlcHpFGdtg2x7epmsPO/PsmHYSeIw9iHdS468fufaVsuz4oRbREC+OO8u+fz8P8n62CNlb9rYDv02y6b/VCpIE9f2nVL19XapSM+VT5kjR89yS2GhLd7ixSLPPmt/x0C2Ly0QAant11/kxhtF1qwRefttkeuvFxk/XuSEE6TwR7+XFRwt25MntZqHulq3lMYMlHqT2OwBWpZ9nlSbFHHExEujiZeHuUZuPnmNOBwiUlYm+4bPlHoSpIF4eSrrl0EVzoUT/igCsiNpvAjIy/0uky2vrfPXSh96SNwxMdJAvFSbFEmjQn7845a1a0O/o2Q3w2WjGe97iLeOPFHOSXhH0imX7ya8LG8OulJ2DpgmO3PnSP6gqSIgqx79Mug8H837g03/VnCVfM1V94uArH5+ixz8ZJPkP/mh7HntCzmw9oDcdsxie80bbrA7O51SM/1YqSBNGomT4vkXNatpS3GxuOad6MvrTaeu9e3idovcd+rb8pWZKgW/+j+R2lqR4mJpmDBV6kiUe+ctkoKFS2TLgjtl+8yLZdu5N8nWWx4RAfn09Dtb/R+DqK0VF0bqSZDNXwQoncMhlUmDpThhiLiOmGArTcOGNTt82ZxfSg39ZP16kepqEWdljeQv+lJW/fwZWX7On+Xr6x6UPX9/SZ45Y6F8xDxbzrg4kbQ0aUxIkTIy5P3bFwdnqaxeBGQlM+Wvf7VplZUi35+4UgRkX/xwEZC34s+RZ/62v83i/f3GnfIh3xIByT/2u3LAZMnqoWcG7fPkiU+JgJRm5sn2n90vjaVVvm0lK7dKcfooqSBNTklYLGfFvSOrmCYC9j1w6aUiq5pb1h1FRaEJKx/6UvaTJWVkyP+c/F/56KPmz5Cr0Sm1JPkeosBPA/HiJEa+n/Ss/OhHItdcI3JBwiJfuvel5CBWtn+4s3kGNmwQufZaaYgNPr+v5gqyIWaCPJB6i5yTtVyOGOeSdetaL0/prnIpNEN8xzqIlSUn/bFDv82H8/5fs/L+vxMXt1apD/7NXCJPXrVU6s65WCQ+3n+Ofv1ETjtNZNo0W7sFWTp6QZvnqjn7EqmdOrtZ+tvz/ioNxMtD5ody1uRd8oc/iBUED+7SMtmaOV0E5INfBlt/3zxmH/aCuBHyzg3/kfr65tdtfPVNqSZZHkz4qbz4Yuv5+/yIy6XRxEvV7JNE7rtP5C9/EcnNDfrdKuL6y+LEU+VDM1+WMVveiTlDSvc3BJ1nxU3WrfLlr4Mt0m3Pfd7ivef97E87zL4dveXevkOqTKrsixsu7pLSljPtcMgHM2+TF1OulJKS4E1utzV+gyguloLBU4Ouu4NRQc/F0p+80PqP1ISt8eNlSf9zmm945BHrgj3/fJHvf1/kmWea7bLi9Ob3ZWuf4tQR9v8IMMPz85tXYkREPjj8h/LWD4P9oGWlbtmdNEaqYtJk8RWPS3VVK77QAKqrRe78g1P+kvZHcRIjArLmz8H3X/5ut9w67jX5lDkiIFWkSL4ZLrvMSKkiRYoYKHed/4Xs32/rQps3ueWTuz+Vsm/yW7lqx2lLFIzdfmgyY8YMWeUdTCcMXPuLKDrlUjKeup9+R41vdb8NT31BybpCGuJTqY9NQYqKSdiznX6F26mbezJz/3ymLx5WUQEf/GEZyUveweDGiJv4GVM58dHvtnr+im1FfPzr/2JKS0ioKSPO1UDc7JkM/95x5M0aHFYjg21fVVN5oI6Bo9LIHpFIUr+OtVDYsbqMlXe8RVK8k36JQtphWcz505nhN3goKrIdgUaOtM17vNPAlZay6h8ryDxxGocfl9P68TU1tnOYZ+gNLxWlLr5eWc9Rx6Y0baDioyq/jHX/8x9m3XsJsfHBgfbC5z9m8KnTiOvfysHAmo8rGDQiheF5ca3nz+m0zbwCO2M0NtoxlrZssU2BZ8+GuDhE/I1ZmhSHhvI6Vi34B7MW3kh8qn+sc3G5WXHO3TQ2gnPoCMjNJaa6kvjCfBKK9zL85u8y9NtTgs618vH1JGenceQZrTcRdbtti6uWhlVvicpdpXz+038jow8j5eQ5DJkwgOIDLopX7cK5cw8n/vZYUjPb+J0C+OKVfDKHpTLm6AHt79wEV/5e9t75BBUlTirKhWpHAvETxzLouAlkzRxJ4ZYq9n1dRHVRHWf/YTrJ6aHlqVX27rVN5cKczMbhgE//sgzz6VKOf+tWYuKaN/QoLYWvH/mMhBeeIa6xlhjcEB9P5h9vYsxZrb+PuhJjzGoRmdHitr4oCoqiKH2ZtkRBm6QqiqIoPlQUFEVRFB8qCoqiKIoPFQVFURTFh4qCoiiK4kNFQVEURfGhoqAoiqL4UFFQFEVRfBzSndeMMUXA7nZ3bJ1BQMcmqD106Ytlhr5Zbi1z3yHcco8UkcEtbTikRaGzGGNWtdarr7fSF8sMfbPcWua+Q1eWW91HiqIoig8VBUVRFMVHXxeFRyKdgQjQF8sMfbPcWua+Q5eVu0/HFBRFUZRg+rqloCiKogSgoqAoiqL46JOiYIw5zRiz2RizzRhzW6Tz0x0YY4YbYxYbYzYaY9YbY270pA8wxrxvjNnqWfaPdF67A2NMrDFmjTHmLc96ry63MSbTGPOyMWaT5z+f3dvLDGCM+bnn/l5njHnOGJPUG8ttjHncGHPQGLMuIK3VchpjfuV5v202xpwazrX6nCgYY2KBB4BvAxOA7xpjJkQ2V92CE7hJRI4AjgGu95TzNuBDERkDfOhZ743cCGwMWO/t5b4PeFdExgNTsGXv1WU2xuQCNwAzRGQSEAtcQu8s95PAaU3SWiyn5zm/BJjoOeafnvdeSPQ5UQBmAdtEZIeINALPA+dEOE9djogUisiXnu9V2JdELrasT3l2ewo4NyIZ7EaMMcOAM4DHApJ7bbmNMenA8cC/AESkUUTK6cVlDiAO6GeMiQOSgX30wnKLyFKgtElya+U8B3heRBpEZCewDfveC4m+KAq5wJ6A9QJPWq/FGDMKOApYCWSLSCFY4QCyIpi17uJ/gVsAd0Baby73aKAIeMLjMnvMGJNC7y4zIrIXuAfIBwqBChF5j15e7gBaK2en3nF9URRMC2m9tl2uMSYVeAX4mYhURjo/3Y0x5kzgoIisjnReepA4YBrwoIgcBdTQO1wmbeLxoZ8D5AFDgRRjzGWRzVVU0Kl3XF8UhQJgeMD6MKzJ2eswxsRjBWGhiLzqST5gjMnxbM8BDkYqf93EXOBsY8wurGvwRGPMM/TuchcABSKy0rP+MlYkenOZAU4CdopIkYg4gFeBOfT+cntprZydesf1RVH4AhhjjMkzxiRgAzJvRDhPXY4xxmB9zBtF5N6ATW8ACzzfFwCLejpv3YmI/EpEhonIKOx/+5GIXEYvLreI7Af2GGPGeZLmAxvoxWX2kA8cY4xJ9tzv87Gxs95ebi+tlfMN4BJjTKIxJg8YA3we8llFpM99gNOBLcB24PZI56ebyngs1mT8BvjK8zkdGIhtqbDVsxwQ6bx2428wD3jL871XlxuYCqzy/N+vA/17e5k95f4DsAlYB/wbSOyN5Qaew8ZNHFhL4Kq2ygnc7nm/bQa+Hc61dJgLRVEUxUdfdB8piqIoraCioCiKovhQUVAURVF8qCgoiqIoPlQUFEVRFB8qCoqiKIoPFQVFURTFx/8HQo4IgFAFc7cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sns.displot(y_test)\n",
    "plt.plot(np.sort(y_test.detach().numpy()), color='blue', label='true likelihood')\n",
    "plt.plot(np.sort(y_pred.detach().numpy()), color='red', label='neural likelihood')\n",
    "plt.ylabel('loglikelihood')\n",
    "plt.legend()\n",
    "print(max(y_test.detach().numpy()), max(y_pred.detach().numpy()))\n",
    "print(min(y_test.detach().numpy()), min(y_pred.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47ff911a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1]), torch.Size([100, 1]), (100, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_pred), np.shape(y_test), np.shape(y_pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58b48c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51,), (51,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(history_train), np.shape(history_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4744941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 60.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ytest, bins=len(ytest), color='red')\n",
    "# plt.hist(ypred, bins=len(ypred), color='green')\n",
    "plt.ylim(0,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a45db1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)==len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a07e1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f802e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
