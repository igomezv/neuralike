{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee35f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fd99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om\t\t\t\\Omega_m\n",
    "# Obh2\t\t\t\\Omega_{b}h^2\n",
    "# h\t\n",
    "datafile = 'chains/LCDM_phy_HD_nested_dynesty_multi_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc69788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataSet(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Prepare the dataset for regression\n",
    "    '''\n",
    "    def __init__(self, X, y, scale_data=True):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c80912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ncols = 3\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(ncols, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e24b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1036, 3) (1036, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Load Boston dataset\n",
    "    X = np.loadtxt(datafile, usecols=(2,3,4))\n",
    "    y = np.loadtxt(datafile, usecols=1).reshape(-1, 1)\n",
    "    randomize = np.random.permutation(len(X))\n",
    "    X = X[randomize]\n",
    "    y = y[randomize]\n",
    "    print(np.shape(X), np.shape(y))\n",
    "    X_test, y_test = X[:100, :], y[:100, :]\n",
    "    X, y = X[100:, :], y[100:, :]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f173672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LoadDataSet(X_train, y_train)\n",
    "dataset_val = LoadDataSet(X_val, y_val)\n",
    "# dataset_test = LoadDataSet(X_test, y_test)\n",
    "X_test, y_test = torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=1)\n",
    "validloader = torch.utils.data.DataLoader(dataset_val, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ea89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "mlp.float()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f533ceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       800\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       40,200\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       201\n",
       "=================================================================\n",
       "Total params: 41,201\n",
       "Trainable params: 41,201\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mlp, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00af4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Loss after mini-batch     1: 17.874\n",
      "Loss after mini-batch    11: 15.234\n",
      "Loss after mini-batch    21: 13.711\n",
      "Loss after mini-batch    31: 7.808\n",
      "Loss after mini-batch    41: 10.769\n",
      "Loss after mini-batch    51: 21.282\n",
      "Loss after mini-batch    61: 20.683\n",
      "Loss after mini-batch    71: 12.503\n",
      "Training Loss: 19.789 \t\t Validation Loss:31.927\n",
      "Starting epoch 2\n",
      "Loss after mini-batch     1: 7.829\n",
      "Loss after mini-batch    11: 12.825\n",
      "Loss after mini-batch    21: 6.663\n",
      "Loss after mini-batch    31: 6.700\n",
      "Loss after mini-batch    41: 5.545\n",
      "Loss after mini-batch    51: 10.091\n",
      "Loss after mini-batch    61: 8.777\n",
      "Loss after mini-batch    71: 5.741\n",
      "Training Loss: 7.286 \t\t Validation Loss:18.048\n",
      "Starting epoch 3\n",
      "Loss after mini-batch     1: 13.845\n",
      "Loss after mini-batch    11: 9.735\n",
      "Loss after mini-batch    21: 5.536\n",
      "Loss after mini-batch    31: 4.321\n",
      "Loss after mini-batch    41: 2.700\n",
      "Loss after mini-batch    51: 4.253\n",
      "Loss after mini-batch    61: 3.719\n",
      "Loss after mini-batch    71: 2.420\n",
      "Training Loss: 2.672 \t\t Validation Loss:4.560\n",
      "Starting epoch 4\n",
      "Loss after mini-batch     1: 2.726\n",
      "Loss after mini-batch    11: 5.444\n",
      "Loss after mini-batch    21: 3.481\n",
      "Loss after mini-batch    31: 2.759\n",
      "Loss after mini-batch    41: 2.643\n",
      "Loss after mini-batch    51: 0.400\n",
      "Loss after mini-batch    61: 7.788\n",
      "Loss after mini-batch    71: 5.121\n",
      "Training Loss: 2.222 \t\t Validation Loss:6.463\n",
      "Starting epoch 5\n",
      "Loss after mini-batch     1: 2.947\n",
      "Loss after mini-batch    11: 1.505\n",
      "Loss after mini-batch    21: 4.658\n",
      "Loss after mini-batch    31: 0.591\n",
      "Loss after mini-batch    41: 2.032\n",
      "Loss after mini-batch    51: 1.244\n",
      "Loss after mini-batch    61: 5.417\n",
      "Loss after mini-batch    71: 0.991\n",
      "Training Loss: 5.204 \t\t Validation Loss:5.979\n",
      "Starting epoch 6\n",
      "Loss after mini-batch     1: 0.696\n",
      "Loss after mini-batch    11: 0.509\n",
      "Loss after mini-batch    21: 4.111\n",
      "Loss after mini-batch    31: 0.689\n",
      "Loss after mini-batch    41: 1.178\n",
      "Loss after mini-batch    51: 1.047\n",
      "Loss after mini-batch    61: 3.431\n",
      "Loss after mini-batch    71: 0.710\n",
      "Training Loss: 4.880 \t\t Validation Loss:7.499\n",
      "Starting epoch 7\n",
      "Loss after mini-batch     1: 4.759\n",
      "Loss after mini-batch    11: 6.000\n",
      "Loss after mini-batch    21: 0.500\n",
      "Loss after mini-batch    31: 8.044\n",
      "Loss after mini-batch    41: 0.933\n",
      "Loss after mini-batch    51: 1.379\n",
      "Loss after mini-batch    61: 0.424\n",
      "Loss after mini-batch    71: 2.770\n",
      "Training Loss: 5.524 \t\t Validation Loss:6.211\n",
      "Starting epoch 8\n",
      "Loss after mini-batch     1: 0.712\n",
      "Loss after mini-batch    11: 5.306\n",
      "Loss after mini-batch    21: 2.924\n",
      "Loss after mini-batch    31: 1.341\n",
      "Loss after mini-batch    41: 3.306\n",
      "Loss after mini-batch    51: 2.209\n",
      "Loss after mini-batch    61: 4.615\n",
      "Loss after mini-batch    71: 3.358\n",
      "Training Loss: 4.500 \t\t Validation Loss:8.990\n",
      "Starting epoch 9\n",
      "Loss after mini-batch     1: 0.946\n",
      "Loss after mini-batch    11: 0.655\n",
      "Loss after mini-batch    21: 2.504\n",
      "Loss after mini-batch    31: 0.812\n",
      "Loss after mini-batch    41: 7.950\n",
      "Loss after mini-batch    51: 3.684\n",
      "Loss after mini-batch    61: 1.573\n",
      "Loss after mini-batch    71: 1.414\n",
      "Training Loss: 0.760 \t\t Validation Loss:1.202\n",
      "Starting epoch 10\n",
      "Loss after mini-batch     1: 0.720\n",
      "Loss after mini-batch    11: 5.518\n",
      "Loss after mini-batch    21: 0.819\n",
      "Loss after mini-batch    31: 1.252\n",
      "Loss after mini-batch    41: 3.265\n",
      "Loss after mini-batch    51: 2.564\n",
      "Loss after mini-batch    61: 0.678\n",
      "Loss after mini-batch    71: 0.578\n",
      "Training Loss: 3.377 \t\t Validation Loss:3.698\n",
      "Starting epoch 11\n",
      "Loss after mini-batch     1: 1.174\n",
      "Loss after mini-batch    11: 4.181\n",
      "Loss after mini-batch    21: 0.690\n",
      "Loss after mini-batch    31: 5.495\n",
      "Loss after mini-batch    41: 1.084\n",
      "Loss after mini-batch    51: 0.688\n",
      "Loss after mini-batch    61: 1.145\n",
      "Loss after mini-batch    71: 1.453\n",
      "Training Loss: 0.502 \t\t Validation Loss:3.469\n",
      "Starting epoch 12\n",
      "Loss after mini-batch     1: 1.361\n",
      "Loss after mini-batch    11: 1.120\n",
      "Loss after mini-batch    21: 0.688\n",
      "Loss after mini-batch    31: 1.786\n",
      "Loss after mini-batch    41: 2.140\n",
      "Loss after mini-batch    51: 0.611\n",
      "Loss after mini-batch    61: 1.071\n",
      "Loss after mini-batch    71: 3.104\n",
      "Training Loss: 7.372 \t\t Validation Loss:11.257\n",
      "Starting epoch 13\n",
      "Loss after mini-batch     1: 0.543\n",
      "Loss after mini-batch    11: 0.479\n",
      "Loss after mini-batch    21: 3.793\n",
      "Loss after mini-batch    31: 1.565\n",
      "Loss after mini-batch    41: 0.474\n",
      "Loss after mini-batch    51: 0.504\n",
      "Loss after mini-batch    61: 0.214\n",
      "Loss after mini-batch    71: 0.569\n",
      "Training Loss: 0.526 \t\t Validation Loss:1.513\n",
      "Starting epoch 14\n",
      "Loss after mini-batch     1: 1.706\n",
      "Loss after mini-batch    11: 1.378\n",
      "Loss after mini-batch    21: 0.401\n",
      "Loss after mini-batch    31: 2.200\n",
      "Loss after mini-batch    41: 3.100\n",
      "Loss after mini-batch    51: 2.475\n",
      "Loss after mini-batch    61: 0.763\n",
      "Loss after mini-batch    71: 4.370\n",
      "Training Loss: 6.974 \t\t Validation Loss:7.506\n",
      "Starting epoch 15\n",
      "Loss after mini-batch     1: 2.046\n",
      "Loss after mini-batch    11: 0.376\n",
      "Loss after mini-batch    21: 0.649\n",
      "Loss after mini-batch    31: 2.739\n",
      "Loss after mini-batch    41: 1.093\n",
      "Loss after mini-batch    51: 1.948\n",
      "Loss after mini-batch    61: 0.623\n",
      "Loss after mini-batch    71: 2.171\n",
      "Training Loss: 1.202 \t\t Validation Loss:1.706\n",
      "Starting epoch 16\n",
      "Loss after mini-batch     1: 4.883\n",
      "Loss after mini-batch    11: 4.028\n",
      "Loss after mini-batch    21: 1.075\n",
      "Loss after mini-batch    31: 0.806\n",
      "Loss after mini-batch    41: 1.076\n",
      "Loss after mini-batch    51: 1.262\n",
      "Loss after mini-batch    61: 3.406\n",
      "Loss after mini-batch    71: 4.386\n",
      "Training Loss: 0.849 \t\t Validation Loss:1.141\n",
      "Starting epoch 17\n",
      "Loss after mini-batch     1: 1.461\n",
      "Loss after mini-batch    11: 7.557\n",
      "Loss after mini-batch    21: 2.141\n",
      "Loss after mini-batch    31: 2.375\n",
      "Loss after mini-batch    41: 0.670\n",
      "Loss after mini-batch    51: 0.199\n",
      "Loss after mini-batch    61: 0.541\n",
      "Loss after mini-batch    71: 0.695\n",
      "Training Loss: 2.135 \t\t Validation Loss:2.845\n",
      "Starting epoch 18\n",
      "Loss after mini-batch     1: 4.574\n",
      "Loss after mini-batch    11: 0.799\n",
      "Loss after mini-batch    21: 0.832\n",
      "Loss after mini-batch    31: 1.611\n",
      "Loss after mini-batch    41: 1.420\n",
      "Loss after mini-batch    51: 0.952\n",
      "Loss after mini-batch    61: 0.750\n",
      "Loss after mini-batch    71: 0.925\n",
      "Training Loss: 0.021 \t\t Validation Loss:0.486\n",
      "Starting epoch 19\n",
      "Loss after mini-batch     1: 0.579\n",
      "Loss after mini-batch    11: 1.034\n",
      "Loss after mini-batch    21: 0.865\n",
      "Loss after mini-batch    31: 1.834\n",
      "Loss after mini-batch    41: 0.435\n",
      "Loss after mini-batch    51: 0.280\n",
      "Loss after mini-batch    61: 0.256\n",
      "Loss after mini-batch    71: 0.090\n",
      "Training Loss: 0.303 \t\t Validation Loss:0.395\n",
      "Starting epoch 20\n",
      "Loss after mini-batch     1: 2.468\n",
      "Loss after mini-batch    11: 0.153\n",
      "Loss after mini-batch    21: 1.907\n",
      "Loss after mini-batch    31: 5.737\n",
      "Loss after mini-batch    41: 0.736\n",
      "Loss after mini-batch    51: 1.859\n",
      "Loss after mini-batch    61: 0.882\n",
      "Loss after mini-batch    71: 0.303\n",
      "Training Loss: 1.106 \t\t Validation Loss:2.137\n",
      "Starting epoch 21\n",
      "Loss after mini-batch     1: 2.256\n",
      "Loss after mini-batch    11: 3.110\n",
      "Loss after mini-batch    21: 1.246\n",
      "Loss after mini-batch    31: 0.906\n",
      "Loss after mini-batch    41: 3.043\n",
      "Loss after mini-batch    51: 0.194\n",
      "Loss after mini-batch    61: 0.275\n",
      "Loss after mini-batch    71: 4.798\n",
      "Training Loss: 2.399 \t\t Validation Loss:3.871\n",
      "Starting epoch 22\n",
      "Loss after mini-batch     1: 0.508\n",
      "Loss after mini-batch    11: 0.904\n",
      "Loss after mini-batch    21: 0.466\n",
      "Loss after mini-batch    31: 0.791\n",
      "Loss after mini-batch    41: 0.649\n",
      "Loss after mini-batch    51: 1.311\n",
      "Loss after mini-batch    61: 4.318\n",
      "Loss after mini-batch    71: 2.052\n",
      "Training Loss: 0.611 \t\t Validation Loss:2.208\n",
      "Starting epoch 23\n",
      "Loss after mini-batch     1: 0.467\n",
      "Loss after mini-batch    11: 0.229\n",
      "Loss after mini-batch    21: 1.574\n",
      "Loss after mini-batch    31: 0.269\n",
      "Loss after mini-batch    41: 1.133\n",
      "Loss after mini-batch    51: 1.057\n",
      "Loss after mini-batch    61: 1.561\n",
      "Loss after mini-batch    71: 5.495\n",
      "Training Loss: 0.565 \t\t Validation Loss:0.869\n",
      "Starting epoch 24\n",
      "Loss after mini-batch     1: 3.570\n",
      "Loss after mini-batch    11: 1.246\n",
      "Loss after mini-batch    21: 1.578\n",
      "Loss after mini-batch    31: 2.441\n",
      "Loss after mini-batch    41: 2.098\n",
      "Loss after mini-batch    51: 0.442\n",
      "Loss after mini-batch    61: 0.261\n",
      "Loss after mini-batch    71: 0.752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 1.150 \t\t Validation Loss:2.594\n",
      "Starting epoch 25\n",
      "Loss after mini-batch     1: 1.405\n",
      "Loss after mini-batch    11: 1.366\n",
      "Loss after mini-batch    21: 0.439\n",
      "Loss after mini-batch    31: 0.285\n",
      "Loss after mini-batch    41: 0.433\n",
      "Loss after mini-batch    51: 0.388\n",
      "Loss after mini-batch    61: 0.216\n",
      "Loss after mini-batch    71: 0.410\n",
      "Training Loss: 1.031 \t\t Validation Loss:1.338\n",
      "Starting epoch 26\n",
      "Loss after mini-batch     1: 1.628\n",
      "Loss after mini-batch    11: 0.432\n",
      "Loss after mini-batch    21: 3.358\n",
      "Loss after mini-batch    31: 1.280\n",
      "Loss after mini-batch    41: 0.282\n",
      "Loss after mini-batch    51: 0.423\n",
      "Loss after mini-batch    61: 0.249\n",
      "Loss after mini-batch    71: 2.389\n",
      "Training Loss: 4.423 \t\t Validation Loss:5.760\n",
      "Starting epoch 27\n",
      "Loss after mini-batch     1: 0.382\n",
      "Loss after mini-batch    11: 3.510\n",
      "Loss after mini-batch    21: 0.786\n",
      "Loss after mini-batch    31: 1.767\n",
      "Loss after mini-batch    41: 0.996\n",
      "Loss after mini-batch    51: 0.623\n",
      "Loss after mini-batch    61: 1.725\n",
      "Loss after mini-batch    71: 0.793\n",
      "Training Loss: 1.950 \t\t Validation Loss:3.752\n",
      "Starting epoch 28\n",
      "Loss after mini-batch     1: 0.889\n",
      "Loss after mini-batch    11: 0.401\n",
      "Loss after mini-batch    21: 0.098\n",
      "Loss after mini-batch    31: 1.496\n",
      "Loss after mini-batch    41: 0.659\n",
      "Loss after mini-batch    51: 0.828\n",
      "Loss after mini-batch    61: 0.616\n",
      "Loss after mini-batch    71: 0.440\n",
      "Training Loss: 1.335 \t\t Validation Loss:4.193\n",
      "Starting epoch 29\n",
      "Loss after mini-batch     1: 0.601\n",
      "Loss after mini-batch    11: 1.166\n",
      "Loss after mini-batch    21: 2.855\n",
      "Loss after mini-batch    31: 1.222\n",
      "Loss after mini-batch    41: 0.352\n",
      "Loss after mini-batch    51: 1.451\n",
      "Loss after mini-batch    61: 3.220\n",
      "Loss after mini-batch    71: 3.871\n",
      "Training Loss: 1.954 \t\t Validation Loss:3.972\n",
      "Starting epoch 30\n",
      "Loss after mini-batch     1: 0.317\n",
      "Loss after mini-batch    11: 3.263\n",
      "Loss after mini-batch    21: 0.513\n",
      "Loss after mini-batch    31: 1.657\n",
      "Loss after mini-batch    41: 0.475\n",
      "Loss after mini-batch    51: 3.442\n",
      "Loss after mini-batch    61: 2.618\n",
      "Loss after mini-batch    71: 0.481\n",
      "Training Loss: 3.603 \t\t Validation Loss:4.171\n",
      "Starting epoch 31\n",
      "Loss after mini-batch     1: 0.977\n",
      "Loss after mini-batch    11: 0.493\n",
      "Loss after mini-batch    21: 0.146\n",
      "Loss after mini-batch    31: 0.330\n",
      "Loss after mini-batch    41: 0.738\n",
      "Loss after mini-batch    51: 4.677\n",
      "Loss after mini-batch    61: 0.426\n",
      "Loss after mini-batch    71: 0.884\n",
      "Training Loss: 1.213 \t\t Validation Loss:2.548\n",
      "Starting epoch 32\n",
      "Loss after mini-batch     1: 0.194\n",
      "Loss after mini-batch    11: 0.537\n",
      "Loss after mini-batch    21: 0.305\n",
      "Loss after mini-batch    31: 0.077\n",
      "Loss after mini-batch    41: 1.020\n",
      "Loss after mini-batch    51: 0.368\n",
      "Loss after mini-batch    61: 0.781\n",
      "Loss after mini-batch    71: 0.777\n",
      "Training Loss: 0.160 \t\t Validation Loss:0.722\n",
      "Starting epoch 33\n",
      "Loss after mini-batch     1: 0.464\n",
      "Loss after mini-batch    11: 1.773\n",
      "Loss after mini-batch    21: 0.840\n",
      "Loss after mini-batch    31: 0.841\n",
      "Loss after mini-batch    41: 0.387\n",
      "Loss after mini-batch    51: 0.289\n",
      "Loss after mini-batch    61: 1.505\n",
      "Loss after mini-batch    71: 0.615\n",
      "Training Loss: 0.355 \t\t Validation Loss:1.404\n",
      "Starting epoch 34\n",
      "Loss after mini-batch     1: 0.631\n",
      "Loss after mini-batch    11: 0.958\n",
      "Loss after mini-batch    21: 1.535\n",
      "Loss after mini-batch    31: 0.075\n",
      "Loss after mini-batch    41: 0.869\n",
      "Loss after mini-batch    51: 0.455\n",
      "Loss after mini-batch    61: 0.203\n",
      "Loss after mini-batch    71: 0.233\n",
      "Training Loss: 0.419 \t\t Validation Loss:2.973\n",
      "Starting epoch 35\n",
      "Loss after mini-batch     1: 0.123\n",
      "Loss after mini-batch    11: 0.090\n",
      "Loss after mini-batch    21: 0.663\n",
      "Loss after mini-batch    31: 0.544\n",
      "Loss after mini-batch    41: 0.654\n",
      "Loss after mini-batch    51: 0.136\n",
      "Loss after mini-batch    61: 0.223\n",
      "Loss after mini-batch    71: 0.386\n",
      "Training Loss: 0.942 \t\t Validation Loss:1.084\n",
      "Starting epoch 36\n",
      "Loss after mini-batch     1: 0.311\n",
      "Loss after mini-batch    11: 4.094\n",
      "Loss after mini-batch    21: 1.486\n",
      "Loss after mini-batch    31: 6.192\n",
      "Loss after mini-batch    41: 0.246\n",
      "Loss after mini-batch    51: 0.265\n",
      "Loss after mini-batch    61: 0.403\n",
      "Loss after mini-batch    71: 0.148\n",
      "Training Loss: 0.684 \t\t Validation Loss:0.800\n",
      "Starting epoch 37\n",
      "Loss after mini-batch     1: 0.583\n",
      "Loss after mini-batch    11: 0.823\n",
      "Loss after mini-batch    21: 0.246\n",
      "Loss after mini-batch    31: 0.818\n",
      "Loss after mini-batch    41: 0.851\n",
      "Loss after mini-batch    51: 3.909\n",
      "Loss after mini-batch    61: 1.698\n",
      "Loss after mini-batch    71: 0.590\n",
      "Training Loss: 0.310 \t\t Validation Loss:0.534\n",
      "Starting epoch 38\n",
      "Loss after mini-batch     1: 2.719\n",
      "Loss after mini-batch    11: 0.921\n",
      "Loss after mini-batch    21: 0.384\n",
      "Loss after mini-batch    31: 0.726\n",
      "Loss after mini-batch    41: 2.931\n",
      "Loss after mini-batch    51: 1.058\n",
      "Loss after mini-batch    61: 0.094\n",
      "Loss after mini-batch    71: 0.971\n",
      "Training Loss: 0.499 \t\t Validation Loss:1.409\n",
      "Starting epoch 39\n",
      "Loss after mini-batch     1: 0.136\n",
      "Loss after mini-batch    11: 1.443\n",
      "Loss after mini-batch    21: 0.333\n",
      "Loss after mini-batch    31: 0.696\n",
      "Loss after mini-batch    41: 0.886\n",
      "Loss after mini-batch    51: 0.111\n",
      "Loss after mini-batch    61: 0.086\n",
      "Loss after mini-batch    71: 2.920\n",
      "Training Loss: 0.140 \t\t Validation Loss:0.678\n",
      "Starting epoch 40\n",
      "Loss after mini-batch     1: 0.426\n",
      "Loss after mini-batch    11: 0.151\n",
      "Loss after mini-batch    21: 0.949\n",
      "Loss after mini-batch    31: 1.065\n",
      "Loss after mini-batch    41: 0.391\n",
      "Loss after mini-batch    51: 0.264\n",
      "Loss after mini-batch    61: 2.261\n",
      "Loss after mini-batch    71: 0.213\n",
      "Training Loss: 0.880 \t\t Validation Loss:1.852\n",
      "Starting epoch 41\n",
      "Loss after mini-batch     1: 5.300\n",
      "Loss after mini-batch    11: 1.109\n",
      "Loss after mini-batch    21: 0.461\n",
      "Loss after mini-batch    31: 0.428\n",
      "Loss after mini-batch    41: 0.873\n",
      "Loss after mini-batch    51: 0.447\n",
      "Loss after mini-batch    61: 0.116\n",
      "Loss after mini-batch    71: 0.057\n",
      "Training Loss: 0.181 \t\t Validation Loss:2.708\n",
      "Starting epoch 42\n",
      "Loss after mini-batch     1: 1.956\n",
      "Loss after mini-batch    11: 0.469\n",
      "Loss after mini-batch    21: 0.632\n",
      "Loss after mini-batch    31: 0.687\n",
      "Loss after mini-batch    41: 0.320\n",
      "Loss after mini-batch    51: 1.101\n",
      "Loss after mini-batch    61: 0.367\n",
      "Loss after mini-batch    71: 0.307\n",
      "Training Loss: 0.717 \t\t Validation Loss:2.871\n",
      "Starting epoch 43\n",
      "Loss after mini-batch     1: 0.338\n",
      "Loss after mini-batch    11: 0.571\n",
      "Loss after mini-batch    21: 1.133\n",
      "Loss after mini-batch    31: 1.723\n",
      "Loss after mini-batch    41: 0.232\n",
      "Loss after mini-batch    51: 2.737\n",
      "Loss after mini-batch    61: 0.151\n",
      "Loss after mini-batch    71: 0.598\n",
      "Training Loss: 0.251 \t\t Validation Loss:1.847\n",
      "Starting epoch 44\n",
      "Loss after mini-batch     1: 0.137\n",
      "Loss after mini-batch    11: 0.711\n",
      "Loss after mini-batch    21: 0.193\n",
      "Loss after mini-batch    31: 2.380\n",
      "Loss after mini-batch    41: 0.609\n",
      "Loss after mini-batch    51: 0.504\n",
      "Loss after mini-batch    61: 1.199\n",
      "Loss after mini-batch    71: 2.336\n",
      "Training Loss: 0.130 \t\t Validation Loss:0.543\n",
      "Starting epoch 45\n",
      "Loss after mini-batch     1: 0.208\n",
      "Loss after mini-batch    11: 0.139\n",
      "Loss after mini-batch    21: 1.151\n",
      "Loss after mini-batch    31: 0.337\n",
      "Loss after mini-batch    41: 0.140\n",
      "Loss after mini-batch    51: 1.729\n",
      "Loss after mini-batch    61: 1.335\n",
      "Loss after mini-batch    71: 1.718\n",
      "Training Loss: 0.150 \t\t Validation Loss:0.444\n",
      "Starting epoch 46\n",
      "Loss after mini-batch     1: 0.795\n",
      "Loss after mini-batch    11: 0.196\n",
      "Loss after mini-batch    21: 0.624\n",
      "Loss after mini-batch    31: 1.307\n",
      "Loss after mini-batch    41: 0.138\n",
      "Loss after mini-batch    51: 0.100\n",
      "Loss after mini-batch    61: 0.656\n",
      "Loss after mini-batch    71: 0.411\n",
      "Training Loss: 1.182 \t\t Validation Loss:2.443\n",
      "Starting epoch 47\n",
      "Loss after mini-batch     1: 0.409\n",
      "Loss after mini-batch    11: 0.296\n",
      "Loss after mini-batch    21: 0.727\n",
      "Loss after mini-batch    31: 0.572\n",
      "Loss after mini-batch    41: 0.161\n",
      "Loss after mini-batch    51: 0.269\n",
      "Loss after mini-batch    61: 1.111\n",
      "Loss after mini-batch    71: 0.432\n",
      "Training Loss: 1.141 \t\t Validation Loss:1.327\n",
      "Starting epoch 48\n",
      "Loss after mini-batch     1: 0.684\n",
      "Loss after mini-batch    11: 0.751\n",
      "Loss after mini-batch    21: 0.539\n",
      "Loss after mini-batch    31: 0.952\n",
      "Loss after mini-batch    41: 0.441\n",
      "Loss after mini-batch    51: 0.069\n",
      "Loss after mini-batch    61: 0.543\n",
      "Loss after mini-batch    71: 0.342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.216 \t\t Validation Loss:2.468\n",
      "Starting epoch 49\n",
      "Loss after mini-batch     1: 1.944\n",
      "Loss after mini-batch    11: 1.689\n",
      "Loss after mini-batch    21: 0.513\n",
      "Loss after mini-batch    31: 0.895\n",
      "Loss after mini-batch    41: 0.189\n",
      "Loss after mini-batch    51: 0.296\n",
      "Loss after mini-batch    61: 1.280\n",
      "Loss after mini-batch    71: 0.521\n",
      "Training Loss: 0.806 \t\t Validation Loss:1.579\n",
      "Starting epoch 50\n",
      "Loss after mini-batch     1: 0.072\n",
      "Loss after mini-batch    11: 0.089\n",
      "Loss after mini-batch    21: 0.784\n",
      "Loss after mini-batch    31: 0.207\n",
      "Loss after mini-batch    41: 1.018\n",
      "Loss after mini-batch    51: 1.492\n",
      "Loss after mini-batch    61: 0.526\n",
      "Loss after mini-batch    71: 0.804\n",
      "Training Loss: 0.513 \t\t Validation Loss:1.542\n",
      "Starting epoch 51\n",
      "Loss after mini-batch     1: 0.130\n",
      "Loss after mini-batch    11: 0.251\n",
      "Loss after mini-batch    21: 0.776\n",
      "Loss after mini-batch    31: 0.273\n",
      "Loss after mini-batch    41: 0.294\n",
      "Loss after mini-batch    51: 0.556\n",
      "Loss after mini-batch    61: 2.507\n",
      "Loss after mini-batch    71: 2.273\n",
      "Training Loss: 2.869 \t\t Validation Loss:3.757\n",
      "Starting epoch 52\n",
      "Loss after mini-batch     1: 0.226\n",
      "Loss after mini-batch    11: 0.169\n",
      "Loss after mini-batch    21: 0.869\n",
      "Loss after mini-batch    31: 0.336\n",
      "Loss after mini-batch    41: 1.257\n",
      "Loss after mini-batch    51: 0.584\n",
      "Loss after mini-batch    61: 0.686\n",
      "Loss after mini-batch    71: 0.121\n",
      "Training Loss: 0.383 \t\t Validation Loss:0.627\n",
      "Starting epoch 53\n",
      "Loss after mini-batch     1: 2.428\n",
      "Loss after mini-batch    11: 2.284\n",
      "Loss after mini-batch    21: 0.709\n",
      "Loss after mini-batch    31: 0.413\n",
      "Loss after mini-batch    41: 0.954\n",
      "Loss after mini-batch    51: 0.230\n",
      "Loss after mini-batch    61: 0.108\n",
      "Loss after mini-batch    71: 2.058\n",
      "Training Loss: 3.123 \t\t Validation Loss:3.768\n",
      "Starting epoch 54\n",
      "Loss after mini-batch     1: 1.524\n",
      "Loss after mini-batch    11: 1.132\n",
      "Loss after mini-batch    21: 0.540\n",
      "Loss after mini-batch    31: 0.187\n",
      "Loss after mini-batch    41: 0.306\n",
      "Loss after mini-batch    51: 0.291\n",
      "Loss after mini-batch    61: 0.670\n",
      "Loss after mini-batch    71: 0.626\n",
      "Training Loss: 0.214 \t\t Validation Loss:1.970\n",
      "Starting epoch 55\n",
      "Loss after mini-batch     1: 0.495\n",
      "Loss after mini-batch    11: 0.126\n",
      "Loss after mini-batch    21: 0.810\n",
      "Loss after mini-batch    31: 0.270\n",
      "Loss after mini-batch    41: 1.468\n",
      "Loss after mini-batch    51: 0.753\n",
      "Loss after mini-batch    61: 0.176\n",
      "Loss after mini-batch    71: 0.303\n",
      "Training Loss: 0.236 \t\t Validation Loss:0.411\n",
      "Starting epoch 56\n",
      "Loss after mini-batch     1: 0.771\n",
      "Loss after mini-batch    11: 0.675\n",
      "Loss after mini-batch    21: 1.950\n",
      "Loss after mini-batch    31: 0.796\n",
      "Loss after mini-batch    41: 1.003\n",
      "Loss after mini-batch    51: 0.639\n",
      "Loss after mini-batch    61: 0.111\n",
      "Loss after mini-batch    71: 2.087\n",
      "Training Loss: 0.279 \t\t Validation Loss:1.610\n",
      "Starting epoch 57\n",
      "Loss after mini-batch     1: 0.365\n",
      "Loss after mini-batch    11: 0.929\n",
      "Loss after mini-batch    21: 0.748\n",
      "Loss after mini-batch    31: 0.090\n",
      "Loss after mini-batch    41: 0.226\n",
      "Loss after mini-batch    51: 0.685\n",
      "Loss after mini-batch    61: 0.380\n",
      "Loss after mini-batch    71: 0.100\n",
      "Training Loss: 2.226 \t\t Validation Loss:3.098\n",
      "Starting epoch 58\n",
      "Loss after mini-batch     1: 2.149\n",
      "Loss after mini-batch    11: 0.046\n",
      "Loss after mini-batch    21: 0.982\n",
      "Loss after mini-batch    31: 0.537\n",
      "Loss after mini-batch    41: 0.471\n",
      "Loss after mini-batch    51: 1.943\n",
      "Loss after mini-batch    61: 0.164\n",
      "Loss after mini-batch    71: 1.052\n",
      "Training Loss: 0.391 \t\t Validation Loss:1.001\n",
      "Starting epoch 59\n",
      "Loss after mini-batch     1: 1.014\n",
      "Loss after mini-batch    11: 0.671\n",
      "Loss after mini-batch    21: 0.407\n",
      "Loss after mini-batch    31: 0.452\n",
      "Loss after mini-batch    41: 0.270\n",
      "Loss after mini-batch    51: 1.370\n",
      "Loss after mini-batch    61: 0.541\n",
      "Loss after mini-batch    71: 2.883\n",
      "Training Loss: 0.440 \t\t Validation Loss:1.461\n",
      "Starting epoch 60\n",
      "Loss after mini-batch     1: 1.272\n",
      "Loss after mini-batch    11: 0.251\n",
      "Loss after mini-batch    21: 2.288\n",
      "Loss after mini-batch    31: 0.953\n",
      "Loss after mini-batch    41: 0.222\n",
      "Loss after mini-batch    51: 0.366\n",
      "Loss after mini-batch    61: 1.112\n",
      "Loss after mini-batch    71: 0.097\n",
      "Training Loss: 0.197 \t\t Validation Loss:0.953\n",
      "Starting epoch 61\n",
      "Loss after mini-batch     1: 0.275\n",
      "Loss after mini-batch    11: 0.663\n",
      "Loss after mini-batch    21: 0.320\n",
      "Loss after mini-batch    31: 0.549\n",
      "Loss after mini-batch    41: 0.114\n",
      "Loss after mini-batch    51: 0.349\n",
      "Loss after mini-batch    61: 2.112\n",
      "Loss after mini-batch    71: 0.602\n",
      "Training Loss: 0.417 \t\t Validation Loss:1.241\n",
      "Starting epoch 62\n",
      "Loss after mini-batch     1: 0.083\n",
      "Loss after mini-batch    11: 0.740\n",
      "Loss after mini-batch    21: 1.620\n",
      "Loss after mini-batch    31: 1.166\n",
      "Loss after mini-batch    41: 1.281\n",
      "Loss after mini-batch    51: 0.096\n",
      "Loss after mini-batch    61: 0.206\n",
      "Loss after mini-batch    71: 0.497\n",
      "Training Loss: 0.547 \t\t Validation Loss:0.706\n",
      "Starting epoch 63\n",
      "Loss after mini-batch     1: 0.346\n",
      "Loss after mini-batch    11: 0.214\n",
      "Loss after mini-batch    21: 0.205\n",
      "Loss after mini-batch    31: 0.591\n",
      "Loss after mini-batch    41: 0.091\n",
      "Loss after mini-batch    51: 0.203\n",
      "Loss after mini-batch    61: 0.785\n",
      "Loss after mini-batch    71: 0.884\n",
      "Training Loss: 0.224 \t\t Validation Loss:0.973\n",
      "Starting epoch 64\n",
      "Loss after mini-batch     1: 0.592\n",
      "Loss after mini-batch    11: 1.080\n",
      "Loss after mini-batch    21: 0.270\n",
      "Loss after mini-batch    31: 0.076\n",
      "Loss after mini-batch    41: 0.268\n",
      "Loss after mini-batch    51: 1.918\n",
      "Loss after mini-batch    61: 0.194\n",
      "Loss after mini-batch    71: 0.515\n",
      "Training Loss: 0.586 \t\t Validation Loss:1.486\n",
      "Starting epoch 65\n",
      "Loss after mini-batch     1: 0.157\n",
      "Loss after mini-batch    11: 0.073\n",
      "Loss after mini-batch    21: 0.415\n",
      "Loss after mini-batch    31: 0.119\n",
      "Loss after mini-batch    41: 0.604\n",
      "Loss after mini-batch    51: 1.559\n",
      "Loss after mini-batch    61: 0.369\n",
      "Loss after mini-batch    71: 1.838\n",
      "Training Loss: 0.183 \t\t Validation Loss:0.764\n",
      "Starting epoch 66\n",
      "Loss after mini-batch     1: 0.317\n",
      "Loss after mini-batch    11: 0.413\n",
      "Loss after mini-batch    21: 0.508\n",
      "Loss after mini-batch    31: 0.257\n",
      "Loss after mini-batch    41: 0.299\n",
      "Loss after mini-batch    51: 0.196\n",
      "Loss after mini-batch    61: 1.233\n",
      "Loss after mini-batch    71: 0.111\n",
      "Training Loss: 0.132 \t\t Validation Loss:0.871\n",
      "Starting epoch 67\n",
      "Loss after mini-batch     1: 0.608\n",
      "Loss after mini-batch    11: 0.147\n",
      "Loss after mini-batch    21: 0.909\n",
      "Loss after mini-batch    31: 0.139\n",
      "Loss after mini-batch    41: 0.182\n",
      "Loss after mini-batch    51: 0.429\n",
      "Loss after mini-batch    61: 0.079\n",
      "Loss after mini-batch    71: 0.787\n",
      "Training Loss: 0.059 \t\t Validation Loss:1.679\n",
      "Starting epoch 68\n",
      "Loss after mini-batch     1: 0.080\n",
      "Loss after mini-batch    11: 0.083\n",
      "Loss after mini-batch    21: 0.144\n",
      "Loss after mini-batch    31: 0.389\n",
      "Loss after mini-batch    41: 0.698\n",
      "Loss after mini-batch    51: 1.578\n",
      "Loss after mini-batch    61: 0.062\n",
      "Loss after mini-batch    71: 0.268\n",
      "Training Loss: 0.131 \t\t Validation Loss:0.894\n",
      "Starting epoch 69\n",
      "Loss after mini-batch     1: 1.618\n",
      "Loss after mini-batch    11: 0.250\n",
      "Loss after mini-batch    21: 0.108\n",
      "Loss after mini-batch    31: 0.133\n",
      "Loss after mini-batch    41: 1.013\n",
      "Loss after mini-batch    51: 0.095\n",
      "Loss after mini-batch    61: 0.623\n",
      "Loss after mini-batch    71: 0.077\n",
      "Training Loss: 2.236 \t\t Validation Loss:3.468\n",
      "Starting epoch 70\n",
      "Loss after mini-batch     1: 0.804\n",
      "Loss after mini-batch    11: 0.288\n",
      "Loss after mini-batch    21: 0.488\n",
      "Loss after mini-batch    31: 0.149\n",
      "Loss after mini-batch    41: 0.920\n",
      "Loss after mini-batch    51: 0.689\n",
      "Loss after mini-batch    61: 0.032\n",
      "Loss after mini-batch    71: 0.134\n",
      "Training Loss: 0.159 \t\t Validation Loss:0.504\n",
      "Starting epoch 71\n",
      "Loss after mini-batch     1: 0.135\n",
      "Loss after mini-batch    11: 2.631\n",
      "Loss after mini-batch    21: 0.168\n",
      "Loss after mini-batch    31: 1.432\n",
      "Loss after mini-batch    41: 0.308\n",
      "Loss after mini-batch    51: 0.181\n",
      "Loss after mini-batch    61: 0.225\n",
      "Loss after mini-batch    71: 0.263\n",
      "Training Loss: 0.350 \t\t Validation Loss:1.418\n",
      "Starting epoch 72\n",
      "Loss after mini-batch     1: 0.066\n",
      "Loss after mini-batch    11: 0.215\n",
      "Loss after mini-batch    21: 0.081\n",
      "Loss after mini-batch    31: 0.327\n",
      "Loss after mini-batch    41: 0.145\n",
      "Loss after mini-batch    51: 0.049\n",
      "Loss after mini-batch    61: 0.220\n",
      "Loss after mini-batch    71: 2.633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.088 \t\t Validation Loss:0.268\n",
      "Starting epoch 73\n",
      "Loss after mini-batch     1: 1.580\n",
      "Loss after mini-batch    11: 0.213\n",
      "Loss after mini-batch    21: 0.283\n",
      "Loss after mini-batch    31: 1.011\n",
      "Loss after mini-batch    41: 0.287\n",
      "Loss after mini-batch    51: 0.342\n",
      "Loss after mini-batch    61: 0.197\n",
      "Loss after mini-batch    71: 0.089\n",
      "Training Loss: 0.069 \t\t Validation Loss:0.735\n",
      "Starting epoch 74\n",
      "Loss after mini-batch     1: 0.142\n",
      "Loss after mini-batch    11: 0.390\n",
      "Loss after mini-batch    21: 0.164\n",
      "Loss after mini-batch    31: 0.023\n",
      "Loss after mini-batch    41: 0.302\n",
      "Loss after mini-batch    51: 0.035\n",
      "Loss after mini-batch    61: 0.042\n",
      "Loss after mini-batch    71: 0.221\n",
      "Training Loss: 0.272 \t\t Validation Loss:0.741\n",
      "Starting epoch 75\n",
      "Loss after mini-batch     1: 0.121\n",
      "Loss after mini-batch    11: 0.114\n",
      "Loss after mini-batch    21: 2.212\n",
      "Loss after mini-batch    31: 0.264\n",
      "Loss after mini-batch    41: 1.037\n",
      "Loss after mini-batch    51: 1.566\n",
      "Loss after mini-batch    61: 0.176\n",
      "Loss after mini-batch    71: 0.567\n",
      "Training Loss: 0.336 \t\t Validation Loss:0.860\n",
      "Starting epoch 76\n",
      "Loss after mini-batch     1: 0.196\n",
      "Loss after mini-batch    11: 1.296\n",
      "Loss after mini-batch    21: 0.274\n",
      "Loss after mini-batch    31: 0.634\n",
      "Loss after mini-batch    41: 0.065\n",
      "Loss after mini-batch    51: 3.971\n",
      "Loss after mini-batch    61: 0.536\n",
      "Loss after mini-batch    71: 0.995\n",
      "Training Loss: 1.276 \t\t Validation Loss:1.359\n",
      "Starting epoch 77\n",
      "Loss after mini-batch     1: 1.101\n",
      "Loss after mini-batch    11: 0.094\n",
      "Loss after mini-batch    21: 0.317\n",
      "Loss after mini-batch    31: 0.192\n",
      "Loss after mini-batch    41: 0.178\n",
      "Loss after mini-batch    51: 0.330\n",
      "Loss after mini-batch    61: 0.026\n",
      "Loss after mini-batch    71: 0.072\n",
      "Training Loss: 2.215 \t\t Validation Loss:2.347\n",
      "Starting epoch 78\n",
      "Loss after mini-batch     1: 0.752\n",
      "Loss after mini-batch    11: 0.151\n",
      "Loss after mini-batch    21: 0.553\n",
      "Loss after mini-batch    31: 0.339\n",
      "Loss after mini-batch    41: 0.945\n",
      "Loss after mini-batch    51: 0.314\n",
      "Loss after mini-batch    61: 1.351\n",
      "Loss after mini-batch    71: 0.149\n",
      "Training Loss: 0.168 \t\t Validation Loss:0.481\n",
      "Starting epoch 79\n",
      "Loss after mini-batch     1: 0.166\n",
      "Loss after mini-batch    11: 2.094\n",
      "Loss after mini-batch    21: 0.040\n",
      "Loss after mini-batch    31: 0.210\n",
      "Loss after mini-batch    41: 0.717\n",
      "Loss after mini-batch    51: 0.077\n",
      "Loss after mini-batch    61: 0.116\n",
      "Loss after mini-batch    71: 0.488\n",
      "Training Loss: 0.243 \t\t Validation Loss:0.596\n",
      "Starting epoch 80\n",
      "Loss after mini-batch     1: 0.153\n",
      "Loss after mini-batch    11: 0.391\n",
      "Loss after mini-batch    21: 0.137\n",
      "Loss after mini-batch    31: 0.177\n",
      "Loss after mini-batch    41: 0.522\n",
      "Loss after mini-batch    51: 0.031\n",
      "Loss after mini-batch    61: 0.069\n",
      "Loss after mini-batch    71: 1.309\n",
      "Training Loss: 0.381 \t\t Validation Loss:1.416\n",
      "Starting epoch 81\n",
      "Loss after mini-batch     1: 0.637\n",
      "Loss after mini-batch    11: 0.174\n",
      "Loss after mini-batch    21: 0.104\n",
      "Loss after mini-batch    31: 0.150\n",
      "Loss after mini-batch    41: 0.317\n",
      "Loss after mini-batch    51: 0.800\n",
      "Loss after mini-batch    61: 0.126\n",
      "Loss after mini-batch    71: 0.268\n",
      "Training Loss: 0.173 \t\t Validation Loss:0.666\n",
      "Starting epoch 82\n",
      "Loss after mini-batch     1: 0.746\n",
      "Loss after mini-batch    11: 0.311\n",
      "Loss after mini-batch    21: 0.110\n",
      "Loss after mini-batch    31: 0.126\n",
      "Loss after mini-batch    41: 0.214\n",
      "Loss after mini-batch    51: 0.434\n",
      "Loss after mini-batch    61: 0.230\n",
      "Loss after mini-batch    71: 0.429\n",
      "Training Loss: 0.157 \t\t Validation Loss:0.796\n",
      "Starting epoch 83\n",
      "Loss after mini-batch     1: 0.523\n",
      "Loss after mini-batch    11: 0.302\n",
      "Loss after mini-batch    21: 0.120\n",
      "Loss after mini-batch    31: 0.068\n",
      "Loss after mini-batch    41: 0.062\n",
      "Loss after mini-batch    51: 0.033\n",
      "Loss after mini-batch    61: 0.069\n",
      "Loss after mini-batch    71: 0.519\n",
      "Training Loss: 0.149 \t\t Validation Loss:0.206\n",
      "Starting epoch 84\n",
      "Loss after mini-batch     1: 0.227\n",
      "Loss after mini-batch    11: 0.643\n",
      "Loss after mini-batch    21: 0.746\n",
      "Loss after mini-batch    31: 0.083\n",
      "Loss after mini-batch    41: 0.114\n",
      "Loss after mini-batch    51: 0.473\n",
      "Loss after mini-batch    61: 0.150\n",
      "Loss after mini-batch    71: 0.062\n",
      "Training Loss: 0.288 \t\t Validation Loss:1.069\n",
      "Starting epoch 85\n",
      "Loss after mini-batch     1: 0.144\n",
      "Loss after mini-batch    11: 0.206\n",
      "Loss after mini-batch    21: 0.497\n",
      "Loss after mini-batch    31: 0.419\n",
      "Loss after mini-batch    41: 0.290\n",
      "Loss after mini-batch    51: 0.132\n",
      "Loss after mini-batch    61: 0.812\n",
      "Loss after mini-batch    71: 1.105\n",
      "Training Loss: 0.134 \t\t Validation Loss:1.179\n",
      "Starting epoch 86\n",
      "Loss after mini-batch     1: 0.236\n",
      "Loss after mini-batch    11: 0.110\n",
      "Loss after mini-batch    21: 0.238\n",
      "Loss after mini-batch    31: 0.202\n",
      "Loss after mini-batch    41: 0.230\n",
      "Loss after mini-batch    51: 0.694\n",
      "Loss after mini-batch    61: 0.346\n",
      "Loss after mini-batch    71: 1.368\n",
      "Training Loss: 0.104 \t\t Validation Loss:0.986\n",
      "Starting epoch 87\n",
      "Loss after mini-batch     1: 0.098\n",
      "Loss after mini-batch    11: 0.406\n",
      "Loss after mini-batch    21: 0.116\n",
      "Loss after mini-batch    31: 0.614\n",
      "Loss after mini-batch    41: 0.153\n",
      "Loss after mini-batch    51: 1.065\n",
      "Loss after mini-batch    61: 1.153\n",
      "Loss after mini-batch    71: 1.521\n",
      "Training Loss: 1.503 \t\t Validation Loss:2.279\n",
      "Starting epoch 88\n",
      "Loss after mini-batch     1: 0.579\n",
      "Loss after mini-batch    11: 0.077\n",
      "Loss after mini-batch    21: 1.881\n",
      "Loss after mini-batch    31: 0.244\n",
      "Loss after mini-batch    41: 1.361\n",
      "Loss after mini-batch    51: 0.092\n",
      "Loss after mini-batch    61: 0.567\n",
      "Loss after mini-batch    71: 0.881\n",
      "Training Loss: 0.121 \t\t Validation Loss:0.835\n",
      "Starting epoch 89\n",
      "Loss after mini-batch     1: 1.898\n",
      "Loss after mini-batch    11: 0.064\n",
      "Loss after mini-batch    21: 0.062\n",
      "Loss after mini-batch    31: 0.830\n",
      "Loss after mini-batch    41: 0.156\n",
      "Loss after mini-batch    51: 0.218\n",
      "Loss after mini-batch    61: 0.108\n",
      "Loss after mini-batch    71: 0.053\n",
      "Training Loss: 0.707 \t\t Validation Loss:2.384\n",
      "Starting epoch 90\n",
      "Loss after mini-batch     1: 1.313\n",
      "Loss after mini-batch    11: 0.156\n",
      "Loss after mini-batch    21: 0.927\n",
      "Loss after mini-batch    31: 0.093\n",
      "Loss after mini-batch    41: 0.102\n",
      "Loss after mini-batch    51: 0.539\n",
      "Loss after mini-batch    61: 0.286\n",
      "Loss after mini-batch    71: 0.958\n",
      "Training Loss: 0.101 \t\t Validation Loss:0.197\n",
      "Starting epoch 91\n",
      "Loss after mini-batch     1: 0.147\n",
      "Loss after mini-batch    11: 0.151\n",
      "Loss after mini-batch    21: 0.928\n",
      "Loss after mini-batch    31: 0.213\n",
      "Loss after mini-batch    41: 1.718\n",
      "Loss after mini-batch    51: 0.131\n",
      "Loss after mini-batch    61: 0.460\n",
      "Loss after mini-batch    71: 0.112\n",
      "Training Loss: 0.037 \t\t Validation Loss:0.089\n",
      "Starting epoch 92\n",
      "Loss after mini-batch     1: 0.914\n",
      "Loss after mini-batch    11: 0.286\n",
      "Loss after mini-batch    21: 0.303\n",
      "Loss after mini-batch    31: 0.271\n",
      "Loss after mini-batch    41: 0.092\n",
      "Loss after mini-batch    51: 0.109\n",
      "Loss after mini-batch    61: 0.505\n",
      "Loss after mini-batch    71: 1.536\n",
      "Training Loss: 0.754 \t\t Validation Loss:2.623\n",
      "Starting epoch 93\n",
      "Loss after mini-batch     1: 0.144\n",
      "Loss after mini-batch    11: 0.498\n",
      "Loss after mini-batch    21: 0.055\n",
      "Loss after mini-batch    31: 0.752\n",
      "Loss after mini-batch    41: 0.319\n",
      "Loss after mini-batch    51: 0.142\n",
      "Loss after mini-batch    61: 0.079\n",
      "Loss after mini-batch    71: 1.300\n",
      "Training Loss: 0.025 \t\t Validation Loss:0.330\n",
      "Starting epoch 94\n",
      "Loss after mini-batch     1: 0.499\n",
      "Loss after mini-batch    11: 2.590\n",
      "Loss after mini-batch    21: 0.476\n",
      "Loss after mini-batch    31: 0.073\n",
      "Loss after mini-batch    41: 0.538\n",
      "Loss after mini-batch    51: 0.479\n",
      "Loss after mini-batch    61: 1.489\n",
      "Loss after mini-batch    71: 2.401\n",
      "Training Loss: 0.066 \t\t Validation Loss:0.392\n",
      "Starting epoch 95\n",
      "Loss after mini-batch     1: 0.071\n",
      "Loss after mini-batch    11: 0.285\n",
      "Loss after mini-batch    21: 0.589\n",
      "Loss after mini-batch    31: 0.227\n",
      "Loss after mini-batch    41: 0.063\n",
      "Loss after mini-batch    51: 0.305\n",
      "Loss after mini-batch    61: 0.505\n",
      "Loss after mini-batch    71: 0.296\n",
      "Training Loss: 2.293 \t\t Validation Loss:2.656\n",
      "Starting epoch 96\n",
      "Loss after mini-batch     1: 0.543\n",
      "Loss after mini-batch    11: 0.123\n",
      "Loss after mini-batch    21: 0.110\n",
      "Loss after mini-batch    31: 0.579\n",
      "Loss after mini-batch    41: 0.369\n",
      "Loss after mini-batch    51: 1.516\n",
      "Loss after mini-batch    61: 0.195\n",
      "Loss after mini-batch    71: 0.977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.213 \t\t Validation Loss:2.330\n",
      "Starting epoch 97\n",
      "Loss after mini-batch     1: 0.088\n",
      "Loss after mini-batch    11: 1.307\n",
      "Loss after mini-batch    21: 0.309\n",
      "Loss after mini-batch    31: 0.565\n",
      "Loss after mini-batch    41: 1.141\n",
      "Loss after mini-batch    51: 0.067\n",
      "Loss after mini-batch    61: 0.272\n",
      "Loss after mini-batch    71: 0.753\n",
      "Training Loss: 0.179 \t\t Validation Loss:0.718\n",
      "Starting epoch 98\n",
      "Loss after mini-batch     1: 0.583\n",
      "Loss after mini-batch    11: 0.378\n",
      "Loss after mini-batch    21: 0.135\n",
      "Loss after mini-batch    31: 0.777\n",
      "Loss after mini-batch    41: 0.690\n",
      "Loss after mini-batch    51: 0.035\n",
      "Loss after mini-batch    61: 0.123\n",
      "Loss after mini-batch    71: 0.060\n",
      "Training Loss: 0.045 \t\t Validation Loss:0.355\n",
      "Starting epoch 99\n",
      "Loss after mini-batch     1: 0.844\n",
      "Loss after mini-batch    11: 0.103\n",
      "Loss after mini-batch    21: 0.154\n",
      "Loss after mini-batch    31: 0.213\n",
      "Loss after mini-batch    41: 0.084\n",
      "Loss after mini-batch    51: 0.020\n",
      "Loss after mini-batch    61: 0.507\n",
      "Loss after mini-batch    71: 0.643\n",
      "Training Loss: 1.409 \t\t Validation Loss:1.816\n",
      "Starting epoch 100\n",
      "Loss after mini-batch     1: 0.047\n",
      "Loss after mini-batch    11: 0.104\n",
      "Loss after mini-batch    21: 0.190\n",
      "Loss after mini-batch    31: 0.540\n",
      "Loss after mini-batch    41: 1.102\n",
      "Loss after mini-batch    51: 0.187\n",
      "Loss after mini-batch    61: 2.387\n",
      "Loss after mini-batch    71: 0.389\n",
      "Training Loss: 1.263 \t\t Validation Loss:1.629\n",
      "Training process has finished.\n"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "history_train = np.empty((1,))\n",
    "history_val = np.empty((1,))\n",
    "for epoch in range(0, 100): # 5 epochs at maximum  \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "          # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "#                 (i + 1, current_loss / 500))\n",
    "                  (i + 1, loss.item()))\n",
    "            current_loss = 0.0\n",
    "    history_train = np.append(history_train, current_loss)\n",
    "\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    mlp.eval()     # Optional when not using Model Specific layer\n",
    "    for i, data in enumerate(validloader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        output_val = mlp(inputs)\n",
    "        valid_loss = loss_function(output_val, targets)\n",
    "    \n",
    "        valid_loss += loss.item()\n",
    "    history_val = np.append(history_val, valid_loss.item())\n",
    "    print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "         '{:.3f}'.format(loss.item(), valid_loss.item()))\n",
    "#     print('Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "#           '{:.3f}'.format(current_loss / len(trainloader), valid_loss / len(validloader)))\n",
    "#     if min_valid_loss > valid_loss:\n",
    "#         print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "#         min_valid_loss = valid_loss\n",
    "#         # Saving State Dict\n",
    "#         torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7b9d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.random.randn(13)\n",
    "# torch usa tensores de torch y no numpy.darrays\n",
    "dtype = torch.float\n",
    "test = torch.randn((1, 3), device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3332d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62c6a43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.862593650817871"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27fc4848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0, 0], X_test[0]\n",
    "# xtest = [x[0] for x in X_test]\n",
    "ypred = [y[0].item() for y in y_pred]\n",
    "ytest = [y[0].item() for y in y_test]\n",
    "diff=np.array(ytest)-np.array(ypred)\n",
    "len(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87b7e8ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 4.5)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diff,bins=100)\n",
    "plt.xscale('log')\n",
    "plt.ylim(0,4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f558b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb184108a00>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ep = np.arange(0, 101)\n",
    "plt.plot(ep, history_train, label='train')\n",
    "plt.plot(ep, history_val, label='val')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1355f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58b48c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((101,), (101,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(history_train), np.shape(history_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4744941b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 60.0)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(ytest, bins=len(ytest), color='red')\n",
    "# plt.hist(ypred, bins=len(ypred), color='green')\n",
    "plt.ylim(0,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a45db1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)==len(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8a07e1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f802e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
