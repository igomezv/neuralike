{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1bae12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee35f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17fd99c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Om\t\t\t\\Omega_m\n",
    "# Obh2\t\t\t\\Omega_{b}h^2\n",
    "# h\t\n",
    "datafile = 'chains/LCDM_phy_HD_nested_dynesty_multi_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc69788b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadDataSet(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Prepare the dataset for regression\n",
    "    '''\n",
    "    def __init__(self, X, y, scale_data=False):\n",
    "        if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "            # Apply scaling if necessary\n",
    "            if scale_data:\n",
    "                X = StandardScaler().fit_transform(X)\n",
    "            self.X = torch.from_numpy(X)\n",
    "            self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c80912",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer Perceptron for regression.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ncols = 3\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(ncols, 200),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 100),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(200, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "          Forward pass\n",
    "        '''\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e24b9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1036, 3) (1036, 1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Set fixed random number seed\n",
    "    torch.manual_seed(42)\n",
    "  \n",
    "    # Load Boston dataset\n",
    "    X = np.loadtxt(datafile, usecols=(2,3,4))\n",
    "    y = np.loadtxt(datafile, usecols=1).reshape(-1, 1)\n",
    "    randomize = np.random.permutation(len(X))\n",
    "    X = X[randomize]\n",
    "    y = y[randomize]\n",
    "    print(np.shape(X), np.shape(y))\n",
    "    X_test, y_test = X[:100, :], y[:100, :]\n",
    "    X, y = X[100:, :], y[100:, :]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f173672",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = LoadDataSet(X_train, y_train)\n",
    "dataset_val = LoadDataSet(X_val, y_val)\n",
    "# dataset_test = LoadDataSet(X_test, y_test)\n",
    "X_test, y_test = torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62b89106",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(dataset_train, batch_size=10, shuffle=True, num_workers=1)\n",
    "validloader = torch.utils.data.DataLoader(dataset_val, batch_size=10, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae205483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 19.1538],\n",
       "        [  7.3417],\n",
       "        [  7.6976],\n",
       "        [ 26.1786],\n",
       "        [  7.3291],\n",
       "        [  7.2617],\n",
       "        [  7.2828],\n",
       "        [ 10.3391],\n",
       "        [  8.0182],\n",
       "        [  7.2940],\n",
       "        [  7.3562],\n",
       "        [  7.5132],\n",
       "        [  7.3282],\n",
       "        [ 11.3084],\n",
       "        [ 12.0782],\n",
       "        [  7.8405],\n",
       "        [  7.3650],\n",
       "        [ 13.7270],\n",
       "        [  8.8290],\n",
       "        [  7.4780],\n",
       "        [ 30.0162],\n",
       "        [  7.3157],\n",
       "        [ 50.3997],\n",
       "        [  7.9663],\n",
       "        [  7.9109],\n",
       "        [  9.7131],\n",
       "        [ 27.7799],\n",
       "        [ 17.1714],\n",
       "        [  7.3262],\n",
       "        [  8.6780],\n",
       "        [  7.2915],\n",
       "        [ 67.9956],\n",
       "        [  7.5308],\n",
       "        [  9.6618],\n",
       "        [  7.3285],\n",
       "        [  7.2830],\n",
       "        [  7.3393],\n",
       "        [  7.4748],\n",
       "        [  7.4181],\n",
       "        [  8.9489],\n",
       "        [  7.5656],\n",
       "        [  8.9591],\n",
       "        [  7.4192],\n",
       "        [  7.3015],\n",
       "        [  7.4124],\n",
       "        [  8.1102],\n",
       "        [  8.1995],\n",
       "        [ 17.5000],\n",
       "        [  8.8137],\n",
       "        [  7.2790],\n",
       "        [ 17.2627],\n",
       "        [  7.6168],\n",
       "        [ 14.8048],\n",
       "        [  7.3822],\n",
       "        [ 27.2267],\n",
       "        [ 29.5540],\n",
       "        [  7.5283],\n",
       "        [ 14.2183],\n",
       "        [ 17.0660],\n",
       "        [ 21.0750],\n",
       "        [ 10.1089],\n",
       "        [ 10.5984],\n",
       "        [ 15.4048],\n",
       "        [  7.2515],\n",
       "        [ 16.5463],\n",
       "        [  7.6949],\n",
       "        [  8.0643],\n",
       "        [ 63.8599],\n",
       "        [  7.4708],\n",
       "        [ 33.7616],\n",
       "        [ 92.9268],\n",
       "        [ 29.3597],\n",
       "        [ 24.8698],\n",
       "        [  7.9636],\n",
       "        [ 15.1175],\n",
       "        [  7.6568],\n",
       "        [  7.4689],\n",
       "        [  7.3512],\n",
       "        [ 11.6595],\n",
       "        [  7.7491],\n",
       "        [ 11.6360],\n",
       "        [  7.3144],\n",
       "        [  7.2867],\n",
       "        [  7.3846],\n",
       "        [  7.3471],\n",
       "        [ 11.7705],\n",
       "        [  7.2698],\n",
       "        [  7.5474],\n",
       "        [ 13.6137],\n",
       "        [  7.3590],\n",
       "        [ 16.3585],\n",
       "        [ 71.7131],\n",
       "        [  7.4866],\n",
       "        [  7.3345],\n",
       "        [  8.9638],\n",
       "        [  7.2623],\n",
       "        [  7.2602],\n",
       "        [ 46.6430],\n",
       "        [  7.2820],\n",
       "        [  7.2576],\n",
       "        [  7.3683],\n",
       "        [  7.3190],\n",
       "        [ 13.9170],\n",
       "        [  7.3299],\n",
       "        [ 35.7841],\n",
       "        [ 37.5503],\n",
       "        [  7.2811],\n",
       "        [  7.3371],\n",
       "        [ 10.4242],\n",
       "        [ 16.8435],\n",
       "        [ 44.1189],\n",
       "        [ 23.1077],\n",
       "        [  9.7048],\n",
       "        [  7.9341],\n",
       "        [ 11.7181],\n",
       "        [  7.4430],\n",
       "        [  7.4195],\n",
       "        [  7.8385],\n",
       "        [ 13.0160],\n",
       "        [  7.5524],\n",
       "        [ 22.2853],\n",
       "        [  8.0601],\n",
       "        [  7.3135],\n",
       "        [  7.3588],\n",
       "        [  8.0542],\n",
       "        [ 13.7575],\n",
       "        [ 30.3654],\n",
       "        [  7.9084],\n",
       "        [  7.5895],\n",
       "        [  7.8411],\n",
       "        [  7.3935],\n",
       "        [102.7997],\n",
       "        [ 64.7686],\n",
       "        [ 13.3542],\n",
       "        [  7.5989],\n",
       "        [ 13.6712],\n",
       "        [  8.7532],\n",
       "        [  7.9675],\n",
       "        [  7.3221],\n",
       "        [  7.3613],\n",
       "        [  7.2805],\n",
       "        [  7.3213],\n",
       "        [ 17.1585],\n",
       "        [  8.7642],\n",
       "        [  7.4461],\n",
       "        [  8.4234],\n",
       "        [ 12.3405],\n",
       "        [ 12.9242],\n",
       "        [  7.6144],\n",
       "        [  7.4881],\n",
       "        [ 13.2993],\n",
       "        [  7.3497],\n",
       "        [  7.2726],\n",
       "        [  7.5857],\n",
       "        [ 11.1686],\n",
       "        [  9.1608],\n",
       "        [  7.2892],\n",
       "        [ 13.4144],\n",
       "        [ 17.4292],\n",
       "        [  7.3395],\n",
       "        [  8.7355],\n",
       "        [  7.3461],\n",
       "        [  8.0245],\n",
       "        [ 15.1399],\n",
       "        [  7.9317],\n",
       "        [  7.3692],\n",
       "        [  7.9845],\n",
       "        [ 11.3994],\n",
       "        [  7.3900],\n",
       "        [  9.0712],\n",
       "        [  7.7940],\n",
       "        [ 11.1071],\n",
       "        [  8.4502],\n",
       "        [ 12.8739],\n",
       "        [  7.6503],\n",
       "        [  7.7145],\n",
       "        [ 38.4508],\n",
       "        [ 23.5017],\n",
       "        [  7.2725],\n",
       "        [  7.2915],\n",
       "        [  7.9653],\n",
       "        [ 20.6751],\n",
       "        [  8.9274],\n",
       "        [  8.3880],\n",
       "        [  9.2267],\n",
       "        [  9.1802],\n",
       "        [  8.1964],\n",
       "        [ 10.8861],\n",
       "        [ 10.2863],\n",
       "        [ 15.0049],\n",
       "        [  7.6039],\n",
       "        [  7.4910],\n",
       "        [  7.7154],\n",
       "        [  7.3424],\n",
       "        [  7.3797],\n",
       "        [ 30.4809],\n",
       "        [  7.2748],\n",
       "        [  7.3885],\n",
       "        [ 26.2732],\n",
       "        [  7.3588],\n",
       "        [ 22.6010],\n",
       "        [  7.3065],\n",
       "        [  8.3508],\n",
       "        [ 22.2064],\n",
       "        [  7.3762],\n",
       "        [  9.6423],\n",
       "        [  7.6161],\n",
       "        [ 10.1839],\n",
       "        [ 10.8927],\n",
       "        [ 42.3802],\n",
       "        [ 15.0424],\n",
       "        [  7.2779],\n",
       "        [  7.5756],\n",
       "        [  7.6695],\n",
       "        [  8.1756],\n",
       "        [  7.3669],\n",
       "        [  7.3611],\n",
       "        [  7.3014],\n",
       "        [ 58.7373],\n",
       "        [ 28.8280],\n",
       "        [ 16.1392],\n",
       "        [ 12.3413],\n",
       "        [  9.9118],\n",
       "        [  7.3055],\n",
       "        [  7.2924],\n",
       "        [  8.2902],\n",
       "        [  8.3709],\n",
       "        [  7.3502],\n",
       "        [ 11.4875],\n",
       "        [ 23.0528],\n",
       "        [  7.5750],\n",
       "        [  7.4518],\n",
       "        [  7.2630],\n",
       "        [  7.3063],\n",
       "        [ 47.9722],\n",
       "        [ 24.8672],\n",
       "        [ 22.8064],\n",
       "        [  7.2993],\n",
       "        [  7.2952],\n",
       "        [  7.8957],\n",
       "        [  7.3310],\n",
       "        [  7.2621],\n",
       "        [ 62.8507],\n",
       "        [ 13.4586],\n",
       "        [  7.3489],\n",
       "        [  7.4488],\n",
       "        [  9.6751],\n",
       "        [  7.6233],\n",
       "        [  7.3302],\n",
       "        [  7.4366],\n",
       "        [  7.6853],\n",
       "        [  7.3327],\n",
       "        [  7.9437],\n",
       "        [  7.8001],\n",
       "        [  7.6557],\n",
       "        [ 35.5054],\n",
       "        [  7.6289],\n",
       "        [ 15.5183],\n",
       "        [  7.5971],\n",
       "        [  7.6278],\n",
       "        [  8.0831],\n",
       "        [ 10.2539],\n",
       "        [  7.4588],\n",
       "        [ 16.0879],\n",
       "        [  7.8973],\n",
       "        [  7.2884],\n",
       "        [  7.6413],\n",
       "        [  7.3998],\n",
       "        [ 10.7595],\n",
       "        [ 65.4029],\n",
       "        [  7.4915],\n",
       "        [ 17.5760],\n",
       "        [ 24.7810],\n",
       "        [  7.7857],\n",
       "        [ 26.6647],\n",
       "        [  7.3359],\n",
       "        [  7.7449],\n",
       "        [  7.5265],\n",
       "        [  7.3873],\n",
       "        [  7.3201],\n",
       "        [ 17.8094],\n",
       "        [ 10.1433],\n",
       "        [ 12.1487],\n",
       "        [ 44.4729],\n",
       "        [  7.3564],\n",
       "        [  7.7138],\n",
       "        [  7.4531],\n",
       "        [ 36.4044],\n",
       "        [ 63.8747],\n",
       "        [  7.3389],\n",
       "        [ 12.0734],\n",
       "        [ 10.6231],\n",
       "        [ 21.6820],\n",
       "        [  8.5068],\n",
       "        [  8.6627],\n",
       "        [  7.4191],\n",
       "        [  7.9599],\n",
       "        [  7.6137],\n",
       "        [ 73.6426],\n",
       "        [  7.2812],\n",
       "        [  7.4810],\n",
       "        [ 24.0998],\n",
       "        [  7.8488],\n",
       "        [  7.3083],\n",
       "        [ 54.4696],\n",
       "        [ 23.2997],\n",
       "        [  7.2846],\n",
       "        [ 19.8450],\n",
       "        [  9.6692],\n",
       "        [  8.1025],\n",
       "        [  7.2950],\n",
       "        [  7.3021],\n",
       "        [  7.2594],\n",
       "        [  8.0732],\n",
       "        [  7.2820],\n",
       "        [  7.5233],\n",
       "        [  7.2858],\n",
       "        [ 13.2112],\n",
       "        [  9.1211],\n",
       "        [  7.3419],\n",
       "        [  9.7323],\n",
       "        [  7.3709],\n",
       "        [  8.1452],\n",
       "        [  7.3255],\n",
       "        [  7.4320],\n",
       "        [ 15.3519],\n",
       "        [ 30.7865],\n",
       "        [  7.3060],\n",
       "        [  7.3126],\n",
       "        [  7.3160],\n",
       "        [  7.4921],\n",
       "        [ 12.3169],\n",
       "        [  7.6755],\n",
       "        [ 11.8086],\n",
       "        [  7.3597],\n",
       "        [  7.4401],\n",
       "        [  7.3172],\n",
       "        [  7.9897],\n",
       "        [  7.3452],\n",
       "        [ 65.7472],\n",
       "        [  8.4409],\n",
       "        [ 12.8390],\n",
       "        [ 11.8174],\n",
       "        [ 10.5598],\n",
       "        [  7.4107],\n",
       "        [  7.5350],\n",
       "        [  8.1309],\n",
       "        [ 14.5200],\n",
       "        [ 41.9208],\n",
       "        [ 64.8089],\n",
       "        [  8.6128],\n",
       "        [  8.5958],\n",
       "        [ 11.8384],\n",
       "        [  7.8815],\n",
       "        [  7.9090],\n",
       "        [  7.3073],\n",
       "        [  7.4880],\n",
       "        [ 11.5635],\n",
       "        [ 14.1772],\n",
       "        [105.5863],\n",
       "        [  7.4991],\n",
       "        [  7.3872],\n",
       "        [  7.3836],\n",
       "        [  7.6699],\n",
       "        [ 83.0968],\n",
       "        [ 33.7883],\n",
       "        [  7.3221],\n",
       "        [ 12.7066],\n",
       "        [  7.3659],\n",
       "        [ 11.4697],\n",
       "        [  9.2678],\n",
       "        [  7.3064],\n",
       "        [ 11.2616],\n",
       "        [113.1624],\n",
       "        [  7.3467],\n",
       "        [ 11.1972],\n",
       "        [  7.7609],\n",
       "        [  7.3607],\n",
       "        [  7.4826],\n",
       "        [  7.4031],\n",
       "        [  7.6602],\n",
       "        [ 32.1541],\n",
       "        [ 16.2861],\n",
       "        [ 12.3057],\n",
       "        [  7.3666],\n",
       "        [  8.7923],\n",
       "        [  7.2561],\n",
       "        [  9.0036],\n",
       "        [  7.2924],\n",
       "        [ 20.7615],\n",
       "        [ 15.2550],\n",
       "        [ 46.8513],\n",
       "        [  7.3901],\n",
       "        [  7.3147],\n",
       "        [  8.1829],\n",
       "        [  7.6245],\n",
       "        [  8.6371],\n",
       "        [  7.2732],\n",
       "        [ 11.5392],\n",
       "        [  7.3289],\n",
       "        [ 19.2035],\n",
       "        [  7.3096],\n",
       "        [  8.9710],\n",
       "        [  7.6635],\n",
       "        [ 24.1513],\n",
       "        [  7.8350],\n",
       "        [  7.6390],\n",
       "        [ 21.8574],\n",
       "        [ 60.9052],\n",
       "        [  8.9997],\n",
       "        [ 21.6858],\n",
       "        [  7.8273],\n",
       "        [  7.4959],\n",
       "        [  8.0343],\n",
       "        [ 19.5933],\n",
       "        [  7.3070],\n",
       "        [  7.2898],\n",
       "        [  7.2948],\n",
       "        [  7.3538],\n",
       "        [  7.9823],\n",
       "        [ 10.4685],\n",
       "        [ 19.2683],\n",
       "        [ 26.9182],\n",
       "        [  7.3723],\n",
       "        [  8.6130],\n",
       "        [ 54.5670],\n",
       "        [  7.3227],\n",
       "        [ 43.5243],\n",
       "        [ 55.9020],\n",
       "        [  7.6958],\n",
       "        [ 11.2502],\n",
       "        [  7.3105],\n",
       "        [  7.4718],\n",
       "        [ 32.6486],\n",
       "        [ 44.1708],\n",
       "        [  7.2665],\n",
       "        [  8.7618],\n",
       "        [  7.2943],\n",
       "        [  7.3123],\n",
       "        [ 17.3558],\n",
       "        [  7.5848],\n",
       "        [  7.2776],\n",
       "        [  8.2276],\n",
       "        [  7.5488],\n",
       "        [  7.8457],\n",
       "        [  7.4733],\n",
       "        [  7.2628],\n",
       "        [  7.3862],\n",
       "        [ 14.4424],\n",
       "        [  7.3604],\n",
       "        [  9.3902],\n",
       "        [  8.0572],\n",
       "        [ 12.1170],\n",
       "        [  9.2003],\n",
       "        [  7.2896],\n",
       "        [  7.4790],\n",
       "        [  7.8886],\n",
       "        [  7.5113],\n",
       "        [ 10.0245],\n",
       "        [ 39.3171],\n",
       "        [  7.9807],\n",
       "        [  7.8480],\n",
       "        [  7.3303],\n",
       "        [ 12.6934],\n",
       "        [  8.2651],\n",
       "        [  7.5424],\n",
       "        [  8.9772],\n",
       "        [  8.4376],\n",
       "        [ 46.5646],\n",
       "        [  7.5156],\n",
       "        [  9.5810],\n",
       "        [  8.1288],\n",
       "        [  7.3690],\n",
       "        [ 17.2852],\n",
       "        [  8.0929],\n",
       "        [ 15.2351],\n",
       "        [ 21.6409],\n",
       "        [ 36.0730],\n",
       "        [  9.6637],\n",
       "        [ 21.3402],\n",
       "        [ 10.2132],\n",
       "        [ 19.5702],\n",
       "        [ 12.5886],\n",
       "        [ 49.4521],\n",
       "        [  7.4083],\n",
       "        [ 22.9225],\n",
       "        [  7.5051],\n",
       "        [  7.7985],\n",
       "        [  7.2667],\n",
       "        [  8.0582],\n",
       "        [  7.2842],\n",
       "        [  7.3049],\n",
       "        [ 19.2080],\n",
       "        [  8.1242],\n",
       "        [  7.6067],\n",
       "        [  9.8511],\n",
       "        [ 16.3406],\n",
       "        [  8.1085],\n",
       "        [  7.5951],\n",
       "        [  7.4148],\n",
       "        [  7.6193],\n",
       "        [  7.7382],\n",
       "        [  7.6122],\n",
       "        [  7.3720],\n",
       "        [  8.6467],\n",
       "        [  7.7299],\n",
       "        [  8.1159],\n",
       "        [ 14.4064],\n",
       "        [  7.3562],\n",
       "        [  8.0366],\n",
       "        [  7.4179],\n",
       "        [  8.0096],\n",
       "        [ 10.7220],\n",
       "        [  7.5181],\n",
       "        [  7.7488],\n",
       "        [ 13.9617],\n",
       "        [  7.5575],\n",
       "        [  7.7277],\n",
       "        [  7.3121],\n",
       "        [ 37.5538],\n",
       "        [ 17.1518],\n",
       "        [  9.8693],\n",
       "        [  7.2919],\n",
       "        [ 25.4225],\n",
       "        [ 37.4586],\n",
       "        [  7.8780],\n",
       "        [ 23.0881],\n",
       "        [  8.2189],\n",
       "        [  7.2710],\n",
       "        [  7.3269],\n",
       "        [  9.3384],\n",
       "        [ 36.1579],\n",
       "        [ 34.9397],\n",
       "        [ 58.6413],\n",
       "        [ 52.7399],\n",
       "        [  7.3436],\n",
       "        [  7.3205],\n",
       "        [ 13.7024],\n",
       "        [ 13.2948],\n",
       "        [  8.3609],\n",
       "        [  7.7536],\n",
       "        [  8.0049],\n",
       "        [ 45.6704],\n",
       "        [ 25.5737],\n",
       "        [  8.0442],\n",
       "        [  7.2805],\n",
       "        [  7.4448],\n",
       "        [ 17.4037],\n",
       "        [ 13.7722],\n",
       "        [  7.4525],\n",
       "        [  9.8482],\n",
       "        [  7.5379],\n",
       "        [ 31.3458],\n",
       "        [  7.5697],\n",
       "        [ 12.9341],\n",
       "        [ 19.9603],\n",
       "        [  8.0011],\n",
       "        [  7.2879],\n",
       "        [  7.3694],\n",
       "        [  7.2677],\n",
       "        [  7.3416],\n",
       "        [  7.7936],\n",
       "        [  7.3923],\n",
       "        [ 15.8654],\n",
       "        [ 23.7584],\n",
       "        [  9.6085],\n",
       "        [ 16.4962],\n",
       "        [  9.3953],\n",
       "        [ 16.3184],\n",
       "        [ 12.4445],\n",
       "        [ 20.3359],\n",
       "        [  7.7041],\n",
       "        [ 11.2672],\n",
       "        [  7.4683],\n",
       "        [  7.6109],\n",
       "        [  7.2641],\n",
       "        [ 27.4161],\n",
       "        [ 13.8390],\n",
       "        [ 10.3878],\n",
       "        [  7.6510],\n",
       "        [  7.2618],\n",
       "        [ 10.4625],\n",
       "        [ 11.0321],\n",
       "        [  9.9561],\n",
       "        [ 30.2104],\n",
       "        [ 17.7227],\n",
       "        [ 12.7569],\n",
       "        [ 54.2568],\n",
       "        [  8.2280],\n",
       "        [  7.6819],\n",
       "        [ 43.3909],\n",
       "        [  7.3459],\n",
       "        [  8.5252],\n",
       "        [  7.5803],\n",
       "        [ 10.6710],\n",
       "        [  9.2468],\n",
       "        [ 35.4992],\n",
       "        [  7.3723],\n",
       "        [  7.2970],\n",
       "        [ 27.9612],\n",
       "        [  9.9140],\n",
       "        [ 25.5243],\n",
       "        [  7.4502],\n",
       "        [  8.4757],\n",
       "        [  7.3226],\n",
       "        [  7.3166],\n",
       "        [ 23.5995],\n",
       "        [ 44.9695],\n",
       "        [  7.3786],\n",
       "        [  7.7688],\n",
       "        [ 10.6496],\n",
       "        [  7.2784],\n",
       "        [  8.6297],\n",
       "        [  7.3326],\n",
       "        [  7.5491],\n",
       "        [  7.3477],\n",
       "        [  8.2679],\n",
       "        [  7.7515],\n",
       "        [  8.9009],\n",
       "        [  7.5327],\n",
       "        [  7.9288],\n",
       "        [  7.3648],\n",
       "        [  7.3853],\n",
       "        [  7.3808],\n",
       "        [  9.5241],\n",
       "        [ 10.8049],\n",
       "        [  7.8378],\n",
       "        [  7.6502],\n",
       "        [  7.3092],\n",
       "        [  7.5902],\n",
       "        [ 18.4632],\n",
       "        [  7.4036],\n",
       "        [ 20.0318],\n",
       "        [ 80.3591],\n",
       "        [  7.2636],\n",
       "        [  8.9315],\n",
       "        [ 29.3141],\n",
       "        [ 53.1166],\n",
       "        [  7.6325],\n",
       "        [ 27.3518],\n",
       "        [  7.3624],\n",
       "        [  7.5759],\n",
       "        [  9.3225],\n",
       "        [  9.8467],\n",
       "        [  7.3444],\n",
       "        [  7.4045],\n",
       "        [ 11.7815],\n",
       "        [ 24.1000],\n",
       "        [  7.5612],\n",
       "        [ 13.2475],\n",
       "        [  7.2972],\n",
       "        [  8.3664],\n",
       "        [ 11.5096],\n",
       "        [  7.3324],\n",
       "        [ 22.3525],\n",
       "        [  7.4733],\n",
       "        [  7.4399],\n",
       "        [  8.0925],\n",
       "        [  7.3884],\n",
       "        [  8.5565],\n",
       "        [ 29.3073],\n",
       "        [  8.3697],\n",
       "        [  7.4427],\n",
       "        [ 17.6689],\n",
       "        [  8.2857],\n",
       "        [ 12.7581],\n",
       "        [  7.4992],\n",
       "        [  7.3626],\n",
       "        [  7.2889],\n",
       "        [  8.1179],\n",
       "        [  7.3863],\n",
       "        [  7.4290],\n",
       "        [  7.7276],\n",
       "        [  7.6435],\n",
       "        [  7.3375],\n",
       "        [ 22.1081],\n",
       "        [  9.7005],\n",
       "        [  7.6489],\n",
       "        [ 19.9451],\n",
       "        [  7.3956],\n",
       "        [  9.4524],\n",
       "        [  7.8022],\n",
       "        [  8.5521],\n",
       "        [  7.3349],\n",
       "        [ 12.2390],\n",
       "        [ 22.3737],\n",
       "        [  7.8593],\n",
       "        [  7.6503],\n",
       "        [  7.8286],\n",
       "        [  7.8772],\n",
       "        [ 14.5004],\n",
       "        [  7.3083],\n",
       "        [  7.2958],\n",
       "        [  7.2798],\n",
       "        [  7.9454],\n",
       "        [  9.6585],\n",
       "        [  7.4749],\n",
       "        [  7.7959],\n",
       "        [ 24.3936],\n",
       "        [ 24.7871],\n",
       "        [ 22.9127],\n",
       "        [  7.2546],\n",
       "        [  7.9715],\n",
       "        [  7.8180],\n",
       "        [  7.8633],\n",
       "        [  7.5243],\n",
       "        [  7.3327],\n",
       "        [ 11.8512],\n",
       "        [  7.5267],\n",
       "        [  8.0280],\n",
       "        [  8.9139],\n",
       "        [  9.5853],\n",
       "        [  7.3207],\n",
       "        [ 28.5567],\n",
       "        [  9.3148],\n",
       "        [ 17.9963],\n",
       "        [  7.2934],\n",
       "        [  7.5435],\n",
       "        [  9.1997],\n",
       "        [  7.3495],\n",
       "        [  7.4994],\n",
       "        [  7.4345],\n",
       "        [  7.3366],\n",
       "        [  9.3094],\n",
       "        [ 65.2708],\n",
       "        [ 38.9404],\n",
       "        [  7.3754],\n",
       "        [  8.3332],\n",
       "        [  9.4915],\n",
       "        [  8.9519],\n",
       "        [  7.9437],\n",
       "        [  7.3362],\n",
       "        [  9.5957],\n",
       "        [ 18.0235],\n",
       "        [ 18.3616],\n",
       "        [ 28.3283],\n",
       "        [ 30.6242],\n",
       "        [ 16.9882],\n",
       "        [  7.3226],\n",
       "        [  9.2012],\n",
       "        [  8.0201],\n",
       "        [  9.4775],\n",
       "        [  7.2986],\n",
       "        [  9.8155],\n",
       "        [ 13.8411],\n",
       "        [  7.9762],\n",
       "        [  7.6991],\n",
       "        [ 20.1202]], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_train.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ea89b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the MLP\n",
    "mlp = MLP()\n",
    "mlp.float()\n",
    "# Define the loss function and optimizer\n",
    "loss_function = nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f533ceb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       800\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       20,100\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       201\n",
       "=================================================================\n",
       "Total params: 21,101\n",
       "Trainable params: 21,101\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(mlp, batch_size=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00af4a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x100 and 200x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_42999/2667329530.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Perform forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_42999/1029325614.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mForward\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         '''\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x100 and 200x1)"
     ]
    }
   ],
   "source": [
    "# Run the training loop\n",
    "for epoch in range(0, 100): # 5 epochs at maximum  \n",
    "    # Print epoch\n",
    "    print(f'Starting epoch {epoch+1}')\n",
    "\n",
    "    # Set current loss value\n",
    "    current_loss = 0.0\n",
    "\n",
    "    # Iterate over the DataLoader for training data\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "          # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform forward pass\n",
    "        outputs = mlp(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_function(outputs, targets)\n",
    "\n",
    "        # Perform backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Perform optimization\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        current_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print('Loss after mini-batch %5d: %.3f' %\n",
    "                (i + 1, current_loss / 500))\n",
    "            current_loss = 0.0\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    mlp.eval()     # Optional when not using Model Specific layer\n",
    "    for i, data in enumerate(validloader, 0):\n",
    "        # Get and prepare inputs\n",
    "        inputs, targets = data\n",
    "        inputs, targets = inputs.float(), targets.float()\n",
    "        targets = targets.reshape((targets.shape[0], 1))\n",
    "        \n",
    "        output_val = mlp(inputs)\n",
    "        loss = loss_function(output_val, targets)\n",
    "        valid_loss += loss.item()\n",
    "\n",
    "    print('Epoch {:.3f} \\t\\t Training Loss: {:.3f} \\t\\t Validation Loss:'\\\n",
    "          '{:.3f}'.format(epoch, current_loss / len(trainloader), valid_loss / len(validloader)))\n",
    "#     if min_valid_loss > valid_loss:\n",
    "#         print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
    "#         min_valid_loss = valid_loss\n",
    "#         # Saving State Dict\n",
    "#         torch.save(model.state_dict(), 'saved_model.pth')\n",
    "\n",
    "# Process is complete.\n",
    "print('Training process has finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9d06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.random.randn(13)\n",
    "# torch usa tensores de torch y no numpy.darrays\n",
    "dtype = torch.float\n",
    "test = torch.randn((1, 3), device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3332d5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = mlp.forward(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c6a43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc4848",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0, 0], X_test[0]\n",
    "# xtest = [x[0] for x in X_test]\n",
    "ypred = [y[0].item() for y in y_pred]\n",
    "ytest = [y[0].item() for y in y_test]\n",
    "ytest, ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b7e8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(ytest, ypred)\n",
    "plt.xlabel(\"True\")\n",
    "plt.ylabel(\"Predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f558b3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
